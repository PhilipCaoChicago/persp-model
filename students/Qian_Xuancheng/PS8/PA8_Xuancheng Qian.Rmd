---
title: "Problem set 8#Xuancheng Qian"
author: "Xuancheng Qian"
date: "3/6/2017"
output:
  github_document:
    toc: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
```


```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(titanic)
library(rcfss)
library(pROC)
library(gbm)
# to get the tree graphs with the labels and values, use the forked
# version of ggdendro
# devtools::install_github("bensoltoff/ggdendro")
library(ggdendro)
library(e1071)
library(grid)
library(gridExtra)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())

```

```{r err-rate-rf}
err.rate.rf <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  
  pred <- predict(model, newdata = data, type = "response")
  actual <- data[[response]]
  
  return(mean(pred != actual, na.rm = TRUE))
}
```

# Part 1: Sexy Joe Biden (redux times two) [3 points]

```{r biden}
#import data set
df_biden = read.csv('data/biden.csv')
# str(df)
# 
df_biden<- df_biden %>%
    mutate_each(funs(as.factor(.)), dem, rep,female)

```


1. Split the data into a training set (70%) and a validation set (30%). **Be sure to set your seed prior to this part of your code to guarantee reproducibility of results.**
1. Fit a decision tree to the training data, with `biden` as the response variable and the other variables as predictors. Plot the tree and interpret the results. What is the test MSE?
    * Leave the control options for `tree()` at their default values

```{r mse-function}
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}
```

```{r biden-tree-mse}
set.seed(1234)
biden_split <- resample_partition(df_biden, c(test = 0.3, train = 0.7))
biden_tree <- tree(biden~ age+ female + educ + dem + rep, data = biden_split$train)
tree_data <- dendro_data(biden_tree)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()
ptree

# summary(train_model)
# tidy(train_model)
biden_mse = mse(biden_tree, biden_split$test) 
# biden_mse
```
* From the tree above, we can see that if $dem = TRUE$, which indicates that respondent is a Democrat, the respondent would have feeling thermometer towards Joe Biden is 74.51 by average. And when the $dem=FALSE$ and $rep=TRUE$, which indicates that respondent is a Republican, the respondent would have feeling thermometer towards Joe Biden is 43.23 by average. And if the respondent is neither a Democrat nor a Republican, the respondent would have feeling thermometer towards Joe Biden is 57.6 by average. The Democratic respondents would have strong positive feeling towards Joe Biden compared with other respondents.
* When the control options for tree() at their default values, the test MSE is 406.4167.

1. Now fit another tree to the training data with the following `control` options:

    ```{r eval = FALSE}
    tree(control = tree.control(nobs = # number of rows in the training set,
                                  mindev = 0))
    ```
    
    Use cross-validation to determine the optimal level of tree complexity, plot the optimal tree, and interpret the results. Does pruning the tree improve the test MSE?
    
```{r biden-tree-10-fold-mse}
biden_tree <- tree(biden~ age+ female + educ + dem + rep, data = biden_split$train, control = tree.control(nobs = nrow(biden_split$train),
                            mindev = 0))
# mod <- biden_tree
# generate 10-fold CV trees
biden_cv <- crossv_kfold(as_tibble(biden_split$train), k = 10) %>%
  mutate(tree = map(train, ~ tree(biden~ age+ female + educ + dem + rep, data = .,
     control = tree.control(nobs = nrow(biden_split$train),
                            mindev = 0))))
# calculate each possible prune result for each fold
biden_cv <- expand.grid(biden_cv$.id, 2:10) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(biden_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         mse = map2_dbl(prune, test, mse))

biden_cv %>%
  select(k, mse) %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of terminal nodes",
       y = "Test MSE")

mod <- prune.tree(biden_tree, best = 4)
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend),
               alpha = 0.5) +
  geom_text(data = label(tree_data),
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data),
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()
ptree

biden_mse = mse(mod, biden_split$test)
biden_mse
biden_mse_full = mse(biden_tree, biden_split$test)
biden_mse_full




```
* Use 10-fold cross validation, we can see that the optimal level is 4, then we prune and get the following tree.
* From the tree above, we can see that if $dem=TRUE$, which indicates that respondent is a Democrat, and the respondent's age is 53.5 or more, the respondent would have feeling thermometer towards Joe Biden is 78.64 by average. And when the respondent is a Democrat with 53.5 years old or younger,the respondent would have feeling thermometer towards Joe Biden is 71.86 by average. When the $dem=FALSE$ and $rep=TRUE$, which indicates that respondent is a Republican, the respondent would have feeling thermometer towards Joe Biden is 43.23 by average. And if the respondent is neither a Democrat nor a Republican, the respondent would have feeling thermometer towards Joe Biden is 57.6 by average. The older Democratic respondents would have strong positive feeling towards Joe Biden compared with other respondents.
* When the control options based on the requirement, the test MSE after pruning the tree is 407.1560. Pruning the tree actually improved the test MSE compared with the test MSE of full tree which is 481.4899. However, the test MSE did not change a lot compared with the test MSE (406.4167) of default tree in the problem above.

1. Use the bagging approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results.

```{r biden-bag-oob}
(biden_bag <- randomForest(biden ~ ., data = biden_split$train,
                             mtry = 5, ntree = 500))
biden_mse = mse(biden_bag, biden_split$test)
biden_mse

data_frame(var = rownames(importance(biden_bag)),
           MeanDecreaseRSS = importance(biden_bag)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseRSS, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseRSS)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting feeling value towards Joe Biden ",
       subtitle = "Bagging",
       x = NULL,
       y = "Average decrease in the residual sum of squares")
```



* The test MSE of the bagging approach is 485.2877, which is higher than the pruned tree. When we consider that the number of variables tried at each split is 5, in terms of variable importance measures, the regression tree use the average decrease in residual sum of squares measure. The importance ranking of variables in the biden data set is as following: age > dem > educ > rep > female. The variables like age, dem, educ, rep is more important in the predicting feeling towards Joe Biden in terms of reducing RSS compared with the variable like female.

1. Use the random forest approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results. Describe the effect of $m$, the number of variables considered at each split, on the error rate obtained.

```{r biden-rf}
set.seed(1234)
(biden_rf <- randomForest(biden ~ ., data = biden_split$train,
                            ntree = 500))

seq.int(biden_rf$ntree) %>%
  map_df(~ getTree(biden_rf, k = ., labelVar = TRUE)[1,]) %>%
  count(`split var`) %>%
  knitr::kable(caption = "Variable used to generate the first split in each tree",
               col.names = c("Variable used to split", "Number of training observations"))

data_frame(var = rownames(importance(biden_rf)),
           `Random forest` = importance(biden_rf)[,1]) %>%
  left_join(data_frame(var = rownames(importance(biden_rf)),
           Bagging = importance(biden_bag)[,1])) %>%
  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%
  gather(model, rss, -var) %>%
  ggplot(aes(var, rss, color = model)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting feeling value towards Joe Biden ",
       subtitle = "Bagging",
       x = NULL,
       y = "Average decrease in the residual sum of squares")

biden_mse = mse(biden_rf, biden_split$test)
biden_mse



```

```{r biden-rf-m}
set.seed(1234)
biden_rf_m =data.frame()
biden_mse_m=data.frame()
library(foreach)
biden_rf_m <- foreach(i = 1:5) %do% (randomForest(biden ~ ., data = biden_split$train,mtry =i, ntree = 500))
biden_mse_m <- foreach(i = 1:5) %do% {
  (imp <-randomForest(biden ~ ., data = biden_split$train,mtry =i, ntree = 500))
   result = mse(imp,biden_split$test)
}
biden_mse_m 

# (biden_rf <- randomForest(biden ~ ., data = biden_split$train,
#                             mtry=2,ntree = 500))
# 
# seq.int(biden_rf$ntree) %>%
#   map_df(~ getTree(biden_rf, k = ., labelVar = TRUE)[1,]) %>%
#   count(`split var`) %>%
#   knitr::kable(caption = "Variable used to generate the first split in each tree",
#                col.names = c("Variable used to split", "Number of training observations"))
# 
# data_frame(var = rownames(importance(biden_rf)),
#            `Random forest` = importance(biden_rf)[,1]) %>%
#   left_join(data_frame(var = rownames(importance(biden_rf)),
#            Bagging = importance(biden_bag)[,1])) %>%
#   mutate(var = fct_reorder(var, Bagging, fun = median)) %>%
#   gather(model, rss, -var) %>%
#   ggplot(aes(var, rss, color = model)) +
#   geom_point() +
#   coord_flip() +
#   labs(title = "Predicting feeling value towards Joe Biden ",
#        subtitle = "Bagging",
#        x = NULL,
#        y = "Average decrease in the residual sum of squares")


```

* The test MSE of the random forest approach is 408.0484. In terms of variable importance measures, the regression tree still uses the average decrease in residual sum of squares measure. The importance ranking of variables in the biden data set is as following: dem > rep > age > educ > female. The variables like dem, rep, age is more important in the predicting feeling towards Joe Biden in terms of reducing RSS compared with the variable like educ, female. The test MSE achieves the minimum when the number is 2, and gradually increases with the increase of m. And it achieves the maximum when the number of splitting is 5.


1. Use the boosting approach to analyze the data. What test MSE do you obtain? How does the value of the shrinkage parameter $\lambda$ influence the test MSE?

```{r biden-compare-all}
set.seed(1234)

biden_boost <- gbm(biden ~ ., data = biden_split$train, n.trees = 10000, interaction.depth = 1)
pred_boost = predict(biden_boost, newdata = biden_split$test, n.trees = 10000)

mse_boost <- mean((pred_boost - df_biden[biden_split$test[2]$idx, ]$biden)^2)
mse_boost_m <- numeric(5)
shrinkages <- numeric(5)
trees_m <- numeric(5)
for (t in 1:5){
  shrinkages[t] <- 10^(-t)
  trees_m[t]<-10^(5)*2*t
  biden_boost <- gbm(biden ~ ., data = biden_split$train, n.trees = trees_m[t], interaction.depth = 1, shrinkage = shrinkages[t])
  pred_boost = predict(biden_boost, newdata = biden_split$test, n.trees = 10000)
  mse_boost_m[t] <- mean((pred_boost - df_biden[biden_split$test[2]$idx, ]$biden)^2)
}

data_frame(mse = mse_boost_m, shrinkage = shrinkages) %>% 
  ggplot(aes(shrinkage, mse)) +
  geom_point() +
  geom_line() +
  labs(title = "Predicting Biden thermometer",
       subtitle = "Boosting",
       x = "Shrinkage",
       y = "Test MSE")

```
* The test MSE of boosting approach is 399.5437, which performs better compared with the previous random forest approach (test MSE 408.0484). Generally, as the shrinkage parameter gets smaller, then the number of trees must increase. As testing different shrinkage level (1e-5,0.0001, 0.001, 0.01, 0.1), and the number of trees (2e+05,4e+05,6e+05,8e+05, 1e+06), the test MSE goes down first from 1e-5 to 0.001, and goes up from 0.001 to 0.1. The best shrinkage level in this case would be 0.001, which has a better test MSE around 400.

# Part 2: Modeling voter turnout [3 points]

1. Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five tree-based models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)


```{r voter-tree-compare-all}
set.seed(1234)
(mh <- read_csv("data/mental_health.csv") %>%
  mutate_each(funs(as.factor(.)), vote96, black, female, married) %>%
  na.omit)

mh_split <- resample_partition(mh, p = c("test" = .3, "train" = .7))

# tree 0
voter_tree0 <- tree(vote96~., data = mh_split$train)
err_rate0 = err.rate.tree(voter_tree0,mh_split$test)

fitted <- predict(voter_tree0, as_tibble(mh_split$test), type = "class")

roc_0 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
auc_0 <- auc(roc_0)


# tree 1
voter_tree <- tree(vote96~., data = mh_split$train, control = tree.control(nobs = nrow(mh_split$train),
                            mindev = 0))

mod <- voter_tree

err_rate1 = err.rate.tree(voter_tree,mh_split$test)

fitted <- predict(voter_tree, as_tibble(mh_split$test), type = "class")

roc_1 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
auc_1 <- auc(roc_1)

# tree 2
err.rate.tree <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  
  pred <- predict(model, newdata = data, type = "class")
  actual <- data[[response]]
  
  return(mean(pred != actual, na.rm = TRUE))
}

mh_cv <- as_tibble(mh_split$train) %>%
  na.omit() %>%
  crossv_kfold(k = 10) %>%
  mutate(tree = map(train, ~ tree(vote96~., data = .,
     control = tree.control(nobs = nrow(mh_split$train),
                            mindev = .001))))
# calculate each possible prune result for each fold
mh_cv <- expand.grid(mh_cv$.id,2:15) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(mh_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.misclass(.x, best = .y)),
         mse = map2_dbl(prune, test, err.rate.tree))

mh_cv %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(title = "Voter tree",
       subtitle = "classfication tree",
       x = "Number of terminal nodes",
       y = "Test error rate")

mod <- prune.tree(voter_tree, best = 13)

err_rate2 = err.rate.tree(mod, mh_split$test)

fitted <- predict(mod, as_tibble(mh_split$test), type = "class")

roc_2 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
auc_2 <- auc(roc_2)

# bagging tree
(voter_bag <- randomForest(vote96 ~ ., data = mh_split$train,
                             mtry = 7, ntree = 500))

err_rate3 = err.rate.tree(voter_bag,mh_split$test)
fitted <- predict(voter_bag, as_tibble(mh_split$test), type = "class")

roc_3 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
auc_3 <- auc(roc_3)


# random forest
(voter_rf <- randomForest(vote96 ~ ., data = mh_split$train,
                            ntree = 500))
fitted <- predict(voter_rf, as_tibble(mh_split$test), type = "class")

err_rate4 = err.rate.tree(voter_rf,mh_split$test)

roc_4 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
auc_4 <- auc(roc_4)




data_frame(var = rownames(importance(voter_rf)),
           `Random forest` = importance(voter_rf)[,1]) %>%
  left_join(data_frame(var = rownames(importance(voter_rf)),
           Bagging = importance(voter_bag)[,1])) %>%
  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%
  gather(model, gini, -var) %>%
  ggplot(aes(var, gini, color = model)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting vote in the 1996 presidential election",
       x = NULL,
       y = "Average decrease in the Gini Index",
       color = "Method")



err_rate <- data.frame(err_rate0,err_rate1,err_rate2,err_rate3,err_rate4)
auc_m <-data.frame(auc_0,auc_1,auc_2,auc_3,auc_4)
err_rate
auc_m
plot(roc_0, print.auc = TRUE, col = "blue")
plot(roc_1, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_2, print.auc = TRUE, col = "yellow", print.auc.y = .6, add = TRUE)
plot(roc_3, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
plot(roc_4, print.auc = TRUE, col = "green", print.auc.y = .7, add = TRUE)

```
* In this part, we split the dataset into 70% training set and 30% testing set.We apply five tree models. The first tree is decision tree with default setting. The error rate for this model is 0.304. And the area under the curve is 0.56 (Blue line).
The second tree model is full tree model with all predictors and control option including age, inc10 (family income), mhealth_sum (respondent's mental health), educ (Number of years of formal education), whether the respondent is black or not, whether the respondent is female or not, whether the respondent is married or not.The error rate of this tree model is 0.335. The area under the curve is 0.623 (red line). Next, we use 10-fold cross validation to determine the optimal level of tree complexity and prune the tree. The error rate of this model is 0.298, and the area under the curve is 0.64 (yellow line). Then we apply bagging approach, the error rate is 0.312, and the area under the curve is 0.623 (orange line). Finally we apply the random forest approach, the error rate is 0.307 and the area under the curve is 0.615 (green line).
* For classification trees, in terms of variable importance measure, larger values are better. So for the voter bagging model and random forest model, age, inc10, mhelth_sum are the most important predictors, whereas educ, female, married, black are relatively unimportant.
* From the classification error rate and area under the curve, we can see that the pruned tree has a better performance compared with other tree models. The bagging approach performs a little better than random forest approach in this dataset in terms of error rate and AUC.

```{r voter-svm-compare-all}
set.seed(1234)
(mh <- read_csv("data/mental_health.csv") %>%
  mutate_each(funs(as.factor(.)), vote96, black, female, married) %>%
  na.omit)

mh_split <- resample_partition(mh, p = c("test" = .3, "train" = .7))

# svm 1
mh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_lin_tune)
mh_lin <- mh_lin_tune$best.model
summary(mh_lin)

fitted <- predict(mh_lin, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
# plot(roc_line)

auc(roc_line)

# svm 2
mh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_poly_tune)

mh_poly <- mh_poly_tune$best.model
summary(mh_poly)
fitted <- predict(mh_poly, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
# plot(roc_poly)
auc(roc_poly)

#svm 3
mh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_rad_tune)

mh_rad <- mh_rad_tune$best.model
summary(mh_rad)

fitted <- predict(mh_rad, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
# plot(roc_rad)
auc(roc_rad)

#svm 4

mh_sig_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "sigmoid",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_sig_tune)

mh_sig <- mh_sig_tune$best.model
summary(mh_sig)

fitted <- predict(mh_sig, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_sig <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
# plot(roc_rad)
auc(roc_sig)


# svm 5
mh_poly2_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",degree =10,
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_poly2_tune)

mh_poly2 <- mh_poly2_tune$best.model
summary(mh_poly2)
fitted <- predict(mh_poly2, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly2 <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
# plot(roc_poly)
auc(roc_poly2)


plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_poly2, print.auc = TRUE, col = "yellow", print.auc.y = .6, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
plot(roc_sig, print.auc = TRUE, col = "green", print.auc.y = .7, add = TRUE)
svm_auc = data.frame(auc(roc_line), auc(roc_poly),auc(roc_poly2),auc(roc_rad),auc(roc_sig))
svm_auc
```

* In this plot, we evaluate five SVM models of voter turnout including linear kernel svm,polynomial kernel svm with default setting (degree 3),polynomial kernel svm with degree 10, radial kernel svm and sigmoid kernel svm. 
* The blue indicates the linear kernel svm, the area under the curve is 0.742. The red line indicates the polynomial kernel svm with degree 3, the area under the curve is 0.741. The yellow line indicates the polynomial kernel svm with degree 10, the area under the curve is 0.661. The orange indicates the radial kernel svm, the area under the curve is 0.735. And the green line indicates the sigmoid kernel svm, the area under the curve is 0.733. The larger AUC indicates the better performance of models. In this dataset, we can see most of the svm approachs have a good performance as the AUC is larger than 0.733 except polynomial kernel with degree 10, which is 0.661. And the svm with linear kernel and polynomial kernel with degree 3 perform better than other svms in terms of AUC.

# Part 3: OJ Simpson [4 points]

```{r OJ simpson data set}
df_simpson <- read_csv("data/simpson.csv") %>%
  na.omit() %>%
  mutate_each(funs(as.factor(.)), guilt, dem, rep, ind, female, black, hispanic, educ, income)

```
1. What is the relationship between race and belief of OJ Simpson's guilt? Develop a robust statistical learning model and use this model to explain the impact of an individual's race on their beliefs about OJ Simpson's guilt.

```{r OJ simpson logistic}
set.seed(1234)

simpson_split <- resample_partition(df_simpson, c(test = 0.3, train = 0.7))

simpson_logistic <- glm(guilt ~ black + hispanic, data = simpson_split$train, family = binomial)
summary(simpson_logistic)

Prob <- function(model, data){
  data <- data %>% 
    add_predictions(model) %>% 
    mutate(prob = exp(pred) / (1 + exp(pred)),
           pred_bi = as.numeric(prob > .5))
  return(data)
}

logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}

prob2odds <- function(x){
  x / (1 - x)
}

prob2logodds <- function(x){
  log(prob2odds(x))
}

simpson_logistic_test <- Prob(simpson_logistic, as_tibble(simpson_split$test))


grid1<- as.data.frame(simpson_split$test) %>%
  data_grid(black, hispanic) %>%
  add_predictions(simpson_logistic) %>% 
  mutate(prob = exp(pred) / (1 + exp(pred))) %>%
  mutate(odds = prob2odds(prob)) %>%
  mutate(logodds = prob2logodds(prob))


ggplot(grid1, aes(black, prob, group = factor(hispanic), color = factor(hispanic))) +
  geom_line() +
  scale_color_discrete(name = "Hispanic or not (hispanic = 1)") +
  labs(title = "Predicted probability of guilt belief",
       subtitle = "Logistic regression with race",
       x = "Black or not (black = 1)",
       y = "Predicted probability of guilt belief")

#Accuracy
simp_accuracy <- mean(simpson_logistic_test$guilt == simpson_logistic_test$pred_bi, na.rm = TRUE)
simp_accuracy
#PRE
simp_y<- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(simp_y != median(simp_y)))
E2 <- 1 - simp_accuracy
PRE <- (E1 - E2) / E1
PRE

#AUC
simp_auc <- auc(simpson_logistic_test$guilt, simpson_logistic_test$pred_bi)
simp_auc

```

```{r OJ simpson decision tree}
set.seed(1234)


simpson_tree<- tree(guilt ~ black + hispanic, data = simpson_split$train)

mod <- simpson_tree
tree_data <- dendro_data(mod)

ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), alpha = 0.5) +
  geom_text(data = label(tree_data), aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Simpson guilt tree",
       subtitle = "black + hispanic")


#AUC

simpson_err_rate <-err.rate.tree(mod, simpson_split$test)
fitted <- predict(mod, as_tibble(simpson_split$test), type = "class")
simpson_roc <- roc(as.numeric(as_tibble(simpson_split$test)$guilt), as.numeric(fitted))
simpson_auc<- auc(simpson_roc)
plot(simpson_roc)
simpson_auc

```

```{r OJ simpson part1 k-fold}
#k-fold cross validation
fold_model_mse <- function(df, k){
  cv10_data <- crossv_kfold(df, k = k)
  cv10_models <- map(cv10_data$train, ~ glm(guilt ~ black + hispanic, family = binomial, data = .))
  cv10_prob <- map2(cv10_models, cv10_data$train, ~Prob(.x, as.data.frame(.y)))
  cv10_mse <- map(cv10_prob, ~ mean(.$guilt != .$pred_bi, na.rm = TRUE))
  return(data_frame(cv10_mse))
}

set.seed(1234)
mses <- rerun(100, fold_model_mse(df_simpson, 10)) %>%
  bind_rows(.id = "id")

mse_100cv10 <- mean(as.numeric(mses$cv10_mse))
mseSd_100cv10 <- sd(as.numeric(mses$cv10_mse))
mse_100cv10
mseSd_100cv10


```

* We first apply logistic regression to this problem. From the summary, we can see that the coefficient of black factor is statistically significant under $\alpha=0.001$, and the coefficient of hispanic is statistically significant under $\alpha=0.1$. Both factors have negative relationship with the response of belief. For one unit increase in black (being black), the estimated odds of believing guilt would decrease by roughtly a factor of 25. For one unit increase in hispanic (being hispanic), the estimated odds of believing guilt would decrease by roughtly a factor of 1.72. 
* The error rate of this mode is 0.17, and PRE is 0.434, and the area under the curve is 0.744 indicating that this model fits the dataset well.
* We also apply a decision tree to this problem as it is intuitive to explain the relationship for classification problem. The AUC is 0.744 indicating that this model performs well in this dataset. From the tree, we can see that being black would believe O.J. Simpson was probably not guilty and not being black would believe O.J. Simpson was guilty. 
* Both models fit the data well. And I choose logistic regression as my model to explain the relationship between race and belief of OJ Simpson's guiltmore due to the interpretation. Then We apply 10-fold cross validation to the logistic model. The average test MSE is 0.184, which indicates that this model performs well.

1. How can you predict whether individuals believe OJ Simpson to be guilty of these murders? Develop a robust statistical learning model to predict whether individuals believe OJ Simpson to be either probably guilty or probably not guilty and demonstrate the effectiveness of this model using methods we have discussed in class.

```{r OJ simpson svm}
set.seed(1234)
# svm 1

simpson_lin_tune <- tune(svm, guilt ~ dem + rep + age + educ + female + black + hispanic + income, data = as_tibble(simpson_split$train), kernel = 'linear', range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
simpson_lin <- simpson_lin_tune$best.model                         
summary(simpson_lin)

fitted <- predict(simpson_lin, as_tibble(simpson_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(simpson_split$test)$guilt, fitted$decision.values)
# plot(roc_line)

auc(roc_line)

# svm 2
simpson_poly_tune <- tune(svm, guilt ~ dem + rep + age + educ + female + black + hispanic + income, data = as_tibble(simpson_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100), degree = c(3, 4, 5, 6)))
summary(simpson_poly_tune)

simpson_poly <- simpson_poly_tune$best.model
summary(simpson_poly)


fitted <- predict(simpson_poly, as_tibble(simpson_split$test), decision.values = TRUE) %>%
  attributes

roc_poly<-  roc(as_tibble(simpson_split$test)$guilt, fitted$decision.values)
# plot(roc_line)

auc(roc_poly)


#svm 3
simpson_rad_tune <- tune(svm, guilt ~ dem + rep + age + educ + female + black + hispanic + income, data = as_tibble(simpson_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))

summary(simpson_rad_tune)
simpson_rad <- simpson_rad_tune$best.model
summary(simpson_rad)

fitted <- predict(simpson_rad, as_tibble(simpson_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <-  roc(as_tibble(simpson_split$test)$guilt, fitted$decision.values)
# plot(roc_line)

auc(roc_rad)




```

```{r OJ simpson random forest}
set.seed(1234)

simpson_rf<- randomForest(guilt ~ dem + rep + age + educ + female + black + hispanic + income, data =as_tibble(simpson_split$train))
                            
fitted <- predict(simpson_rf, na.omit(as_tibble(simpson_split$test)), type = "prob")[,2]

varImpPlot(simpson_rf)
roc_rf <- roc(as_tibble(simpson_split$test)$guilt, fitted)

plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
plot(roc_rf,print.auc = TRUE, col = "green", print.auc.y = .6, add = TRUE)
svm_auc = data.frame(auc(roc_line), auc(roc_poly),auc(roc_rad),auc(roc_rf))
svm_auc
```


* In this plot, we evaluate three SVM models including linear kernel svm,polynomial kernel svm with default setting (degree 3) and radial kernel svm. 
* The blue indicates the linear kernel svm, the area under the curve is 0.808. The red line indicates the polynomial kernel svm with degree 3, the area under the curve is 0.778. The orange indicates the radial kernel svm, the area under the curve is 0.781. The larger AUC indicates the better performance of models. In this dataset, we can see most of the svm approachs have a good performance as the AUC is larger than 0.778. The green line indicates the random forest and AUC is 0.806. And the svm with linear kernel and random forest performs  better than other svms in terms of AUC.



