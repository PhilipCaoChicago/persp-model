---
title: "Problem set #9: nonparametric methods and unsupervised learning"
author: "MACS 30100 - Perspectives on Computational Modeling"
date: "**Due Wednesday March 13th at 11:59pm**"
output:
  github_document:
    toc: true
---

# Attitudes towards feminists [3 points]

![](https://tbmwomenintheworld2016.files.wordpress.com/2016/11/rtx2pdge.jpg?w=800)

`feminist.csv` contains a selection of variables from the [2008 American National Election Studies survey](http://www.electionstudies.org/) that allow you to test competing factors that may influence attitudes towards feminists. The variables are coded as follows:

* `feminist` - feeling thermometer ranging from 0-100^[Feeling thermometers are a common metric in survey research used to gauge attitudes or feelings of warmth towards individuals and institutions. They range from 0-100, with 0 indicating extreme coldness and 100 indicating extreme warmth.]
* `female` - 1 if respondent is female, 0 if respondent is male
* `age` - age of respondent in years
* `dem` - 1 if respondent is a Democrat, 0 otherwise
* `rep` - 1 if respondent is a Republican, 0 otherwise
* `educ` - number of years of formal education completed by respondent
    * `17` - 17+ years (aka first year of graduate school and up)
* `income` - ordinal variable indicating respondent's income

    ```
    1. A. None or less than $2,999
    2. B. $3,000 -$4,999
    3. C. $5,000 -$7,499
    4. D. $7,500 -$9,999
    5. E. $10,000 -$10,999
    6. F. $11,000-$12,499
    7. G. $12,500-$14,999
    8. H. $15,000-$16,999
    9. J. $17,000-$19,999
    10. K. $20,000-$21,999
    11. M. $22,000-$24,999
    12. N. $25,000-$29,999
    13. P. $30,000-$34,999
    14. Q. $35,000-$39,999
    15. R. $40,000-$44,999
    16. S. $45,000-$49,999
    17. T. $50,000-$59,999
    18. U. $60,000-$74,999
    19. V. $75,000-$89,999
    20. W. $90,000-$99,999
    21. X. $100,000-$109,999
    22. Y. $110,000-$119,999
    23. Z. $120,000-$134,999
    24. AA. $135,000-$149,999
    25. BB. $150,000 and over
    ```

Estimate a series of models explaining/predicting attitudes towards feminists.

1. Split the data into a training and test set (70/30%).
1. Calculate the test MSE for KNN models with $K = 5, 10, 15, \dots, 100$, using whatever combination of variables you see fit. Which model produces the lowest test MSE?
1. Calculate the test MSE for weighted KNN models with $K = 5, 10, 15, \dots, 100$ using the same combination of variables as before. Which model produces the lowest test MSE?
1. Compare the test MSE for the best KNN/wKNN model(s) to the test MSE for the equivalent linear regression, decision tree, boosting, and random forest methods using the same combination of variables as before. Which performs the best? Why do you think this method performed the best, given your knowledge of how it works?

1.Speparate the data into 7:3 for train and test data.
```{r Q1 1a, echo=FALSE}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(titanic)
library(data.table)
library(tidytext)
library(tm)
library(knitr)
library(topicmodels)
library(e1071)
library(ggfortify)
library(pROC)
library(grid)
library(gridExtra)
library(FNN)
library(kknn)
library(purrr)
library(tree)
library(gbm)
library(randomForest)
library(caret)
library(ggdendro)
library(cowplot)

knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE
                      )
options(digits = 3)
theme_set(theme_minimal())

# import data
feminist <- fread("feminist.csv")
mhealth <- fread("mental_health.csv") %>% na.omit()
college <- fread("College.csv")
usarrests <- USArrests 
```

1. Separate the data
```{r Q1 1, echo=FALSE}
# Split data into train and test sets
fem_split <- resample_partition(feminist, c(test = 0.3, train = 0.7))
fem_train <- feminist[fem_split$train$idx]
fem_test <- feminist[fem_split$test$idx]

# Define a function for getting mse 
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
} 

```
Based on a random machanism, we create a train dataset and test dataset according to 7:3 portion.

2.Identify the model that produce lowest MSE.
```{r kable, echo=FALSE}
fem_mse_knn <- data_frame(k = seq(5, 100, by = 5),
                      knn = map(k, ~ knn.reg(select(fem_train, -feminist), y = fem_train$feminist,
                                             test = select(fem_test, -feminist), k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

# plot
ggplot(fem_mse_knn, aes(k, mse)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using all predictors',
       x = "K",
       y = "Test mean squared error")

# table
kable(fem_mse_knn %>% select(-knn) %>% arrange(by=mse) %>% head(3))


fem_mse_knn1 <- data_frame(k = seq(5, 100, by = 5),
                          knn = map(k, ~ knn.reg(select(fem_train, -c(feminist,age)), y = fem_train$feminist,
                                                 test = select(fem_test, -c(feminist,age)), k = .)),
                          mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

# plot
ggplot(fem_mse_knn1, aes(k, mse)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using p-1 predictors (-age)',
       x = "K",
       y = "Test mean squared error")

# table
kable(fem_mse_knn1 %>% select(-knn) %>% arrange(by=mse) %>% head(3))

fem_mse_knn2 <- data_frame(k = seq(5, 100, by = 5),
                           knn = map(k, ~ knn.reg(select(fem_train, -c(feminist,age,educ)), y = fem_train$feminist,
                                                  test = select(fem_test, -c(feminist,age,educ)), k = .)),
                           mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

# plot
ggplot(fem_mse_knn2, aes(k, mse)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using p-2 predictors (-age, -educ)',
       x = "K",
       y = "Test mean squared error")

# table
kable(fem_mse_knn2 %>% select(-knn) %>% arrange(by=mse) %>% head(3))


fem_mse_knn3 <- data_frame(k = seq(5, 100, by = 5),
                           knn = map(k, ~ knn.reg(select(fem_train, -c(feminist,age,educ,income)), y = fem_train$feminist,
                                                  test = select(fem_test, -c(feminist,age,educ,income)), k = .)),
                           mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

# plot
ggplot(fem_mse_knn3, aes(k, mse)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using p-3 predictors (-age, -educ, -income)',
       x = "K",
       y = "Test mean squared error")

# table
kable(fem_mse_knn3 %>% select(-knn) %>% arrange(by=mse) %>% head(3))

fem_mse_knn4 <- data_frame(k = seq(5, 100, by = 5),
                           knn = map(k, ~ knn.reg(select(fem_train, -c(feminist,age,educ,income,female)), y = fem_train$feminist,
                                                  test = select(fem_test, -c(feminist,age,educ,income,female)), k = .)),
                           mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

# plot
ggplot(fem_mse_knn3, aes(k, mse)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using p-4 predictors (-age, -educ, -income, -femlae)',
       x = "K",
       y = "Test mean squared error")

# table
kable(fem_mse_knn4 %>% select(-knn) %>% arrange(by=mse) %>% head(3))

# store the best 
fem_mse_knn_best <- fem_mse_knn3 %>% filter(mse == min(mse))
fem_mse_knn_best <- fem_mse_knn_best$mse

```
After comparing all the model, we choose KNN-3 model as the best model since it carries with the least MSE.

3.Identify the weighted model with lowest test MSE.
```{r Q1 3, echo=FALSE}
fem_mse_wknn3 <- data_frame(k = seq(5, 100, by = 5),
                            knn = map(k, ~ kknn(feminist ~ .-age-educ-income, train = fem_train, test = fem_test, k = .)),
                            mse = map_dbl(knn, ~ mean((fem_test$feminist - .$fitted.values)^2)))

# plot
ggplot(fem_mse_wknn3, aes(k, mse)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of weighted KNN models',
       subtitle = 'Using p-4 predictors (-age, -educ, -income)',
       x = "K",
       y = "Test mean squared error")

# table
kable(fem_mse_wknn3 %>% select(-knn) %>% arrange(by=mse) %>% head(3))

fem_mse_wknn_best <- fem_mse_wknn3 %>% filter(mse == min(mse)) 
fem_mse_wknn_best <- fem_mse_wknn_best$mse
```
From the analysis, we learn that model with $k=45$ gives the least test MSE. The following plot shows the MSE scores of models with $k$ values.

4. Compare the method, and choose the best current method.
```{r Q1 4, echo=FALSE}
set.seed(03152017) # For Random Forest Reproducibility
# Fit a linear regression model
fem_lin <- lm(feminist ~ .-age-educ-income, data=fem_train)
fem_mse_lin <- mse(fem_lin, fem_test)

# Fit a decision tree
fem_tree <- tree(feminist ~ .-age-educ-income, data=fem_train)
fem_mse_tree <- mse(fem_tree, fem_test)

# Fit a boosting model
fem_bst <- gbm(feminist ~ .-age-educ-income, data=fem_train, n.trees=5000, distribution='gaussian')
fem_mse_bst <- mean((fem_test$feminist - predict(fem_bst, newdata=as_tibble(fem_test), n.trees=5000))^2)

# Fit a random forest model 
fem_rf <- randomForest(feminist ~ .-age-educ-income, data=fem_train, ntree = 5000)
fem_mse_rf <- mse(fem_rf, fem_test)


# Compare all
Models <- c('KNN', 'Weighted KNN', 'Linear regression', 'Decision tree', 'Boosting', 'Random Forest')
MSE <- c(fem_mse_knn_best, fem_mse_wknn_best, fem_mse_lin, fem_mse_tree, fem_mse_bst, fem_mse_rf)
kable(data.frame(Models, MSE) %>%  arrange(by=MSE), caption='Comparison of different models')
```
Based on the our analysis, I think th boosting method is the best since we ontain the least MSE. Compare to other method, this method is optimal due to the process of boosting iteratively works to reduce the size of residuals with each tree created. However, the method may be weak at overcome the overfitting.

# Voter turnout and depression [2 points]

The 1998 General Social Survey included several questions about the respondent's mental health. `mental_health.csv` reports several important variables from this survey.

* `vote96` - 1 if the respondent voted in the 1996 presidential election, 0 otherwise
* `mhealth_sum` - index variable which assesses the respondent's mental health, ranging from 0 (an individual with no depressed mood) to 9 (an individual with the most severe depressed mood)^[The variable is an index which combines responses to four different questions: "In the past 30
days, how often did you feel: 1) so sad nothing could cheer you up, 2) hopeless, 3) that everything was an effort, and 4) worthless?" Valid responses are none of the time, a little of the time, some of the time, most of the time, and all of the time.]
* `age` - age of the respondent
* `educ` - Number of years of formal education completed by the respondent
* `black` - 1 if the respondent is black, 0 otherwise
* `female` - 1 if the respondent is female, 0 if male
* `married` - 1 if the respondent is currently married, 0 otherwise
* `inc10` - Family income, in \$10,000s

Estimate a series of models explaining/predicting voter turnout.

1. Split the data into a training and test set (70/30).
1. Calculate the test error rate for KNN models with $K = 1,2,\dots,10$, using whatever combination of variables you see fit. Which model produces the lowest test MSE?
1. Calculate the test error rate for weighted KNN models with $K = 1,2,\dots,10$ using the same combination of variables as before. Which model produces the lowest test error rate?
1. Compare the test error rate for the best KNN/wKNN model(s) to the test error rate for the equivalent logistic regression, decision tree, boosting, random forest, and SVM methods using the same combination of variables as before. Which performs the best? Why do you think this method performed the best, given your knowledge of how it works?

1. Sperate the data.
```{r Q2 1, echo=FALSE}
mhe_split <- resample_partition(mhealth, c(test = 0.3, train = 0.7))
mhe_train <- mhealth[mhe_split$train$idx]
mhe_test <- mhealth[mhe_split$test$idx]

```
According to a random machanism, we separate our data into 7:3.



2.Identify the model with lowest test MSE.
```{r Q2 2, echo=FALSE}
mhe_err_knn <- data_frame(k = seq(1,10),
                          knn = map(k, ~ knn.reg(select(mhe_train, -vote96), y = mhe_train$vote96,
                                                 test = select(mhe_test, -vote96), k = .)),
                          err.rate = map_dbl(knn, ~ mean(.$pred != mhe_test$vote96)))

# plot
ggplot(mhe_err_knn, aes(k, err.rate)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using all predictors',
       x = "K",
       y = "Test error rate")

# table
kable(mhe_err_knn %>% select(-knn) %>% arrange(by=err.rate) %>% head(3))

```
After comparing multiple KNN models with different settings, we find that least possible test error is with $k=1$ situation.



3. Identify the model with lowest test error rate.
```{r Q2 3, echo=FALSE}
mhe_err_knn1 <- data_frame(k = seq(1,10),
                          knn = map(k, ~ knn.reg(select(mhe_train, -vote96-mhealth_sum), y = mhe_train$vote96,
                                                 test = select(mhe_test, -vote96-mhealth_sum), k = .)),
                          err.rate = map_dbl(knn, ~ mean(.$pred != mhe_test$vote96)))


ggplot(mhe_err_knn1, aes(k, err.rate)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using p-1 predictors (-mhealth_sum)',
       x = "K",
       y = "Test error rate")


kable(mhe_err_knn1 %>% select(-knn) %>% arrange(by=err.rate) %>% head(3))
```
We test a weighted model with multiple settings. From the analysis, the weighted KNN model with $k=1$ give the least test error rate. 

4.Compare the test error rate for the best KNN/wKNN model(s).
```{r Q2 4, echo=FALSE}
mhe_err_knn2 <- data_frame(k = seq(1,10),
                           knn = map(k, ~ knn.reg(select(mhe_train, -vote96-mhealth_sum-inc10), y = mhe_train$vote96,
                                                  test = select(mhe_test, -vote96-mhealth_sum-inc10), k = .)),
                           err.rate = map_dbl(knn, ~ mean(.$pred != mhe_test$vote96)))

# plot
ggplot(mhe_err_knn2, aes(k, err.rate)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of KNN models',
       subtitle = 'Using p-2 predictors (-mhealth_sum, -inc10)',
       x = "K",
       y = "Test error rate")

kable(mhe_err_knn2 %>% select(-knn) %>% arrange(by=err.rate) %>% head(3))


mhe_err_knn_best <- mhe_err_knn1 %>% filter(err.rate == min(err.rate))
mhe_err_knn_best <- mhe_err_knn_best$err.rate
```

```{r Q2 4a, echo=FALSE}
# wKNN of p-1 predictors (-mhealth_sum), k = 1, err.rate = 0.37249
mhe_err_wknn1 <- data_frame(k = seq(1,10),
                            knn = map(k, ~ kknn(vote96 ~ .-mhealth_sum, train = mhe_train, test = mhe_test, k = .)),
                            err.rate = map_dbl(knn, ~ mean(.$fitted.values != mhe_test$vote96)))

# plot
ggplot(mhe_err_wknn1, aes(k, err.rate)) +
  geom_line() +
  geom_point() +
  labs(title = 'Comparison of weighted KNN models',
       subtitle = 'Using p-1 predictors (-mhealth_sum)',
       x = "K",
       y = "Test error rate")

# table
kable(mhe_err_wknn1 %>% select(-knn) %>% arrange(by=err.rate) %>% head(3))

mhe_err_wknn_best <- mhe_err_wknn1 %>% filter(err.rate == min(err.rate))
mhe_err_wknn_best <- mhe_err_wknn_best$err.rate
```

```{r pressure, echo=FALSE}
# Fit a logistic regression model
mhe_logit <- glm(as.factor(vote96) ~ .-mhealth_sum, data = mhe_train, family='binomial')

accuracy <- mhe_train %>%
  add_predictions(mhe_logit) %>%
  mutate(pred = exp(pred) / (1 + exp(pred)),
         pred = as.numeric(pred > .5))

mhe_err_logit <- mean(accuracy$vote96 != accuracy$pred, na.rm = TRUE)

# Fit a decision tree
mhe_tree <- tree(as.factor(vote96) ~ .-mhealth_sum, data = mhe_train)
mhe_err_tree <- mean(predict(mhe_tree, mhe_test, type = "class") != mhe_test$vote96, na.rm = TRUE)

# Fit a boosting model
mhe_bst <- gbm(vote96 ~ .-mhealth_sum, data = mhe_train, n.trees = 5000, distribution='bernoulli')
mhe_err_bst <- mean(ifelse(predict(mhe_bst, mhe_test, n.trees = 5000, type = 'response') > .5, 1, 0) != mhe_test$vote96)

# Fit a random forest model
mhe_rf <- randomForest(vote96 ~ .-mhealth_sum, data = mhe_train, ntree = 5000)
mhe_err_rf <- mean(predict(mhe_rf, mhe_test) != mhe_test$vote96)

# Fit a SVM model
mhe_svm <- svm(vote96 ~ .-mhealth_sum, data = mhe_train, type='C-classification')
mhe_err_svm <- mean(predict(mhe_svm, mhe_test) != mhe_test$vote96)


# Compare all
Models <- c('KNN', 'Weighted KNN', 'Logistic regression', 'Decision tree', 'Boosting', 'Random Forest', 'SVM')
Err.rate <- c(mhe_err_knn_best, mhe_err_wknn_best, mhe_err_logit, mhe_err_tree, mhe_err_bst, mhe_err_rf, mhe_err_svm)
kable(data.frame(Models, Err.rate) %>% arrange(by=Err.rate))
```
After compaing multiple model, we choose KNN model as the best model since it has the lowest test error.

# Colleges [2 points]

The `College` dataset in the `ISLR` library contains statistics for a large number of U.S. colleges from the 1995 issue of U.S. News and World Report.

* `Private` - A factor with levels `No` and `Yes` indicating private or public university.
* `Apps` - Number of applications received.
* `Accept` - Number of applications accepted.
* `Enroll` - Number of new students enrolled.
* `Top10perc` - Percent of new students from top 10% of H.S. class.
* `Top25perc` - Percent of new students from top 25% of H.S. class.
* `F.Undergrad` - Number of fulltime undergraduates.
* `P.Undergrad` - Number of parttime undergraduates.
* `Outstate` - Out-of-state tuition.
* `Room.Board` - Room and board costs.
* `Books` - Estimated book costs.
* `Personal` - Estimated personal spending.
* `PhD` - Percent of faculty with Ph.D.'s.
* `Terminal` - Percent of faculty with terminal degrees.
* `S.F.Ratio` - Student/faculty ratio.
* `perc.alumni` - Percent of alumni who donate.
* `Expend` - Instructional expenditure per student.
* `Grad.Rate` - Graduation rate.

Perform PCA analysis on the college dataset and plot the first two principal components. Describe the results. What variables appear strongly correlated on the first principal component? What about the second principal component?

# Clustering states [3 points]

The `USArrests` dataset contains 50 observations (one for each state) from 1973 with variables on crime statistics:

* `Murder` - Murder arrests (per 100,000)
* `Assault` - Assault arrests (per 100,000)
* `Rape` - Rape arrests (per 100,000)
* `UrbanPop` - Percent urban population

1. Perform PCA on the dataset and plot the observations on the first and second principal components.
1. Perform $K$-means clustering with $K=2$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.
1. Perform $K$-means clustering with $K=4$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.
1. Perform $K$-means clustering with $K=3$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.
1. Perform $K$-means clustering with $K=3$ on the first two principal components score vectors, rather than the raw data. Describe your results and compare them to the clustering results with $K=3$ based on the raw data.
1. Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.
1. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?
1. Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation $1$. What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

1. Perform PCA analysis.
```{r Q3 1, echo=FALSE}
# convert Private into numerical values 
college$Private <- ifelse(college$Private == 'Yes', 1, 0)

# Perform PCA
col_pr.out <- prcomp(college, scale=TRUE)
# Check the result, first 5 principal components 
kable(col_pr.out$rotation[,1:5])

autoplot(col_pr.out, data = cbind(rownames(college), data.frame(college)),
         shape = FALSE, label = TRUE, label.size = 3,
         loadings = TRUE, loadings.label = TRUE, loadings.label.size = 4) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = 'Principal Component Analysis: College data')
# PCA
usa_pr.out <-  prcomp(usarrests, scale=TRUE)
# Check the result
kable(usa_pr.out$rotation)
# plot
autoplot(usa_pr.out, data = usarrests,
         shape = FALSE, label = TRUE, label.size = 3,
         loadings = TRUE, loadings.label = TRUE, loadings.label.size = 4) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = 'Principal Component Analysis: USArrest data')

```
Principal component analysis gives a graph about dimension deduction.



2. Perform $K$-means clustering with $K=2$.
```{r Q3 2, echo=FALSE}
usa_kmeans2 <- kmeans(usarrests, 2, nstart = 20)
pred2 <- as.factor(usa_kmeans2$cluster)

# plot
autoplot(usa_pr.out, data = cbind(usarrests, pred2), 
         shape = FALSE, label = TRUE, label.size = 3, label.colour = 'pred2',
         loadings = TRUE, loadings.label = TRUE, loadings.label.size = 4) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = 'Principal Component Analysis: USArrest data',
       subtitle = 'K-means clustering with k = 2') +
  scale_color_discrete(name = 'Clusters')
```
We set the $centers = 2$.


3. Perform $K$-means clustering with $K=4$.
```{r Q3 3, echo=FALSE}
usa_kmeans4 <- kmeans(USArrests, 4, nstart = 20)
pred4 <- as.factor(usa_kmeans4$cluster)

# plot
autoplot(usa_pr.out, data = cbind(usarrests, pred4), 
         shape = FALSE, label = TRUE, label.size = 3, label.colour = 'pred4',
         loadings = TRUE, loadings.label = TRUE, loadings.label.size = 4) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = 'Principal Component Analysis: USArrest data',
       subtitle = 'K-means clustering with k = 4') +
  scale_color_discrete(name = 'Clusters')
```
We set the $centers = 4$.


4. Perform $K$-means clustering with $K=3$.
```{r Q3 4, echo=FALSE}
usa_kmeans3 <- kmeans(USArrests, 3, nstart = 20)
pred3 <- as.factor(usa_kmeans3$cluster)

# plot
autoplot(usa_pr.out, data = cbind(usarrests, pred3), 
         shape = FALSE, label = TRUE, label.size = 3, label.colour = 'pred3',
         loadings = TRUE, loadings.label = TRUE, loadings.label.size = 4) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = 'Principal Component Analysis: USArrest data',
       subtitle = 'K-means clustering with k = 3') +
  scale_color_discrete(name = 'Clusters')
```
We set the $centers = 3$.


5. Perform $K$-means clustering with $K=3$ on the first two principal components score vectors, rather than the raw data.
```{r Q3 5, echo=FALSE}
usa_kmeans3_pca <- kmeans(data.frame(usa_pr.out$x)[,1:2], 3, nstart=20)
pred3_pca <- as.factor(usa_kmeans3_pca$cluster)
# plot
autoplot(usa_pr.out, data=cbind(data.frame(usa_pr.out$x)[,1:2], pred3_pca),
         shape = FALSE, label = TRUE, label.size = 3, label.colour = 'pred3_pca',
         loadings = TRUE, loadings.label = TRUE, loadings.label.size = 3) +
  geom_vline(xintercept = 0, linetype = 2) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = 'Principal Component Analysis: USArrest data',
       subtitle = 'K-means clustering with k = 3, on principal component score vectors') +
  scale_color_discrete(name = 'Clusters')

```
We set the $centers = 3$.


6. Using hierarchical clustering with complete linkage and Euclidean distance,
```{r Q3 6, echo=FALSE}
usa_hc.complete <- hclust(dist(usarrests), method = 'complete')
ggdendrogram(usa_hc.complete)

h <- 150
# extract dendro data
hcdata <- dendro_data(usa_hc.complete)
hclabs <- label(hcdata) %>%
  left_join(data_frame(label = as.factor(rownames(usarrests)),
                       cl = as.factor(cutree(usa_hc.complete, h = h))))

```

7. Cut the dendrogram at a height that results in three distinct clusters.
```{r Q3 7, echo=FALSE}

ggdendrogram(usa_hc.complete, labels = FALSE) +
  geom_text(data = hclabs,
            aes(label = label, x = x, y = 0, color = cl),
            vjust = .5, angle = 90)+
  geom_hline(yintercept = h, linetype = 2) +
  theme(axis.text.x = element_blank(),
        legend.position = "none")
```

8. Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation $1$.
```{r Q3 8, echo=FALSE}
# scaling
usarrests2 <- scale(usarrests) %>% as.data.frame()

# perform hierarchical clustering
usa2_hc.complete <- hclust(dist(usarrests2), method = 'complete')
ggdendrogram(usa2_hc.complete)
```
From multiple comparison, we learn that different variables may have different scales. Based on current analysis, we can not tell which method is better and future choice may have to be based on the practical situatioin.

# Submission instructions

Assignment submission will work the same as earlier assignments. Submit your work as a pull request before the start of class on Monday. Store it in the same locations as you've been using. However the format of your submission should follow the procedures outlined below.

## If you use R

Submit your assignment as a single [R Markdown document](http://rmarkdown.rstudio.com/). R Markdown is similar to Juptyer Notebooks and compiles all your code, output, and written analysis in a single reproducible file.

## If you use Python

Either:

1. Submit your assignment following the same procedures as required by Dr. Evans. Submit a Python script containing all your code, plus a LaTeX generated PDF document with your results and substantive analysis.
1. Submit your assignment as a single Jupyter Notebook with your code, output, and written analysis compiled there.

