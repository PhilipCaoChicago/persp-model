{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Problem set #8: tree-based methods and support vector machines\"\nauthor: \"Bobae Kang\"\ndate: \"March 6, 2017\"\noutput:\n  github_document:\n    toc: true\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo=FALSE, warning = FALSE, message=FALSE, cache=TRUE)\n\n# import packages\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(modelr)\nlibrary(broom)\nlibrary(tree)\nlibrary(randomForest)\nlibrary(ggdendro) #devtools::install_github(\"bensoltoff/ggdendro\")\nlibrary(forcats)\nlibrary(gbm)\nlibrary(pROC)\nlibrary(e1071)\nlibrary(rmarkdown)\nlibrary(knitr)\n\n# import data\nbiden <- fread('data/biden.csv')\nmhealth <- fread('data/mental_health.csv')\nsimpson <- fread('data/simpson.csv')\n\n# set seed for reproducibility\nset.seed(0)\n\n# define a function to get mse\nmse <- function(model, data) {\n  x <- modelr:::residuals(model, data)\n  mean(x ^ 2, na.rm = TRUE)\n}\n\nerr.rate.tree <- function(model, data) {\n  data <- as_tibble(data)\n  response <- as.character(model$terms[[2]])\n  \n  pred <- predict(model, newdata = data, type = \"class\")\n  actual <- data[[response]]\n  \n  return(mean(pred != actual, na.rm = TRUE))\n}\n```\n\n# Part 1: Sexy Joe Biden (redux times two) [3 points]\nIn this part, I use and compare a variety of tree-based models on the `biden.csv` data. In doing so, I use the cross-validation approach, splitting the original data randomly into a training set (70% of all observations) and a test/validation set (30% of all observations).\n```{r Part 1: setup}\n# split into training and test sets\nbiden_split <- resample_partition(biden, c(test = 0.3, train = 0.7))\nbiden_train <- biden[biden_split$train$idx]\nbiden_test <- biden[biden_split$test$idx]\n```\n\n## Fit a decision tree\nFist, I grow a decision tree using the training data. `biden` is the response variable and other variables are predictors. I have set seed to be 0 for reproducibility. Without any input for control argument, the algorithm chose a model with three terminal nodes. The model predicts that, if an observation is democrat, the themometer score is 74.49. For observations that are not not democrat, the model predicts that the themometer score is 44.17 if an observation is republican and 57.42 otherwise. Its mean squared error (MSE) on the test set is 387.9136.     \n```{r Part 1: a decision tree}\nbiden_tree1 <- tree(biden~., data = biden_train)\n\n# look into the fitted model\nsummary(biden_tree1)\n\n# test MSE\nprint('Test MSE:')\nmse(biden_tree1, biden_test)\n\n# plot the tree\ntree_data <- dendro_data(biden_tree1)\nggplot(segment(tree_data)) +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), \n               alpha = 0.5) +\n  geom_text(data = label(tree_data), \n            aes(x = x, y = y, label=label), vjust = -0.5, size = 3) +\n  geom_text(data = leaf_label(tree_data), \n            aes(x = x, y = y, label=label), vjust = 0.5, size = 3) +\n  theme_dendro() +\n  labs(title = \"Biden feeling thermometer tree\",\n       subtitle = \"All predictors\")\n\n```\n\n## Fit another decision tree\nWhat if I let the model to grow more branches? In this second tree with different `control` options, there are 196 terminal nodes. Again, being democrat is responsible for the first split as in the previous tree. However, in the plot, it is difficult to identify what the predictios are at terminal nodes. The MSE is 528.6294, which is significantly larger than the MSE of the previous tree. More splits seemingly lead to worse performance. \n```{r Part 1: another tree}\nbiden_tree2 <- tree(biden ~ ., data = biden_train,\n                    control = tree.control(nobs = nrow(biden_train),\n                              mindev = 0))\n\n# look into the fitted model\nsummary(biden_tree2)\n\n# test MSE\nprint('Test MSE:')\nmse(biden_tree2, biden_test)\n\n# plot the tree\ntree_data <- dendro_data(biden_tree2)\nggplot(segment(tree_data)) +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), \n               alpha = 0.5) +\n  geom_text(data = label(tree_data), \n            aes(x = x, y = y, label=label), vjust = -0.5, size = 3) +\n  geom_text(data = leaf_label(tree_data), \n            aes(x = x, y = y, label=label), vjust = 0.5, size = 3) +\n  theme_dendro() +\n  labs(title = \"Biden feeling thermometer tree\",\n       subtitle = \"All predictors\")\n```\n\n## Cross-validation\nTo find out the optimal number of terminal nodes, I try the 10-fold cross-validation approach. The plot illustrates the MSE values for different number of terminal nodes. We find that a tree with three terminal nodes has the least MSE value. This is the first tree I fitted above! The MSE score then increases with more terminal nodes. \n```{r Part 1: CV}\n# generate 10-fold CV trees\nbiden_cv <- crossv_kfold(biden, k = 10) %>%\n  mutate(tree = map(train, ~ tree(biden ~ ., data = .,\n                                  control = tree.control(nobs = nrow(biden), mindev = 0))))\n\n# calculate each possible prune result for each fold\nbiden_cv <- expand.grid(biden_cv$.id, 2:10) %>%\n  as_tibble() %>%\n  mutate(Var2 = as.numeric(Var2)) %>%\n  rename(.id = Var1,\n         k = Var2) %>%\n  left_join(biden_cv) %>%\n  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),\n         mse = map2_dbl(prune, test, mse))\n\nbiden_cv %>%\n  select(k, mse) %>%\n  group_by(k) %>%\n  summarize(test_mse = mean(mse),\n            sd = sd(mse, na.rm = TRUE)) %>%\n  ggplot(aes(k, test_mse)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Number of terminal nodes\",\n       y = \"Test MSE\")\n```\n\n## Bagging\nHere, I use bagging approach, growing total 5000 trees. At each split, all five predictors are considered. The MSE score on the test set is 504.1033. The plot shows the effectiveness of all predictors at decreasing the gini index score. Overall, `age` has contributed the most to decreasing the gini index of the model. \n```{r Part 1: bagging}\nbiden_bag <- randomForest(biden ~ ., data = biden_test,\n                          mtry = 5, ntree = 5000)\n\n# look into the fitted model\nbiden_bag\n\n# test MSE\nprint('Test MSE:')\nmse(biden_bag, biden_test)\n\n# plot bagging\ndata_frame(var = rownames(importance(biden_bag)),\n           MeanDecreaseGini = importance(biden_bag)[,1]) %>%\n  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%\n  ggplot(aes(var, MeanDecreaseGini)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting Biden feeling thermometer\",\n       subtitle = \"Bagging\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\")\n```\n\n## Random Forest\nNow, I turn to the random forest approach. Again, I grow 5000 tress. In this case, only one randomly selected predictor is considered at each split. Its test MSE score, 398.9227, is notably lower than that of the bagging model. The plot compares baggning and random forest in terms of the contribution of each predictor to reducing the Gini Index. While `age` makes the greatest contribution in the bagging model, in the random forest model, `dem` makes the greatest contribution.     \n```{r Part 1: random forest}\nbiden_rf <- randomForest(biden ~ ., data = biden_test,\n                          ntree = 5000)\n\n# look into the model\nbiden_rf\n\n# test MSE\nprint('Test MSE:')\nmse(biden_rf, biden_test)\n\n# plot both\ndata_frame(var = rownames(importance(biden_rf)),\n           `Random forest` = importance(biden_rf)[,1]) %>%\n  left_join(data_frame(var = rownames(importance(biden_rf)),\n                       Bagging = importance(biden_bag)[,1])) %>%\n  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%\n  gather(model, gini, -var) %>%\n  ggplot(aes(var, gini, color = model)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting Biden feeling thermometer\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\",\n       color = \"Method\")\n```\n\n## Boosting\nHere, I fit three models with boosting approach, growing 5000 trees for each model. Each of these models have a different value for the shrinkage parmeter, $\\lambda$. The first model has $\\lambda$ = 0.001, the second model has $\\lambda$ = 0.01, and the final model has $\\lambda$ = 0.1. The plot shows the MSE values for each tree with different number of trees. The plot shows changes in test MSE values by the number of trees. With the $\\lambda$ = 0.001 model the test MSE keeps decreasing smoothly, even at n.tree = 5000. On the other hand test MSE for the $\\lambda$ = 0.1 model remains almost unchanged after n.tree > 2000. Overall, however, the test MSE of the $\\lambda$ = 0.01 model is lower at all points than that of the $\\lambda$ = 0.001 model. Finally, the $\\lambda$ = 0.1 model shows a very distinct pattern: with the very low number of trees, its test MSE reaches its minimum (which, by the way, is the lowest of all for all three models) and continues to increase.   \n```{r Part 1: boosting}\n# fit boosting models\nbiden_boosting_models <- list(\"boosting_shrinkage.001\" = gbm(biden ~ ., data = biden_train,\n                                                               n.trees = 5000, shrinkage = 0.001),\n                              \"boosting_shrinkage.01\" = gbm(biden ~ ., data = biden_train,\n                                                              n.trees = 5000, shrinkage = 0.01),\n                              \"boosting_shrinkage.1\" = gbm(biden ~ ., data = biden_train,\n                                                             n.trees = 5000, shrinkage = 0.1))\n\ndata_frame(shrinkage.001 = predict(biden_boosting_models$boosting_shrinkage.001,\n                                   newdata = as_tibble(biden_test), n.trees = 1:5000) %>%\n             apply(2, function(x) (x - as_tibble(biden_test)$biden)^2) %>%\n             apply(2, mean),\n           shrinkage.01 = predict(biden_boosting_models$boosting_shrinkage.01,\n                                  newdata = as_tibble(biden_test), n.trees = 1:5000) %>%\n             apply(2, function(x) (x - as_tibble(biden_test)$biden)^2) %>%\n             apply(2, mean),\n           shrinkage.1 = predict(biden_boosting_models$boosting_shrinkage.1,\n                                 newdata = as_tibble(biden_test), n.trees = 1:5000) %>%\n             apply(2, function(x) (x - as_tibble(biden_test)$biden)^2) %>%\n             apply(2, mean)) -> boost_test_mse\n\n# plot test MSEs of all three models\nboost_test_mse %>%\n  mutate(id = row_number()) %>%\n  mutate_each(funs(cummean(.)), shrinkage.001:shrinkage.01) %>%\n  gather(model, err, -id) %>%\n  ggplot(aes(id, err, color = model)) +\n  geom_line() +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(title = 'Boosting MSE by shrinkage parameters',\n       x = \"Number of trees\",\n       y = \"Test error\",\n       color = 'Model')\n```\n\n\n# Part 2: Modeling voter turnout [3 points]\nIn this part, I fit five tree-based models and five support vector machine(SVM)-based models to the `mhealth.csv` data. In doing so, I use the cross-validation approach and split the original data into a training set (70% of all observations) and a test/validation set (30% of all observations.)\n```{r Part 2: setup}\n# Part 2: Prepare the data\nmhealth <- mhealth %>%\n  # remove rows with missing values\n  na.omit() %>% \n  # make categorical variables into factors\n  mutate(vote96 = factor(vote96, levels = 0:1, labels = c(\"No\", \"Yes\")),\n         black = factor(black, levels = 0:1, labels = c(\"not_black\", \"black\")),\n         female = factor(female, levels = 0:1, labels = c('male', 'female')),\n         married = factor(married, levels = 0:1, labels = c('unmarried', 'married')))\n\n# split into training and validation sets\nmhealth_split <- resample_partition(mhealth, c(test = 0.3, train = 0.7))\nmhealth_train <- mhealth[mhealth_split$train$idx,]\nmhealth_test <- mhealth[mhealth_split$test$idx,]\n```\n\n## Fit tree-based models\nHere I fit and compare the following five tree-based models:\n\n* A pruned decision tree (the best number of terminal nodes = 5 is chosen using the 10-fold cv)\n* Bagging (5000 trees)\n* Random forest (5000 trees; 2 predictors tried at each split)\n* Boosting (5000 trees; shrinkage parameter = 0.001)\n* Boosting (5000 trees; shrinkage parameter = 0.1)\n\n```{r Part 2: tree-based model 1}\n# a decision tree\nmhealth_tree <- tree(vote96 ~ .,\n                     data = mhealth_train,\n                     control = tree.control(nobs = nrow(mhealth_train), mindev = .001))\n\n# 10-fold CV trees to find best tree\nmhealth_cv <- mhealth_train %>%\n  crossv_kfold(k = 10) %>%\n  mutate(tree = map(train, ~ tree(vote96 ~ ., data = .,\n    control = tree.control(nobs = nrow(mhealth_train), mindev = .001))))\n\n# calculate each possible prune result for each fold\nmhealth_cv <-\n  expand.grid(mhealth_cv$.id,\n              seq(from = 2, to = 10)) %>%\n  as_tibble() %>%\n  mutate(Var2 = as.numeric(Var2)) %>%\n  rename(.id = Var1,\n         k = Var2) %>%\n  left_join(mhealth_cv) %>%\n  mutate(prune = map2(tree, k, ~ prune.misclass(.x, best = .y)),\n         mse = map2_dbl(prune, test, err.rate.tree)) %>%\n  group_by(k) %>%\n  summarize(test_mse = mean(mse),\n            sd = sd(mse, na.rm = TRUE))\n\nmhealth_tree_pruned <- prune.tree(mhealth_tree, best = 5)\nsummary(mhealth_tree_pruned)\n```\n\n```{r Part 2: tree-based model 2}\n# bagging (n.tree = 5000)\nmhealth_bag <- randomForest(vote96 ~ ., data = mhealth_train,\n                          mtry = 7, ntree = 5000)\nmhealth_bag\n```\n\n```{r Part 2: tree-based model 3}\n# random forest (n.tree = 5000)\nmhealth_rf <- randomForest(vote96 ~ ., data = mhealth_train,\n                         ntree = 5000)\nmhealth_rf\n```\n\n```{r Part 2: tree-based model 4}\n# Re-Prepare the data\nmhealth_train2 <- mhealth_train\nmhealth_test2 <- mhealth_test\nmhealth_train2$vote96 <- (mhealth_train2$vote96 == 'Yes')*1\nmhealth_test2$vote96 <- (mhealth_test2$vote96 == 'Yes')*1\n\nmhealth_bst.001 <- gbm(vote96 ~ ., data = mhealth_train2, n.trees = 5000, shrinkage = 0.001)\nmhealth_bst.001\n```\n\n```{r Part 2: tree-based model 5}\nmhealth_bst.1 <- gbm(vote96 ~ ., data = mhealth_train2, n.trees = 5000, shrinkage = 0.1)\nmhealth_bst.1\n```\n\n## Compare tree-based models\nI now compare these tree-based models using 1) error rate and 2) ROC/AUC. When I compare test error rates of the five models, the boosting model with shrinkage = 0.001 appears to be the best approach, with the lowest test error rate = 0.2664756. The worst model was bagging, with the highest test error rate = 0.3065903.\n```{r Part 2: compare tree-based model, err rate}\n# pruned tree\nerr_rate_tree <- err.rate.tree(mhealth_tree_pruned, mhealth_test)\n\n# bagging\npred_bag <- predict(mhealth_bag, newdata = mhealth_test, type = 'response', n.trees=5000) %>% as.character()\nactual1 <- as.character(mhealth_test$vote96)\nerr_rate_bag <- mean(pred_bag != actual1, na.rm = TRUE)\n\n# random forest\npred_rf <- predict(mhealth_rf, newdata = mhealth_test, type = 'response', n.trees=5000) %>% as.character()\nerr_rate_rf <- mean(pred_rf != actual1, na.rm = TRUE)\n\n# boosting, shrinkage = 0.001\nactual2 <- as.character(mhealth_test2$vote96)\npred_bst.001 <- predict(mhealth_bst.001, newdata = mhealth_test2, type = 'response', n.trees=5000) %>% round() %>% as.character()\nerr_rate_bst.001 <- mean(pred_bst.001 != actual2, na.rm = TRUE)\n\n# boosting, shrinkage = 0.01\npred_bst.1 <- predict(mhealth_bst.1, newdata = mhealth_test2, type = 'response', n.trees=5000) %>% round() %>% as.character()\nerr_rate_bst.1 <- mean(pred_bst.1 != actual2, na.rm = TRUE)\n\nTree_model = c('Pruned tree', 'Bagging', 'Random Forest', 'Boosting, 0.001', 'Boosing, 0.1')\nTest_error_rate = c(err_rate_tree, err_rate_bag, err_rate_rf, err_rate_bst.001, err_rate_bst.1)\ndata.frame(Tree_model, Test_error_rate)\n```\n\nThen I compare the area under the curve for the ROC curves of the five tree-based models. The following plot shows the ROC curves of all five models, with the corresponding AUC scores. Here, again, boosting with shrinkage = 0.001 (green) appears to be the best model with the highest AUC score = 0.743. The second best is boosting with shrinkage = 0.1 (purple) with the AUC score = 0.734. The worst is the pruned tree model (red), with the AUC score = 0.593.\n```{r Part 2: compare tree-based model, roc auc}\n# tree (pruned)\nfitted_tree <- predict(mhealth_tree_pruned, as_tibble(mhealth_test), type = \"class\")\ntree_err <- mean(as_tibble(mhealth_test)$vote96 != fitted_tree)\nroc_tree <- roc(as.numeric(as_tibble(mhealth_test)$vote96), as.numeric(fitted_tree))\n# bagging\nfitted_bag <- predict(mhealth_bag, as_tibble(mhealth_test), type = \"prob\")[,2]\nroc_bag <- roc(as_tibble(mhealth_test)$vote96, fitted_bag)\n# random forest\nfitted_rf <- predict(mhealth_rf, as_tibble(mhealth_test), type = \"prob\")[,2]\nroc_rf <- roc(as_tibble(mhealth_test)$vote96, fitted_rf)\n# boost, lambda = 0.001\nfitted_bst.001 <- predict(mhealth_bst.001, as_tibble(mhealth_test2), type = \"response\", n.trees=5000)\nroc_bst.001 <- roc(as_tibble(mhealth_test2)$vote96, fitted_bst.001)\n# boost, lambda = 0.1\nfitted_bst.1 <- predict(mhealth_bst.1, as_tibble(mhealth_test2), type = \"response\", n.trees=5000)\nroc_bst.1 <- roc(as_tibble(mhealth_test2)$vote96, fitted_bst.1)\n\n# plot ROC curves\nplot(roc_tree, print.auc = TRUE, col = \"red\", print.auc.x = .2)\nplot(roc_bag, print.auc = TRUE, col = \"blue\", print.auc.x = .2, print.auc.y = .4, add = TRUE)\nplot(roc_rf, print.auc = TRUE, col = \"orange\", print.auc.x = .2, print.auc.y = .3, add = TRUE)\nplot(roc_bst.001, print.auc = TRUE, col = \"green\", print.auc.x = .2, print.auc.y = .2, add = TRUE)\nplot(roc_bst.1, print.auc = TRUE, col = \"purple\", print.auc.x = .2, print.auc.y = .1, add = TRUE)\n```\n\n## Fit SVM models\nHere I fit and compare the following six SVM models:\n\n* SVM, kernel = 'linear'\n* sVM, kernel = 'polynomial'\n* SVM, kernel = 'radial'\n* Tuned SVM, kernel = 'linear'\n* Tuned sVM, kernel = 'polynomial'\n* Tuned SVM, kernel = 'radial'\n\n```{r Part 2: SVM model 1}\nmhealth_svm_lin <- svm(vote96 ~., data = mhealth_train, kernel = \"linear\", scale = FALSE, cost = 1)\nmhealth_svm_lin\n```\n\n```{r Part 2: SVM model 2}\nmhealth_svm_poly <- svm(vote96 ~., data = mhealth_train, kernel = \"polynomial\", scale = FALSE, cost = 1)\nmhealth_svm_poly\n```\n\n```{r Part 2: SVM model 3}\nmhealth_svm_rad <- svm(vote96 ~., data = mhealth_train, kernel = \"radial\", scale = FALSE, cost = 1)\nmhealth_svm_rad\n```\n\n```{r Part 2: SVM model 4}\nmh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mhealth_train),\n                    kernel = \"linear\",\n                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))\nmhealth_svm_ltuned <- mh_lin_tune$best.model\nmhealth_svm_ltuned\n```\n\n```{r Part 2: SVM model 5}\nmh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mhealth_train),\n                    kernel = \"polynomial\",\n                    range = list(cost = c(.001, .01, .1, 1, 10, 100)))\nmhealth_svm_ptuned <- mh_poly_tune$best.model\nmhealth_svm_ptuned\n```\n\n```{r Part 2: SVM model 6}\nmh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mhealth_train),\n                     kernel = \"radial\",\n                     range = list(cost = c(.001, .01, .1, 1, 10, 100)))\nmhealth_svm_rtuned <- mh_rad_tune$best.model\nmhealth_svm_rtuned\n```\n\n## Compare SVM models\nNow I compare SVM models using ROC/AUC. The following plot shows that the tuned model with radial kenel (cyan) is the best model with the highest AUC score: 0.737. The second best model is a tie: both models using the linear kernel (red and green) have the second highest AUC score: 0.736. In fact, as we have seen just above, they are the same model. That is, the best model with the linear kernel is the one with cost = 1 and 491 support vectors, which is identical to the first SVM model. The worst model is the untuned SVM with the polynomial kernel, with the lowest AUC score = 0.601.     \n```{r Part 2: compare SVM models}\n# not tuned\nfitted_lin <- predict(mhealth_svm_lin, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_line <- roc(as_tibble(mhealth_test)$vote96, fitted_lin$decision.values)\nfitted_poly <- predict(mhealth_svm_poly, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_poly <- roc(as_tibble(mhealth_test)$vote96, fitted_poly$decision.values)\nfitted_rad <- predict(mhealth_svm_rad, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_rad <- roc(as_tibble(mhealth_test)$vote96, fitted_rad$decision.values)\n# tuned\nfitted_ltuned <- predict(mhealth_svm_ltuned, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_ltuned <- roc(as_tibble(mhealth_test)$vote96, fitted_ltuned$decision.values)\nfitted_ptuned <- predict(mhealth_svm_ptuned, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_ptuned <- roc(as_tibble(mhealth_test)$vote96, fitted_ptuned$decision.values)\nfitted_rtuned <- predict(mhealth_svm_rtuned, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_rtuned <- roc(as_tibble(mhealth_test)$vote96, fitted_rtuned$decision.values)\n\nplot(roc_line, print.auc = TRUE, col = \"red\", print.auc.x = .2)\nplot(roc_poly, print.auc = TRUE, col = \"blue\", print.auc.x = .2, print.auc.y = .425, add = TRUE)\nplot(roc_rad, print.auc = TRUE, col = \"orange\", print.auc.x = .2, print.auc.y = .35, add = TRUE)\nplot(roc_ltuned, print.auc = TRUE, col = \"green\", print.auc.x = .2, print.auc.y = .275, add = TRUE)\nplot(roc_ptuned, print.auc = TRUE, col = \"purple\", print.auc.x = .2, print.auc.y = .2, add = TRUE)\nplot(roc_rtuned, print.auc = TRUE, col = \"cyan\", print.auc.x = .2, print.auc.y = .125, add = TRUE)\n```\n\n# Part 3: OJ Simpson [4 points]\nIn this part, I use two different approaches to the `simpson.csv` data: explanation and prediction. The goal of explanation is to understand the correlation between the response variable and the predictors. The goal of prediction, on the other hand, is to get best predctions for new observations.\n\n```{r Part 3: setup}\n# remove missing values\nsimpson <- simpson %>%\n  na.omit()\n\n# binarize categorical predictors, for prediction part\nsimpson_pred <- simpson %>%\n  model.matrix(~ ., data=.) %>% \n  as.data.table() %>%\n  select(-`(Intercept)`)\n# simplify colnames\ncolnames(simpson_pred)  <- c('guilt', 'dem', 'rep', 'ind', 'age', 'educ_hs', 'educ_not_hs', 'educ_ref', 'educ_some_col',\n                        'female', 'black', 'hispanic', 'inc_30to50', 'inc_50to75', 'inc_over75', 'inc_ref', 'inc_under15')\n\n# split data into train and test sets, for prediction part  \nsimpson_split <- resample_partition(simpson_pred, c(test = 0.3, train = 0.7))\nsimpson_train <- simpson_pred[simpson_split$train$idx]\nsimpson_test <- simpson_pred[simpson_split$test$idx]\n\n```\n\n\n## Explain \nHere, the goal is to explain an individual's race on their beliefs about OJ Simpson's guilt. I use logistic regression for this task because 1) `guilt` is a binary variable with two possible outcomes: guilt or not guilt and 2) logistic regression provides the coefficient for each independent variable, making it easier to understand the precise relationship between the dependent and independent variables. I fit two different logistic regression models where:\n1. Independent variables only include race-related variables: `black` and `hispanic`\n2. Independent variables  include all variables other than the dependent/response variable, `guilt`\n\nIn the first model, only `black` appears statistically significant, as the extremely low p-value for the coefficient (<2e-16) suggests. The coefficient for `black` is -3.11438, in terms of log-odds. In terms of odds, the exponentiating the coefficient gives 0.04440603. This indicates that,  holding other variables constant, being black leads to an average change in the odds that the responsdent thinks OJ Simpson was \"probabilty guilty\" by a multiplicative factor of 0.04440603. In terms of predicted probabilities, this corresponds to a multiplicative factor of 0.04440603 / (1 + 0.04440603) = 0.04251798 for being black holding other variables constant. That if the respondent is black, she is on avergae 4.25% more likely to think that OJ Simpson is \"probably guilty\" than a non-black respondent. Therefore, although the coefficient is statistically significant, it may not be substantively significant.\n```{r Part 3: explain1}\nsimpson_logit1 <- glm(guilt ~ black + hispanic, data = (simpson %>% na.omit()), family='binomial') # only race predictors\nsummary(simpson_logit1)\n```\n\nIn the second model, coefficients for the following variables are statistically significant: `rep`, `age`, `educHigh School Grad`, `educNOT A HIGH SCHOOL GRAD`, `female`, `black` and `incomeREFUSED/NO ANSWER` with p-values < 0.05. The coefficient for `black` is -2.923476, in terms of log-odds. In terms of odds, the exponentiating the coefficient gives 0.05374654. This indicates that,  holding other variables constant, being black leads to an average change in the odds that the responsdent thinks OJ Simpson was \"probabilty guilty\" by a multiplicative factor of 0.05374654. In terms of predicted probabilities, this corresponds to a multiplicative factor of 0.05374654 / (1 + 0.05374654) = 0.05100519 for being black holding other variables constant. That if the respondent is black, she is on avergae 5.1% more likely to think that OJ Simpson is \"probably guilty\" than a non-black respondent. Therefore, although the coefficient is statistically significant, it may not be substantively significant. The AIC score for the current model (1303.1) is lower than that of the previous model (1355.8). The lower AIC values makes the second regression model more preferable.   \n```{r Part 3: explain2}\nsimpson_logit2 <- glm(guilt ~ ., data = (simpson %>% na.omit()), family='binomial') # all predictors\nsummary(simpson_logit2)\n```\n\n## Predict\nNow, I fit and compare multiple models for prediciton. For this part, I split the data into training (70%) and test (30%) sets. The models I use here are the following:\n\n* Logistic regression\n* Random forest (n.trees = 5000)\n* Boosting (n.trees = 5000)\n* SVM (kernel = 'linear')\n* SVM (kernel = 'radial')\n\n```{r Part 3: predict}\nsimpson_logit <- glm(guilt ~ ., data = simpson_train, family='binomial') # logistic regression\nsimpson_rf <- randomForest(as.factor(guilt) ~ ., data = simpson_train, ntree = 5000)\nsimpson_bst <- gbm(guilt ~ ., data = simpson_train, n.trees = 5000)\nsimpson_svm_lin <- svm(guilt ~ ., data = simpson_train, kernel = \"linear\", scale = FALSE)\nsimpson_svm_rad <- svm(guilt ~ ., data = simpson_train, kernel = \"radial\", scale = FALSE)\n```\n\nTo find out the best model, I compare ROC/AUC scores of the models. Thefollowing plot shows both the ROC curves and the corresponding AUC scores of all six models. Based on the AUC scores, The best model is boosting with the AUC score: 0.826. The logistic regression model is the second best model with only a slightly lower AUC score: 0.823. The model with the lowest AUC score is the SVM with radial kenel: 0.773.  \n\n```{r Part 3: predict -- ROC AUC}\nfitted_logit <- predict(simpson_logit, as_tibble(simpson_test), type = \"response\")\nfitted_rf <- predict(simpson_rf, as_tibble(simpson_test), type = \"prob\")[,2]\nfitted_bst <- predict(simpson_bst, as_tibble(simpson_test), type = \"response\", n.trees=5000)\nfitted_svm_lin <- predict(simpson_svm_lin, as_tibble(simpson_test), decision.values = TRUE) %>% attributes\nfitted_svm_rad <- predict(simpson_svm_rad, as_tibble(simpson_test), decision.values = TRUE) %>% attributes\n\nroc_logit <- roc(as_tibble(simpson_test)$guilt, fitted_logit)\nroc_rf <- roc(as_tibble(simpson_test)$guilt, fitted_rf)\nroc_bst <- roc(as_tibble(simpson_test)$guilt, fitted_bst)\nroc_svm_lin <- roc(as_tibble(simpson_test)$guilt, as.numeric(fitted_svm_lin$decision.values))\nroc_svm_rad <- roc(as_tibble(simpson_test)$guilt, as.numeric(fitted_svm_rad$decision.values))\n\nplot(roc_logit, print.auc = TRUE, col = \"red\", print.auc.x = .2)\nplot(roc_rf, print.auc = TRUE, col = \"blue\", print.auc.x = .2, print.auc.y = .425, add = TRUE)\nplot(roc_bst, print.auc = TRUE, col = \"orange\", print.auc.x = .2, print.auc.y = .35, add = TRUE)\nplot(roc_svm_lin, print.auc = TRUE, col = \"green\", print.auc.x = .2, print.auc.y = .275, add = TRUE)\nplot(roc_svm_rad, print.auc = TRUE, col = \"cyan\", print.auc.x = .2, print.auc.y = .2, add = TRUE)\n```\n",
    "created" : 1488567440665.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "234085924",
    "id" : "483FA339",
    "lastKnownWriteTime" : 1488822573,
    "last_content_update" : 1488822573318,
    "path" : "~/UChicago/Coursework2_winter/MACS 30100/PS8/Kang_persp-model_PS8.Rmd",
    "project_path" : "Kang_persp-model_PS8.Rmd",
    "properties" : {
        "last_setup_crc32" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}