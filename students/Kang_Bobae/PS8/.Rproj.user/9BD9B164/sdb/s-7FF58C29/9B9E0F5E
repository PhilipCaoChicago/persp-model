{
    "collab_server" : "",
    "contents" : "# import packages\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(modelr)\nlibrary(broom)\nlibrary(tree)\nlibrary(randomForest)\nlibrary(ggdendro) #devtools::install_github(\"bensoltoff/ggdendro\")\nlibrary(forcats)\nlibrary(gbm)\nlibrary(pROC)\nlibrary(e1071)\n\n\n# import data\nbiden <- fread('data/biden.csv')\nmhealth <- fread('data/mental_health.csv')\nsimpson <- fread('data/simpson.csv')\n\n# set seed for reproducibility\nset.seed(0)\n\n# define a function to get mse\nmse <- function(model, data) {\n  x <- modelr:::residuals(model, data)\n  mean(x ^ 2, na.rm = TRUE)\n}\n\nerr.rate.tree <- function(model, data) {\n  data <- as_tibble(data)\n  response <- as.character(model$terms[[2]])\n  \n  pred <- predict(model, newdata = data, type = \"class\")\n  actual <- data[[response]]\n  \n  return(mean(pred != actual, na.rm = TRUE))\n}\n\n#-----------------------------------------------------#\n# Part 1: Sexy Joe Biden (redux times two) [3 points]\n#-----------------------------------------------------#\n# Part 1: Prepare the data\n# split into training and test sets\nbiden_split <- resample_partition(biden, c(test = 0.3, train = 0.7))\nbiden_train <- biden[biden_split$train$idx]\nbiden_test <- biden[biden_split$test$idx]\n#-----------------------------------------------------#\n# Part 1: Fit a decision tree to the training data\nbiden_tree1 <- tree(biden ~ .,\n                   data = biden_train)\n\n# plot the tree\ntree_data <- dendro_data(biden_tree1)\nggplot(segment(tree_data)) +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), \n               alpha = 0.5) +\n  geom_text(data = label(tree_data), \n            aes(x = x, y = y, label=label), vjust = -0.5, size = 3) +\n  geom_text(data = leaf_label(tree_data), \n            aes(x = x, y = y, label=label), vjust = 0.5, size = 3) +\n  theme_dendro() +\n  labs(title = \"Biden feeling thermometer tree\",\n       subtitle = \"All predictors\")\n\n#-----------------------------------------------------#\n# Part 1: Fit another tree to the training data\nbiden_tree2 <- tree(biden ~ .,\n                    data = biden_train,\n                    control = tree.control(nobs = nrow(biden_train),\n                              mindev = .001))\n\n# plot the tree\ntree_data <- dendro_data(biden_tree2)\nggplot(segment(tree_data)) +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), \n               alpha = 0.5) +\n  geom_text(data = label(tree_data), \n            aes(x = x, y = y, label=label), vjust = -0.5, size = 3) +\n  geom_text(data = leaf_label(tree_data), \n            aes(x = x, y = y, label=label), vjust = 0.5, size = 3) +\n  theme_dendro() +\n  labs(title = \"Biden feeling thermometer tree\",\n       subtitle = \"All predictors\")\n\n#-----------------------------------------------------#\n# Part 1. CV\n# generate 10-fold CV trees\nbiden_cv <- crossv_kfold(biden, k = 10) %>%\n  mutate(tree = map(train, ~ tree(biden ~ ., data = .,\n                                  control = tree.control(nobs = nrow(biden), mindev = .001))))\n\n# calculate each possible prune result for each fold\nbiden_cv <- expand.grid(biden_cv$.id, 2:10) %>%\n  as_tibble() %>%\n  mutate(Var2 = as.numeric(Var2)) %>%\n  rename(.id = Var1,\n         k = Var2) %>%\n  left_join(biden_cv) %>%\n  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),\n         mse = map2_dbl(prune, test, mse))\n\nbiden_cv %>%\n  select(k, mse) %>%\n  group_by(k) %>%\n  summarize(test_mse = mean(mse),\n            sd = sd(mse, na.rm = TRUE)) %>%\n  ggplot(aes(k, test_mse)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Number of terminal nodes\",\n       y = \"Test MSE\")\n\n#-----------------------------------------------------#\n# Part 1: Use the bagging approach to analyze this data\n\nbiden_bag <- randomForest(biden ~ ., data = biden_test,\n                          mtry = 5, ntree = 5000)\nbiden_bag\n\n# plot bagging\ndata_frame(var = rownames(importance(biden_bag)),\n           MeanDecreaseGini = importance(biden_bag)[,1]) %>%\n  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%\n  ggplot(aes(var, MeanDecreaseGini)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting Biden feeling thermometer\",\n       subtitle = \"Bagging\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\")\n\n\n#-----------------------------------------------------#\n# Part 1: Use the random forest approach to analyze this data.\nbiden_rf <- randomForest(biden ~ ., data = biden_test,\n                          ntree = 5000)\nbiden_rf\n\n# plot rf\ndata_frame(var = rownames(importance(biden_rf)),\n           MeanDecreaseGini = importance(biden_rf)[,1]) %>%\n  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%\n  ggplot(aes(var, MeanDecreaseGini)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting Biden feeling thermometer\",\n       subtitle = \"Random forest\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\")\n\n# plot both\ndata_frame(var = rownames(importance(biden_rf)),\n           `Random forest` = importance(biden_rf)[,1]) %>%\n  left_join(data_frame(var = rownames(importance(biden_rf)),\n                       Bagging = importance(biden_bag)[,1])) %>%\n  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%\n  gather(model, gini, -var) %>%\n  ggplot(aes(var, gini, color = model)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting Biden feeling thermometer\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\",\n       color = \"Method\")\n#-----------------------------------------------------#\n# Part 1: Use the boosting approach to analyze the data.\n\nbiden_boosting_models <- list(\"boosting_shrinkage.001\" = gbm(biden ~ ., data = biden_train,\n                                                               n.trees = 5000, shrinkage = 0.001),\n                              \"boosting_shrinkage.01\" = gbm(biden ~ ., data = biden_train,\n                                                              n.trees = 5000, shrinkage = 0.01),\n                              \"boosting_shrinkage.1\" = gbm(biden ~ ., data = biden_train,\n                                                             n.trees = 5000, shrinkage = 0.1))\n\ndata_frame(shrinkage.001 = predict(biden_boosting_models$boosting_shrinkage.001,\n                                   newdata = as_tibble(biden_test), n.trees = 1:5000) %>%\n             apply(2, function(x) (x - as_tibble(biden_test)$biden)^2) %>%\n             apply(2, mean),\n           shrinkage.01 = predict(biden_boosting_models$boosting_shrinkage.01,\n                                  newdata = as_tibble(biden_test), n.trees = 1:5000) %>%\n             apply(2, function(x) (x - as_tibble(biden_test)$biden)^2) %>%\n             apply(2, mean),\n           shrinkage.1 = predict(biden_boosting_models$boosting_shrinkage.1,\n                                 newdata = as_tibble(biden_test), n.trees = 1:5000) %>%\n             apply(2, function(x) (x - as_tibble(biden_test)$biden)^2) %>%\n             apply(2, mean)) -> boost_test_mse\n\nboost_test_mse %>%\n  mutate(id = row_number()) %>%\n  mutate_each(funs(cummean(.)), shrinkage.001:shrinkage.01) %>%\n  gather(model, err, -id) %>%\n  ggplot(aes(id, err, color = model)) +\n  geom_line() +\n  scale_color_brewer(type = \"qual\", palette = \"Dark2\") +\n  labs(title = 'Boosting MSE by shrinkage parameters',\n       x = \"Number of trees\",\n       y = \"Test error\",\n       color = 'Model')",
    "created" : 1488805082258.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "560069244",
    "id" : "9B9E0F5E",
    "lastKnownWriteTime" : 1488805032,
    "last_content_update" : 1488805032,
    "path" : "~/UChicago/Coursework2_winter/MACS 30100/PS8/Kang_persp-model_PS8_part1.R",
    "project_path" : "Kang_persp-model_PS8_part1.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}