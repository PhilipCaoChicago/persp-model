{
    "collab_server" : "",
    "contents" : "# import packages\nlibrary(tidyverse)\nlibrary(data.table)\nlibrary(modelr)\nlibrary(broom)\nlibrary(tree)\nlibrary(randomForest)\nlibrary(ggdendro) #devtools::install_github(\"bensoltoff/ggdendro\")\nlibrary(forcats)\nlibrary(gbm)\nlibrary(pROC)\nlibrary(e1071)\n\n\n# import data\nbiden <- fread('data/biden.csv')\nmhealth <- fread('data/mental_health.csv')\nsimpson <- fread('data/simpson.csv')\n\n# set seed for reproducibility\nset.seed(0)\n\n# define a function to get mse\nmse <- function(model, data) {\n  x <- modelr:::residuals(model, data)\n  mean(x ^ 2, na.rm = TRUE)\n}\n\nerr.rate.tree <- function(model, data) {\n  data <- as_tibble(data)\n  response <- as.character(model$terms[[2]])\n  \n  pred <- predict(model, newdata = data, type = \"class\")\n  actual <- data[[response]]\n  \n  return(mean(pred != actual, na.rm = TRUE))\n}\n\n#-----------------------------------------------------#\n# Part 2: Modeling voter turnout [3 points]\n#-----------------------------------------------------#\nmhealth <- fread('data/mental_health.csv')\n\n# Part 2: Prepare the data\nmhealth <- mhealth %>%\n  # remove rows with missing values\n  na.omit() %>% \n  # make categorical variables into factors\n  mutate(vote96 = factor(vote96, levels = 0:1, labels = c(\"No\", \"Yes\")),\n         black = factor(black, levels = 0:1, labels = c(\"not_black\", \"black\")),\n         female = factor(female, levels = 0:1, labels = c('male', 'female')),\n         married = factor(married, levels = 0:1, labels = c('unmarried', 'married')))\n\n# split into training and validation sets\nmhealth_split <- resample_partition(mhealth, c(test = 0.3, train = 0.7))\n\n\nmhealth_train <- mhealth[mhealth_split$train$idx,]\nmhealth_test <- mhealth[mhealth_split$test$idx,]\n\n#-----------------------------------------------------#\n# Part 2: Fit at least five tree-based models\n# a decision tree\nmhealth_tree <- tree(vote96 ~ .,\n                     data = mhealth_train,\n                     control = tree.control(nobs = nrow(mhealth_train), mindev = .001))\n\ntree_data <- dendro_data(mhealth_tree)\nggplot(segment(tree_data)) +\n  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), \n               alpha = 0.5) +\n  geom_text(data = label(tree_data), \n            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +\n  geom_text(data = leaf_label(tree_data), \n            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +\n  theme_dendro() +\n  labs(title = \"Predicting Voter turnout\",\n       subtitle = \"All predictors\")\n\n# 10-fold CV trees to find best tree\nmhealth_cv <- mhealth_train %>%\n  crossv_kfold(k = 10) %>%\n  mutate(tree = map(train, ~ tree(vote96 ~ ., data = .,\n    control = tree.control(nobs = nrow(mhealth_train), mindev = .001))))\n\n# calculate each possible prune result for each fold\nmhealth_cv <-\n  expand.grid(mhealth_cv$.id,\n              seq(from = 2, to = 10)) %>%\n  as_tibble() %>%\n  mutate(Var2 = as.numeric(Var2)) %>%\n  rename(.id = Var1,\n         k = Var2) %>%\n  left_join(mhealth_cv) %>%\n  mutate(prune = map2(tree, k, ~ prune.misclass(.x, best = .y)),\n         mse = map2_dbl(prune, mhealth_test, err.rate.tree))\n\nmhealth_cv%>%\n  group_by(k) %>%\n  summarize(test_mse = mean(mse),\n            sd = sd(mse, na.rm = TRUE))\n\nmhealth_tree_pruned <- prune.tree(mhealth_tree, best = 5)\nerr_rate_tree <- err.rate.tree(mhealth_tree_pruned, mhealth_test)\n\n#----------#\n# bagging (n.tree = 5000)\nmhealth_bag <- randomForest(vote96 ~ ., data = mhealth_train,\n                          mtry = 7, ntree = 5000)\n\ndata_frame(var = rownames(importance(mhealth_bag)),\n           MeanDecreaseGini = importance(mhealth_bag)[,1]) %>%\n  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%\n  ggplot(aes(var, MeanDecreaseGini)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting voter turnout\",\n       subtitle = \"Bagging\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\")\n\npred_bag <- predict(mhealth_bag, newdata = mhealth_test, type = 'response', n.trees=5000) %>% as.character()\nactual1 <- as.character(mhealth_test$vote96)\nerr_rate_bag <- mean(pred_bag != actual1, na.rm = TRUE)\n\n#----------#\n# random forest (n.tree = 5000)\nmhealth_rf <- randomForest(vote96 ~ ., data = mhealth_train,\n                         ntree = 5000)\n\nmhealth_rf\n\n# plot rf\ndata_frame(var = rownames(importance(mhealth_rf)),\n           MeanDecreaseGini = importance(mhealth_rf)[,1]) %>%\n  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%\n  ggplot(aes(var, MeanDecreaseGini)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting voter turnout\",\n       subtitle = \"Random forest\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\")\n\n# plot both\ndata_frame(var = rownames(importance(mhealth_rf)),\n           `Random forest` = importance(mhealth_rf)[,1]) %>%\n  left_join(data_frame(var = rownames(importance(mhealth_rf)),\n                       Bagging = importance(mhealth_bag)[,1])) %>%\n  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%\n  gather(model, gini, -var) %>%\n  ggplot(aes(var, gini, color = model)) +\n  geom_point() +\n  coord_flip() +\n  labs(title = \"Predicting voter turnout\",\n       x = NULL,\n       y = \"Average decrease in the Gini Index\",\n       color = \"Method\")\n\n\npred_rf <- predict(mhealth_rf, newdata = mhealth_test, type = 'response', n.trees=5000) %>% as.character()\nactual1 <- as.character(mhealth_test$vote96)\nerr_rate_rf <- mean(pred_rf != actual1, na.rm = TRUE)\n\n#----------#\n# boost (n.tree = 5000)\n\n# Re-Prepare the data\nmhealth_train2 <- mhealth_train\nmhealth_test2 <- mhealth_test\nmhealth_train2$vote96 <- (mhealth_train2$vote96 == 'Yes')*1\nmhealth_test2$vote96 <- (mhealth_test2$vote96 == 'Yes')*1\n\nmhealth_bst.001 <- gbm(vote96 ~ ., data = mhealth_train2, n.trees = 5000, shrinkage = 0.001)\nmhealth_bst.1 <- gbm(vote96 ~ ., data = mhealth_train2, n.trees = 5000, shrinkage = 0.1)\n\nsummary(mhealth_bst.001)\nsummary(mhealth_bst.1)\n\nactual2 <- as.character(mhealth_test2$vote96)\npred_bst.1 <- predict(mhealth_bst.1, newdata = mhealth_test2, type = 'response', n.trees=5000) %>% round() %>% as.character()\npred_bst.001 <- predict(mhealth_bst.001, newdata = mhealth_test2, type = 'response', n.trees=5000) %>% round() %>% as.character()\nerr_rate_bst.1 <- mean(pred_bst.1 != actual2, na.rm = TRUE)\nerr_rate_bst.001 <- mean(pred_bst.001 != actual2, na.rm = TRUE)\n\n\n\nModel <- c('Tree', 'Bagging', 'Random Forest', 'Boosting, 0.001', 'Boosing, 0.1')\nError_rate <- c(err_rate_tree, err_rate_bag, err_rate_rf, err_rate_bst.001, err_rate_bst.1)\ndata.frame(Model, Error_rate)\n\n#-----------------------------------------------------#\n# Part 2: compare and evaluate at least five tree-based models\n# get ROC curves\n# tree (pruned)\nfitted_tree <- predict(mhealth_tree_pruned, as_tibble(mhealth_test), type = \"class\")\ntree_err <- mean(as_tibble(mhealth_test)$vote96 != fitted_tree)\nroc_tree <- roc(as.numeric(as_tibble(mhealth_test)$vote96), as.numeric(fitted_tree))\n# bagging\nfitted_bag <- predict(mhealth_bag, as_tibble(mhealth_test), type = \"prob\")[,2]\nroc_bag <- roc(as_tibble(mhealth_test)$vote96, fitted_bag)\n# random forest\nfitted_rf <- predict(mhealth_rf, as_tibble(mhealth_test), type = \"prob\")[,2]\nroc_rf <- roc(as_tibble(mhealth_test)$vote96, fitted_rf)\n# boost, lambda = 0.001\nfitted_bst.001 <- predict(mhealth_bst.001, as_tibble(mhealth_test2), type = \"response\", n.trees=5000)\nroc_bst.001 <- roc(as_tibble(mhealth_test2)$vote96, fitted_bst.001)\n# boost, lambda = 0.1\nfitted_bst.1 <- predict(mhealth_bst.1, as_tibble(mhealth_test2), type = \"response\", n.trees=5000)\nroc_bst.1 <- roc(as_tibble(mhealth_test2)$vote96, fitted_bst.1)\n\n# plot ROC curves\nplot(roc_tree, print.auc = TRUE, col = \"red\", print.auc.x = .2)\nplot(roc_bag, print.auc = TRUE, col = \"blue\", print.auc.x = .2, print.auc.y = .4, add = TRUE)\nplot(roc_rf, print.auc = TRUE, col = \"orange\", print.auc.x = .2, print.auc.y = .3, add = TRUE)\nplot(roc_bst.001, print.auc = TRUE, col = \"green\", print.auc.x = .2, print.auc.y = .2, add = TRUE)\nplot(roc_bst.1, print.auc = TRUE, col = \"purple\", print.auc.x = .2, print.auc.y = .1, add = TRUE)\n\n#-----------------------------------------------------#\n# Part 2: Fit at least five SVM models\nmhealth_svm_lin <- svm(vote96 ~., data = mhealth_train, kernel = \"linear\", scale = FALSE, cost = 1)\nsummary(mhealth_svm_lin)\nmhealth_svm_poly <- svm(vote96 ~., data = mhealth_train, kernel = \"polynomial\", scale = FALSE, cost = 1)\nmhealth_svm_rad <- svm(vote96 ~., data = mhealth_train, kernel = \"radial\", scale = FALSE, cost = 1)\n\n\nmh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mhealth_train),\n                    kernel = \"linear\",\n                    range = list(cost = c(.001, .01, .1, 1, 10, 100)))\nmh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mhealth_train),\n                    kernel = \"polynomial\",\n                    range = list(cost = c(.001, .01, .1, 1, 10, 100)))\nmh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mhealth_train),\n                     kernel = \"radial\",\n                     range = list(cost = c(.001, .01, .1, 1, 10, 100)))\nmhealth_svm_ltuned <- mh_lin_tune$best.model\nmhealth_svm_ptuned <- mh_poly_tune$best.model\nmhealth_svm_rtuned <- mh_rad_tune$best.model\n\n#-----------------------------------------------------#\n# Part 2: compare and evaluate at least five SVM models\n# not tuned\nfitted_lin <- predict(mhealth_svm_lin, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_line <- roc(as_tibble(mhealth_test)$vote96, fitted_lin$decision.values)\nfitted_poly <- predict(mhealth_svm_poly, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_poly <- roc(as_tibble(mhealth_test)$vote96, fitted_poly$decision.values)\nfitted_rad <- predict(mhealth_svm_rad, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_rad <- roc(as_tibble(mhealth_test)$vote96, fitted_rad$decision.values)\n# tuned\nfitted_ltuned <- predict(mhealth_svm_ltuned, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_ltuned <- roc(as_tibble(mhealth_test)$vote96, fitted_ltuned$decision.values)\nfitted_ptuned <- predict(mhealth_svm_ptuned, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_ptuned <- roc(as_tibble(mhealth_test)$vote96, fitted_ptuned$decision.values)\nfitted_rtuned <- predict(mhealth_svm_rtuned, as_tibble(mhealth_test), decision.values = TRUE) %>% attributes\nroc_rtuned <- roc(as_tibble(mhealth_test)$vote96, fitted_rtuned$decision.values)\n\nplot(roc_line, print.auc = TRUE, col = \"red\", print.auc.x = .2)\nplot(roc_poly, print.auc = TRUE, col = \"blue\", print.auc.x = .2, print.auc.y = .425, add = TRUE)\nplot(roc_rad, print.auc = TRUE, col = \"orange\", print.auc.x = .2, print.auc.y = .35, add = TRUE)\nplot(roc_ltuned, print.auc = TRUE, col = \"green\", print.auc.x = .2, print.auc.y = .275, add = TRUE)\nplot(roc_ptuned, print.auc = TRUE, col = \"purple\", print.auc.x = .2, print.auc.y = .2, add = TRUE)\nplot(roc_rtuned, print.auc = TRUE, col = \"cyan\", print.auc.x = .2, print.auc.y = .125, add = TRUE)\n\n\n\n#-----------------------------------------------------#\n# Part 3: OJ Simpson [4 points]\n#-----------------------------------------------------#\n# Part 3: Explain the impact of an individual's race on their beliefs about OJ Simpson's guilt\nsimpson_logit1 <- glm(guilt ~ black + hispanic, data = (simpson %>% na.omit()), family='binomial') # only race predictors\nsimpson_logit2 <- glm(guilt ~ ., data = (simpson %>% na.omit()), family='binomial') # all predictors\n\nsummary(simpson_logit1)\nsummary(simpson_logit2)\n\n#-----------------------------------------------------#\n# Part 3: Predict the impact of an individual's race on their beliefs about OJ Simpson's guilt\n# Prepare data\nsimpson_pred <- simpson %>%\n  na.omit() %>% # remove missing values\n  model.matrix(~ ., data=.) %>% # binarize categorical predictors\n  as.data.table() %>%\n  select(-`(Intercept)`)\n# simplify colnames\ncolnames(simpson_pred)  <- c('guilt', 'dem', 'rep', 'ind', 'age', 'educ_hs', 'educ_not_hs', 'educ_ref', 'educ_some_col',\n                        'female', 'black', 'hispanic', 'inc_30to50', 'inc_50to75', 'inc_over75', 'inc_ref', 'inc_under15')\n\n# split data into train and test sets  \nsimpson_split <- resample_partition(simpson_pred, c(test = 0.3, train = 0.7))\nsimpson_train <- simpson_pred[simpson_split$train$idx]\nsimpson_test <- simpson_pred[simpson_split$test$idx]\n\n# fit models to compare\nsimpson_logit <- glm(guilt ~ ., data = simpson_train, family='binomial') # logistic regression\nsimpson_rf <- randomForest(as.factor(guilt) ~ ., data = simpson_train, ntree = 5000)\nsimpson_bst <- gbm(guilt ~ ., data = simpson_train, n.trees = 5000)\nsimpson_svm_lin <- svm(guilt ~ ., data = simpson_train, kernel = \"linear\", scale = FALSE, probability=TRUE)\nsimpson_svm_poly <- svm(guilt ~ ., data = simpson_train, kernel = \"polynomial\", scale = FALSE, probability=TRUE)\nsimpson_svm_rad <- svm(guilt ~ ., data = simpson_train, kernel = \"radial\", scale = FALSE, probability=TRUE)\n\nsim_lin_tune <- tune(svm, guilt ~ ., data = as_tibble(simpson_train), kernel = \"linear\",\n                    range = list(cost = c(.01, .1, 1, 10, 100)))\nsim_poly_tune <- tune(svm, guilt ~ ., data = as_tibble(simpson_train), kernel = \"polynomial\",\n                     range = list(cost = c(.01, .1, 1, 10, 100)))\nsim_rad_tune <- tune(svm, guilt ~ ., data = as_tibble(simpson_train), kernel = \"radial\",\n                    range = list(cost = c(.01, .1, 1, 10, 100)))\nsimpson_svm_ltuned <- sim_lin_tune$best.model\nsimpson_svm_ptuned <- sim_poly_tune$best.model\nsimpson_svm_rtuned <- sim_rad_tune$best.model\n\n\n# predict\nfitted_logit <- predict(simpson_logit, as_tibble(simpson_test), type = \"response\")\nfitted_rf <- predict(simpson_rf, as_tibble(simpson_test), type = \"prob\")[,2]\nfitted_bst <- predict(simpson_bst, as_tibble(simpson_test), type = \"response\", n.trees=5000)\nfitted_svm_lin <- predict(simpson_svm_lin, as_tibble(simpson_test), decision.values = TRUE, probability = TRUE) %>% attributes\nfitted_svm_poly <- predict(simpson_svm_poly, as_tibble(simpson_test), decision.values = TRUE, probability = TRUE) %>% attributes\nfitted_svm_rad <- predict(simpson_svm_rad, as_tibble(simpson_test), decision.values = TRUE, probability = TRUE)\n",
    "created" : 1488548646344.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1966346300",
    "id" : "D6E54CFF",
    "lastKnownWriteTime" : 1488809203,
    "last_content_update" : 1488812079387,
    "path" : "~/UChicago/Coursework2_winter/MACS 30100/PS8/Kang_persp-model_PS8_part2.R",
    "project_path" : "Kang_persp-model_PS8_part2.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}