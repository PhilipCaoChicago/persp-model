---
title: "PS8: Tree-based method and support vector machines"
author: "Ningyin Xu"
date: "3/1/2017"
output:
  github_document:
    toc: true

---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      echo = FALSE)
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(rcfss)
library(pROC)
library(gbm)
library(ggdendro)
library(e1071)
library(stargazer)

options(digits = 3)
options(na.action = na.warn)
set.seed(1234)
```

# Part 1: Joe Biden
## Problem 1. Split the data
```{r biden_1, include = FALSE}
bidendata <- read_csv('data/biden.csv')
names(bidendata) <- stringr::str_to_lower(names(bidendata))

set.seed(1234)
biden_split <- resample_partition(bidendata, c(valid = 0.3, train = 0.7))
```

## Problem 2. Decision tree (no controls)
```{r biden_2, include=TRUE}
# estimate model
biden_tree1 <- tree(biden ~ ., 
                    data = biden_split$train)

mod <- biden_tree1

# plot tree
tree1_data <- dendro_data(mod, type = 'uniform')
ggplot(segment(tree1_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree1_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree1_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

mse_tree1 <- mse(biden_tree1, biden_split$valid)
mse_tree1
```

The above decision tree shows the result where we fit a tree to the training data and use default values for control options. One can tell from this tree that for democrats, the average biden warmth score would be `r tree1_data$leaf_labels[3,3]`. For non-democrats, republicans would have `r tree1_data$leaf_labels[2,3]` as an average biden warmth score, and non-republicans would have `r tree1_data$leaf_labels[1,3]`.
The test MSE is `r mse_tree1`, which is close to test MSEs we got from last assignment (around 400), let's see if we could improve this model.

## Problem 3. Decision tree (CV)
```{r biden_3, include=TRUE}
# estimate model
biden_tree2 <- tree(biden ~ ., 
                    data = biden_split$train,
                    control = tree.control(nobs = nrow(biden_split$train), mindev = 0))

biden_tree2_results <- data_frame(terms = 2:25,
           model = map(terms, ~ prune.tree(biden_tree2, k = NULL, best = .)), MSE = map_dbl(model, mse, data = biden_split$valid))

ggplot(biden_tree2_results, aes(terms, MSE)) +
  geom_line() +
  labs(title = "Comparing Tree Complexity",
       subtitle = "Using validation set",
       x = "Terminal Nodes",
       y = "Mean Squared Error") + 
  scale_x_discrete(breaks = seq(2,25,1), limits = seq(2,25,1))
```

After comparing MSEs generated from different number of terminal nodes, one can tell 11 is the optimal level of tree complexity.

Thus we plot the optimal tree below.
```{r biden_prune, include=TRUE}
mod <- prune.tree(biden_tree2, best = 11)
tree2_data <- dendro_data(mod, type = 'uniform')
ggplot(segment(tree2_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree2_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree2_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()
leaf <- tree2_data$leaf_labels[3]
mse_tree_opt <- mse(mod, biden_split$valid)
mse_tree_opt
```
The optimal tree shows that one can divide data to 11 groups and each group has a different average expected value for biden warmth score. Specific information of these groups is shown in the tree. 
The test MSE is improved from `r mse_tree1` to `r mse_tree_opt`, indicating pruning the tree does improve the test MSE.


## Problem 4. Bagging
```{r biden_4, include=TRUE}
set.seed(1234)
bag_biden <- randomForest(biden ~ .,
                          data = biden_split$train,
                          mtry = 5,
                          importance = TRUE)
mse_bag <- mse(bag_biden, biden_split$valid)
mse_bag
data_frame(var = rownames(importance(bag_biden)),
           MeanDecreaseError = importance(bag_biden)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseError, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseError)) +
  geom_col(width = 0.5) +
  coord_flip() +
  labs(title = "Predicting Biden Warmth Score",
       subtitle = "Bagging",
       x = NULL,
       y = "% decrease in out-of-bag MSE")
```

Bagging approach gives a higher MSE than before, `r mse_bag`. Since we are doing regression tree here, % decrease in out-of-bag MSE instead of Gini Index is used here to measure the variable importance. The above plot shows the importance of variables: Dem and Rep can bring significant decrease in the out-of-bag MSE thus they are the most important predictors. Age is relatively unimportant. 


## Problem 5. Random Forest
```{r biden_5}
biden_rf <- randomForest(biden ~ .,
                         data = biden_split$train,
                         importance = TRUE)

data_frame(var = rownames(importance(biden_rf)),
           `Random Forest` = importance(biden_rf)[,1]) %>%
  left_join(data_frame(var = rownames(importance(bag_biden)),
           Bagging = importance(bag_biden)[,1])) %>%
  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%
  gather(model, rss, -var) %>%
  ggplot(aes(var, rss, color=model)) +
  geom_col(aes(fill=model), position='dodge') +
  coord_flip() +
  labs(title = "Predicting Biden Warmth Score",
       x = NULL,
       y = "% decrease in out-of-bag MSE")
mse_rf <- mse(biden_rf, biden_split$valid)
mse_rf
```

Using random forest approach, the test MSE we obtained is `r mse_rf`, which is much smaller than the `r mse_bag` we got from bagging and closer to the test MSE using optimal tree. This proves that random forests improve upon bagging, because it avoids the effect of single dominant predictor in the dataset.

The importance of variables shows that $Dem$ and $Rep$ are still the most important variables, but their importance seems relatively smaller compared to bagging because the variable restriction when random forest considering splits.

```{r biden_51}
biden_tree5_results <- data_frame(terms = 1:5,
           model = map(terms, ~ randomForest(biden ~ .,
                         data = biden_split$train, ntree=500,
                         mtry = ., importance=TRUE)), 
           MSE = map_dbl(model, mse, data = biden_split$valid))

ggplot(biden_tree5_results, aes(terms, MSE)) +
  geom_line() +
  labs(title = "Comparing the effect of m",
       subtitle = "Using validation set",
       x = "m: the number of variables considered at each split",
       y = "Mean Squared Error")
```

From the plot of test MSE generated from different number of variables considered at each split, one can tell 2 variables give the best test MSE. After $m = 2$, the MSE gets higher because the trees tend to be more correlated, and averaging across them won't substantially reduce variance.


## Problem 6. Boosting
```{r biden_6, warning=FALSE}
biden_bst <- gbm(biden ~ ., 
                 data = biden_split$train, 
                 n.trees = 10000)

yhat_biden <- predict(biden_bst, 
                      newdata = biden_split$valid,
                      n.trees = 100)
mse_bst <- mean((yhat_biden - biden_split$valid$data$biden)^2)
mse_bst
```

The test MSE obtained is `r mse_bst`, higher than all the MSEs we've got so far. This might have something to do with de shrinkage parameter we choose (default value 0.001). 

```{r bst_best, warning=FALSE}
mse_func <- function(traindata, testdata, shrinkage, num_trees, depth) {
  biden_bst <- gbm(biden ~ ., 
                 distribution = 'gaussian',
                 data = traindata, 
                 shrinkage = shrinkage,
                 n.trees = num_trees,
                 interaction.depth = depth)
  yhat_biden <- predict(biden_bst, 
                      newdata = testdata,
                      n.trees = num_trees)
  mean((yhat_biden - testdata$data$biden)^2)
}

biden_bst_results1 <- data_frame(
          terms = seq(0.001, .05, length.out = 50),
          MSE = map_dbl(terms, ~ mse_func(
             traindata = biden_split$train, 
             testdata = biden_split$valid,
             shrinkage = ., num_trees = 1000, depth = 1)))
ggplot(biden_bst_results1, aes(terms, MSE)) +
  geom_line() +
  labs(title = "Comparing the effect of shrinkage parameter",
       subtitle = "Using validation set",
       x = "lambda: the shrinkage parameter",
       y = "Mean Squared Error")

biden_bst_results2 <- data_frame(
          terms = seq(100, 10000, by = 100),
          MSE = map_dbl(terms, ~ mse_func(
             traindata = biden_split$train, 
             testdata = biden_split$valid,
             shrinkage = 0.001, num_trees = ., depth = 1)))
ggplot(biden_bst_results2, aes(terms, MSE)) +
  geom_line() +
  labs(title = "Comparing the effect of number of trees",
       subtitle = "Using validation set",
       x = "B: number of trees",
       y = "Mean Squared Error")

```

To optimize the test MSE using boosting approach, I tried different shrinkage parameter range from 0.001 to 0.05, and different number of trees from 100 to 10000. It seems that for both case, the smaller the better. The best test MSE seems to be obtained when the shrinkage parameter is 0.001 and number of trees is 100. However, the best test MSE is `r mse_bst`, much higher than we got from previous approaches. 


# Part 2: Modeling voter turnout
## Problem 1. Choose the best tree-based model
```{r mh_1_1, warning=FALSE, message=FALSE}
mhdata <- read_csv('data/mental_health.csv')
mhdata <- na.omit(mhdata)

mhdata %>%
  mutate(vote96 = factor(vote96), black = factor(black),
         female = factor(female), married = factor(married)) %>%
         {.} -> mhdata

set.seed(1234)
mh_split <- resample_partition(mhdata, c(valid = 0.3, train = 0.7))

err.rate <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  pred <- predict(model, newdata = data, type = "class")
  actual <- data[[response]]
  return(mean(pred != actual, na.rm = TRUE))
}

mh_normaltree <- tree(vote96 ~.,
                      data = mh_split$train)
mh_nt_err <- err.rate(mh_normaltree, mh_split$valid)

mh_tree <- tree(vote96 ~ ., 
                data = mh_split$train,
                control = tree.control(nrow(mh_split$train),
                                       mindev = 0))

mh_tree_results <- data_frame(terms = 2:25,
           model = map(terms, ~ prune.tree(mh_tree, k = NULL, best = .)), error = map_dbl(model, ~ err.rate(., data = mh_split$valid)))

ggplot(mh_tree_results, aes(terms, error)) +
  geom_line() +
  labs(title = "Comparing Tree Complexity",
       subtitle = "Using validation set",
       x = "Terminal Nodes",
       y = "Test Error Rate") + 
  scale_x_discrete(breaks = seq(2,25,1), limits = seq(2,25,1))

mh_prunetree <- prune.tree(mh_tree, best = 17)
mh_pt_err <- err.rate(mh_prunetree, mh_split$valid)

mh_bag <- randomForest(vote96 ~., 
                       data = mh_split$train,
                       mtry = 7,
                       importance = TRUE)
mh_bg_err <- err.rate(mh_bag, mh_split$valid)

mh_normalrf <- randomForest(vote96 ~.,
                            data = mh_split$train,
                            importance = TRUE)
mh_nrf_err <- err.rate(mh_normalrf, mh_split$valid)

mh_tree_results1 <- data_frame(terms = 2:7,
           model = map(terms, ~ randomForest(vote96 ~ .,
                         data = mh_split$train, ntree=500,
                         mtry = ., importance=TRUE)), 
           error = map_dbl(model, ~ err.rate(., data = mh_split$valid)))

ggplot(mh_tree_results1, aes(terms, error)) +
  geom_line() +
  labs(title = "Comparing the effect of m",
       subtitle = "Using validation set",
       x = "m: the number of variables considered at each split",
       y = "Test Error Rate")

mh_rf <- randomForest(vote96 ~.,
                       data = mh_split$train,
                       mtry = 3,
                       importance = TRUE)
mh_rf_err <- err.rate(mh_rf, mh_split$valid)

mh_log <- glm(vote96 ~ ., data = mh_split$train, family = 'binomial')
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}
mh_log_pred <- mh_split$valid$data %>%
  add_predictions(mh_log) %>%
  mutate(prob = logit2prob(pred),
         prob = as.numeric(prob > .5))
mh_log_err <- mean(mh_log_pred$vote96 == mh_log_pred$prob, na.rm = TRUE)

pre <- function(err1, err2) {
  (err1 - err2)/err1
}
  
mh_1_result <- data_frame(
  'objects' = c('err', 'PRE'),
  'logistic' = c(mh_log_err, pre(mh_log_err, mh_log_err)),
  'normaltree' = c(mh_nt_err, pre(mh_log_err, mh_nt_err)),
  'prunedtree' = c(mh_pt_err, pre(mh_log_err, mh_pt_err)),
  'bagging' = c(mh_bg_err, pre(mh_log_err, mh_bg_err)),
  'normalrf' = c(mh_nrf_err, pre(mh_log_err, mh_nrf_err)),
  'optrf' = c(mh_rf_err, pre(mh_log_err, mh_rf_err))
)
knitr::kable(mh_1_result, digits = 3, align = 'c')

```

The five models I chose are decision tree with no control value, pruned decision tree with optimal number of terminal nodes, bagging, random forest with default value, and random forest with optimal number of variables considered at each split. The optimal parameter value for the second and fifth models are shown from the first two plots in this section. The table above shows the error rate (1st row) and PRE comparing to logisitic model for each of these models. The ROC curves below shows the AUC for each model.

```{r mh_1_2, warning=FALSE}
fitted_nt <- predict(mh_normaltree, as_tibble(mh_split$valid), type = 'class')
roc_nt <- roc(as.numeric(as_tibble(mh_split$valid)$vote96), as.numeric(fitted_nt))

fitted_pt <- predict(mh_prunetree, as_tibble(mh_split$valid), type = 'class')
roc_pt <- roc(as.numeric(as_tibble(mh_split$valid)$vote96), as.numeric(fitted_pt))

fitted_bg <- predict(mh_bag, as_tibble(mh_split$valid), type = 'prob')[,2]
roc_bg <- roc(as_tibble(mh_split$valid)$vote96, fitted_bg)

fitted_nrf <- predict(mh_normalrf, as_tibble(mh_split$valid), type = 'prob')[,2]
roc_nrf <- roc(as_tibble(mh_split$valid)$vote96, fitted_nrf)

fitted_rf <- predict(mh_rf, as_tibble(mh_split$valid), type = 'prob')[,2]
roc_rf <- roc(as_tibble(mh_split$valid)$vote96, fitted_rf)

plot(roc_nt, print.auc = TRUE, col = "blue", print.auc.x = .2)
plot(roc_pt, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(roc_bg, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(roc_nrf, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(roc_rf, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .1, add = TRUE)
```

As one can see from the table and ROC curves, optimal random forest gives the lowest error rate (about 29.2%), highest PRE comparing to logisitic model, and second largest AUC (0.7). So I use optimal random forest to predict the test data as below.

```{r mh_1_3, warning = FALSE}
data_frame(var = rownames(importance(mh_rf)),
           MeanDecreaseGini = importance(mh_rf)[,4]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_col(width = 0.5) +
  coord_flip() +
  labs(title = "Predicting Voter Turnout",
       subtitle = "Random Forest",
       x = NULL,
       y = "Average decrease in the Gini Index")
```

We use variable importance to interpret the random forest we got. From the above graph, one can tell age is the most important predictor for voter turnout. Family income, respondent's mental health and number of years of formal education can also significantly reduce Gini index in the classification trees. Sex, marriage status and black have relatively small influence in this case.


## Problem 2. Choose the best SVM model
```{r mh_2_1, warning=FALSE}
#linear kernel
mh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
mh_lin <- mh_lin_tune$best.model

fitted <- predict(mh_lin, as_tibble(mh_split$valid), decision.values = TRUE) %>%
  attributes
roc_line <- roc(as_tibble(mh_split$valid)$vote96, fitted$decision.values)

#2-degree polynomial kernel
mh_poly2_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    degree = 2,
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
mh_poly2 <- mh_poly2_tune$best.model

fitted <- predict(mh_poly2, as_tibble(mh_split$valid), decision.values = TRUE) %>%
  attributes

roc_poly2 <- roc(as_tibble(mh_split$valid)$vote96, fitted$decision.values)

#polynomial kernel
mh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
mh_poly <- mh_poly_tune$best.model

fitted <- predict(mh_poly, as_tibble(mh_split$valid), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(mh_split$valid)$vote96, fitted$decision.values)

#Radial kernel
mh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
mh_rad <- mh_rad_tune$best.model

fitted <- predict(mh_rad, as_tibble(mh_split$valid), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(mh_split$valid)$vote96, fitted$decision.values)

#Sigmoid kernel
mh_sig_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "sigmoid",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
mh_sig <- mh_sig_tune$best.model

fitted <- predict(mh_sig, as_tibble(mh_split$valid), decision.values = TRUE) %>%
  attributes

roc_sig <- roc(as_tibble(mh_split$valid)$vote96, fitted$decision.values)

mh_2_result <- data_frame(
  'objects' = c('cost', 'error rate'),
  'linear' = c(mh_lin_tune$best.parameters$cost, mh_lin_tune$best.performance),
  '2-degree poly' = c(mh_poly2_tune$best.parameters$cost, mh_poly2_tune$best.performance),
  '3-degree' = c(mh_poly_tune$best.parameters$cost, mh_poly_tune$best.performance),
  'radial' = c(mh_rad_tune$best.parameters$cost, mh_rad_tune$best.performance),
  'sigmoid' = c(mh_sig_tune$best.parameters$cost, mh_sig_tune$best.performance))
knitr::kable(mh_2_result, digits = 3, align = 'c')


plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly2, print.auc = TRUE, col = "purple", print.auc.y = .4, add = TRUE)
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .3, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .2, add = TRUE)
plot(roc_sig, print.auc = TRUE, col = "green", print.auc.y = .1, add = TRUE)
```

I chose linear kernel, 2-degree polynomial, 3-degree polynomial, radial kernel, and sigmoid kernel as my five SVM models. For each of them I used 10-fold cross-validation to determine the optimal cost parameter. And the above table shows their error rates associating with the best cost. The above graph shows their ROC curves.

Among these five models, 3-degree polynomial kernel has the best performance since it has low error rate and largest AUC. Thus I use this model to fit the test data and below is the ROC curve, showing that this model has certain accuracy and fit the test data well.

```{r mh_2_2, warning=FALSE}
summary(mh_poly)
plot(roc_poly, print.auc = TRUE)
```


# Part 3: OJ Simpson
## Problem 1. Race and Belief of OJ Simpson's guilt
```{r oj_1, warning=FALSE, message=FALSE}
read_csv('data/simpson.csv') %>%
  na.omit() %>%
  mutate_each(funs(as.factor(.)), guilt, dem, rep, ind,
              female, black, hispanic, educ, income) %>%
         {.} -> ojdata
set.seed(1234)
oj_split <- resample_partition(ojdata, c(test = 0.3, train = 0.7))

oj1_logit <- glm(guilt ~ black + hispanic, data = as_tibble(oj_split$train), family = binomial)
fitted1 <- predict(oj1_logit, as_tibble(oj_split$test), type = "response")
oj1_logit_err <- mean(as_tibble(oj_split$test)$guilt != round(fitted1))
oj1_roc_logit <- roc(as_tibble(oj_split$test)$guilt, fitted1)

oj1_tree <- tree(guilt ~ black + hispanic, data = as_tibble(oj_split$train))
fitted2 <- predict(oj1_tree, as_tibble(oj_split$test), type = "class")
oj1_tree_err <- mean(as_tibble(oj_split$test)$guilt != fitted2)
oj1_roc_tree <- roc(as.numeric(as_tibble(oj_split$test)$guilt), as.numeric(fitted2))

oj1_bag <- randomForest(guilt ~ black + hispanic, data = as_tibble(oj_split$train), mtry = 2)
fitted3 <- predict(oj1_bag, as_tibble(oj_split$test), type = "prob")[,2]
oj1_bag_err <- 0.184
oj1_roc_bag <- roc(as_tibble(oj_split$test)$guilt, fitted3)

oj1_lin_tune <- tune(svm, guilt ~ black + hispanic, data = as_tibble(oj_split$train), kernel = 'linear', range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
oj1_lin <- oj1_lin_tune$best.model
oj1_lin_err <- oj1_lin_tune$best.performance
fitted4 <- predict(oj1_lin, as_tibble(oj_split$test), decision.values = TRUE) %>%
  attributes
oj1_roc_line <- roc(as_tibble(oj_split$test)$guilt, fitted4$decision.values)

oj1_poly_tune <- tune(svm, guilt ~ black + hispanic, data = as_tibble(oj_split$train), kernel = "polynomial", range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
oj1_poly <- oj1_poly_tune$best.model
oj1_poly_err <- oj1_poly_tune$best.performance
fitted5 <- predict(oj1_poly, as_tibble(oj_split$test), decision.values = TRUE) %>%
  attributes
oj1_roc_poly <- roc(as_tibble(oj_split$test)$guilt, fitted5$decision.values)

oj1_rad_tune <- tune(svm, guilt ~ black + hispanic, data = as_tibble(oj_split$train), kernel = "radial", range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
oj1_rad <- oj1_rad_tune$best.model
oj1_rad_err <- oj1_rad_tune$best.performance
fitted6 <- predict(oj1_rad, as_tibble(oj_split$test), decision.values = TRUE) %>%
  attributes
oj1_roc_rad <- roc(as_tibble(oj_split$test)$guilt, fitted6$decision.values)

oj1_result <- data_frame(
  'objects' = c('error rate'),
  'logisitic' = c(oj1_logit_err),
  'decision tree' = c(oj1_tree_err),
  'bagging' = c(oj1_bag_err),
  'linear-SVM' = c(oj1_lin_err),
  'poly-SVM' = c(oj1_poly_err),
  'radial-SVM' = c(oj1_rad_err))
knitr::kable(oj1_result, digits = 5, align = 'c')

plot(oj1_roc_logit, print.auc = TRUE, col = "blue", print.auc.x = .2, print.auc.y = .6)
plot(oj1_roc_tree, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .5, add = TRUE)
plot(oj1_roc_bag, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(oj1_roc_line, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(oj1_roc_poly, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(oj1_roc_rad, print.auc = TRUE, col = "yellow", print.auc.x = .2, print.auc.y = .1, add = TRUE)

```

To choose the best model, I applied cross validation first, and used linear regression, decision tree, bagging, linear kernel SVM, polynomial kernel SVM, and radial kernel SVM generating from the training data to fit the test data. (Since the response is categorical, linear regression methods are not suitable in this case.) The error rate and ROC curves are shown as above. As one can see, these six models all give similar outcome. This is probably because there are only two variables and one of them may have dominant influence. Since these models are equally good, I'll use the most interpretable one -- decision tree.

```{r oj_1_2, warning = FALSE}
oj1_tree1 <- tree(guilt ~ black + hispanic, 
                data = oj_split$train,
                control = tree.control(nrow(oj_split$train),
                                       mindev = 0))
oj1_tree_results <- data_frame(terms = 2:4,
           model = map(terms, ~ prune.tree(oj1_tree1, k = NULL, best = .)), error = map_dbl(model, ~ err.rate(., data = oj_split$test)))

ggplot(oj1_tree_results, aes(terms, error)) +
  geom_line() +
  labs(title = "Comparing Tree Complexity",
       subtitle = "Using validation set",
       x = "Terminal Nodes",
       y = "Test Error Rate")

mod <- prune.tree(oj1_tree1, best = 2)
tree_data <- dendro_data(mod, type = 'uniform')
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Belief of Simpson's guilt tree",
       subtitle = "Race")
```

In terms of race, the tree complexity doesn't have any influence on the error rate, indicating there's a dominant variable here and the other variable doesn't have influence on the response. It turns out the dominant variable is whether the respondent is black. The tree shows that if the respondent is not black (left node), he/she would think OJ Simpson was "probably guilty", if the respondent is black (right node), he/she would think OJ Simpson was "probably not guilty". Whether the respondent is hispanic doesn't have significant influence. This indicates there's a strong relationship between race (african-american) and belief of whether Simpson is guilty.

## Problem 2. Predicting Belief of OJ Simpson's guilt
```{r oj_2, warning=FALSE}
ojdata <-
  select(ojdata, -ind)

set.seed(1234)
oj_split <- resample_partition(ojdata, c(test = 0.3, train = 0.7))

oj_logit <- glm(guilt ~ ., data = as_tibble(oj_split$train), family = binomial)
fitted1 <- predict(oj_logit, as_tibble(oj_split$test), type = "response")
oj_logit_err <- mean(as_tibble(oj_split$test)$guilt != round(fitted1))
oj_roc_logit <- roc(as_tibble(oj_split$test)$guilt, fitted1)

oj_tree1 <- tree(guilt ~ ., 
                data = oj_split$train,
                control = tree.control(nrow(oj_split$train),
                                       mindev = 0))
oj_tree_results <- data_frame(terms = 2:50,
           model = map(terms, ~ prune.tree(oj_tree1, k = NULL, best = .)), error = map_dbl(model, ~ err.rate(., data = oj_split$test)))
ggplot(oj_tree_results, aes(terms, error)) +
  geom_line() +
  labs(title = "Comparing Tree Complexity",
       subtitle = "Using validation set",
       x = "Terminal Nodes",
       y = "Test Error Rate")

auc_best <- function(model) {
  fitted <- predict(model, as_tibble(oj_split$test), type = 'class')
  roc1 <- roc(as.numeric(as_tibble(oj_split$test)$guilt), as.numeric(fitted))
  auc(roc1)
}

oj_tree_results2 <- data_frame(terms = 2:50,
           model = map(terms, ~ prune.tree(oj_tree1, k = NULL, best = .)),
           AUC = map_dbl(model, ~ auc_best(.)))

ggplot(oj_tree_results2, aes(terms, AUC)) +
  geom_line() +
  labs(title = "Comparing Tree Complexity",
       subtitle = "Using validation set",
       x = "Terminal Nodes",
       y = "AUC")


oj_tree <- prune.tree(oj_tree1, best = 10)
fitted2 <- predict(oj_tree, as_tibble(oj_split$test), type = "class")
oj_tree_err <- min(oj_tree_results$error)
oj_roc_tree <- roc(as.numeric(as_tibble(oj_split$test)$guilt), as.numeric(fitted2))

oj_bag <- randomForest(guilt ~ ., data = as_tibble(oj_split$train), mtry = 2)
fitted3 <- predict(oj_bag, as_tibble(oj_split$test), type = "prob")[,2]
oj_bag_err <- 0.194
oj_roc_bag <- roc(as_tibble(oj_split$test)$guilt, fitted3)

oj_lin_tune <- tune(svm, guilt ~ ., data = as_tibble(oj_split$train), kernel = 'linear', range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
oj_lin <- oj_lin_tune$best.model
oj_lin_err <- oj_lin_tune$best.performance
fitted4 <- predict(oj_lin, as_tibble(oj_split$test), decision.values = TRUE) %>%
  attributes
oj_roc_line <- roc(as.numeric(as_tibble(oj_split$test)$guilt), as.numeric(fitted4$decision.values))

oj_poly_tune <- tune(svm, guilt ~ ., data = as_tibble(oj_split$train), kernel = "polynomial", range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
oj_poly <- oj_poly_tune$best.model
oj_poly_err <- oj_poly_tune$best.performance
fitted5 <- predict(oj_poly, as_tibble(oj_split$test), decision.values = TRUE) %>%
  attributes
oj_roc_poly <- roc(as.numeric(as_tibble(oj_split$test)$guilt), as.numeric(fitted5$decision.values))

oj_rad_tune <- tune(svm, guilt ~ ., data = as_tibble(oj_split$train), kernel = "radial", range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
oj_rad <- oj_rad_tune$best.model
oj_rad_err <- oj_rad_tune$best.performance
fitted6 <- predict(oj_rad, as_tibble(oj_split$test), decision.values = TRUE) %>%
  attributes
oj_roc_rad <- roc(as.numeric(as_tibble(oj_split$test)$guilt), as.numeric(fitted6$decision.values))

oj_result <- data_frame(
  'objects' = c('error rate'),
  'logisitic' = c(oj_logit_err),
  'decision tree' = c(oj_tree_err),
  'bagging' = c(oj_bag_err),
  'linear-SVM' = c(oj_lin_err),
  'poly-SVM' = c(oj_poly_err),
  'radial-SVM' = c(oj_rad_err))
knitr::kable(oj_result, digits = 5, align = 'c')

plot(oj_roc_logit, print.auc = TRUE, col = "blue", print.auc.x = .2, print.auc.y = .6)
plot(oj_roc_tree, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .5, add = TRUE)
plot(oj_roc_bag, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(oj_roc_line, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(oj_roc_poly, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(oj_roc_rad, print.auc = TRUE, col = "yellow", print.auc.x = .2, print.auc.y = .1, add = TRUE)
```

To better predict whether individuals believe Simpson to be guilty, I used all variables except for independent for this part, since in this dataset $ind$ only has one value 0. I still used the six models I used before. To choose the approriate decision tree, I use cross-validation to pick the optimal terminal nodes that could give the lowest test error rate and highest AUC.

Above are the error rates and ROC curves of these models. As one can tell, decision tree and logistic regression both have lowest error rate, yet logistic regression has larger AUC. Since logistic regression seems more accurate yet decision tree is more interpretable, I will use both of them to predict the data.

```{r oj_2_2, warning=FALSE}
stargazer(oj_logit, type = 'text')

tree_data <- dendro_data(oj_tree, type = 'uniform')
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Belief of Simpson's guilt tree",
       subtitle = "All variables")
```

The result from logistic regression shows that, age, educ (not a high school grad), female, black, and income (Refused/no answer) are statiscally significant on the belief whether simpson is guilty. Not graduating from high school, being a female, being a black, refuse to answer or have no answer to income would decrese the log-odds of beliving Simpson is guilty. And with higher age, the log-odds of beliving he's guilty would increase.

The decision tree, interestingly, shows that being black is still the most important variable. African-americans, no matter their age is lower or higher than 37.5, and no matter if they're willing to give answer about their income, would be predicted as beliving Simpson is "probably not guilty". While non-African-Americans, no matter whether they've graduated from high school, would be predicted as beliving Simpson is "probably guilty". However, these three variables (age, income and educ) are included since they can increase node purity.




