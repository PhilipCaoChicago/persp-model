---
title: "GLM"
output: md_document
name: Huanye Liu
---
```{r ROCR,echo=TRUE}
# if ROCR package not installed, uncomment the next line
#install.packages("ROCR")
library(ROCR)
```

```{r read, echo=TRUE}

vt = read.csv("mental_health.csv",header=TRUE)
```
# Describe the data
```{r histogram, echo=TRUE}
tab<-table(vt$vote96)
barplot(tab, names.arg=c('did not vote','voted'),main="Voter Turnout",xlab="vote or not",ylab="frequency") 
```
```{r turnout1, echo=TRUE}
prob_vote = tab['1']/(tab['0']+tab['1'])
prob_vote
```
1. The histogram is shown above, and the unconditional probability of a given individual turning out to be a vote is 0.6823574.
```{r crude relation, echo=TRUE}
plot(vt$mhealth_sum,vt$vote96,pch=20,ylab="vote or not",xlab="mental health index")
crude_lm = lm(vote96~mhealth_sum,data=vt)
abline(crude_lm,col='red')
```
       
2. The scatter plot between mental health and observed voter turnout is shown above with the overlaid linear smoothing line. We can see from the fitted line that those who suffer from depressed mood more severely are more reluctant to vote. The problem of applying linear regression model here is that the repsonse varible is meant to be binary, however using this linear model we will predict the response variable to be a real number given a mental health index, and if the given mental health index is limited within the scope from 0 to 9 as specified, the predicted values of the response variable will be in the range approximately from 0.47 to 0.76, which is hard to interpret if we want to do some further classification based on it, and it also implies that the assumed linear relationship between variable mhealth_sum and variable voter96 is problematic. 

# Basic model
```{r basic logistic,echo=TRUE}
basic_vt = glm(vote96~mhealth_sum,data=vt,family = binomial)
summary(basic_vt)
```
1. The relationship between mental health and voter turnout is statistically significant since the P value of the coefficient of variable mhealth_sum is pretty close to zero. The relationship is also substantively significant because the estimated coefficient of variable mhealth_sum is -0.14348, which amounts to a decrease of $e^{-0.14348}$=0.8663381 in odds ratio if variable mhealth_sum increases by 1 unit.

2. The estimated coefficient of variable mhealth_sum is -0.14348 means that if variable mhealth_sum increases by 1 unit, the log-odds of voter turnout will decrease by 0.14348. The relationship between mental health and the log-odds of voter turnout is shown below.
```{r log-odds,echo=TRUE}

plot(c(0:16),predict(basic_vt,list(mhealth_sum=c(0:16))),xlab = "mhealth_sum",ylab="log-odds of voter turnout",main="log-odds of voter turnout v.s. mhealth_sum", type='l')

```

3. The estimated coefficient of variable mhealth_sum is -0.14348 also means that if variable mhealth_sum increases by 1 unit from $x_{0}$, the odds of voter turnout will decrease from $e^{1.13921-0.14348*x_{0}}$ to $e^{0.99573-0.14348*x_{0}}$.The relationship between mental health and the odds of voter turnout is shown below.
```{r odds,echo=TRUE}
expr = function(x){exp(predict(basic_vt,list(mhealth_sum=x)))}
curve(expr,from=0,to=16,xlab = "mhealth_sum",ylab="odds of voter turnout",main="Odds of voter turnout v.s. mhealth_sum")

```
       
4. The estimated coefficient of variable mhealth_sum is -0.14348 also means that if variable mhealth_sum increases by 1 unit from $x_{0}$, the probability of voter turnout will decrease from $e^{1.13921-0.14348*x_{0}}/(1+e^{1.13921-0.14348*x_{0}})$ to $e^{0.99573-0.14348*x_{0}}/(1+e^{0.99573-0.14348*x_{0}})$. The relationship between mental health and the probability of voter turnout is shown below. The first difference for an increase in the mental health index from 1 to 2 is -0.0291793, and the first difference for an increase in the mental health index from 5 to 6 is -0.03477953.
```{r prob,echo=TRUE}
expr = function(x){1/(1+exp(-predict(basic_vt,list(mhealth_sum=x))))}
curve(expr,from=0,to=16,xlab = "mhealth_sum",ylab="probability of voter turnout",main="probability of voter turnout v.s. mhealth_sum")

```
     
5. We choose a threshold of 0.5 such that if the predicted probability of voter turnout is greater than or equal to 0.5, we consider this person will vote, but if the probability is less than 0.5, we predict this person will not vote. After some calculations we find the accracy rate is 0.677761. However, notice that the the modal category of the varible vote95 from the data used for training the basic model is vote95=1, which means if we blindly predict all voter turnout to be 1, the number of error using the useless classifier would be the number of 1s in the vote95 column of the dataframe we use for training the basic model, which filters out all observations with missing values in either column vote95 or column mhealth_sum, and the number of error using the useless classifier is 433. Now using the basic model, we find that the number of wrong prediction is 426, so the proportion reduction error is $(433-426)/(433)= 1.616628\%$, which means the basic model is slightly better than the useless classifier in terms of error rate of prediction.
The AUC for the basic model is 0.6243087. So the basic model using only mhealth_sum as predictor is not a good model.
```{r accuracy rate, echo=TRUE}
threshold = 0.5
pred_prob = predict(basic_vt,type="response")
# filter out observations with missing values in either column vote95 or column mhealth_sum
tmp_df = vt[complete.cases(vt[,1:2]),1:2]
tab = table(pred_prob>threshold,tmp_df$vote96)
accuracy_rate = (tab['FALSE','0']+tab['TRUE','1'])/(tab['FALSE','0']+tab['TRUE','1']+tab['FALSE','1']+tab['TRUE','0'])
print(accuracy_rate)
useless_classifier_error = tab['FALSE','0']+tab['TRUE','0']
basic_model_error = tab['FALSE','1']+tab['TRUE','0']
print(useless_classifier_error)
print(basic_model_error)
pred = prediction(pred_prob, tmp_df$vote96)
auc=as.numeric(performance(pred, "auc")@y.values)
print(auc)
```
# Multiple variable model
1.  The probability distribution is the Bernoulli distribution:   $Pr(vote96_{i}=vote96_{i})|\pi_{i})=\pi_{i}^{vote96_{i}}(1-\pi_{i})^{(1-vote96_{i})}$ where $vote96_{i}$ is the random varible of voter turnout which can only take values of 0 or 1,  therefore the repsonse variable $vote96$ is drawn from a Bernorlli distribution with probability $\pi$ of taking the value 1.  
   
    Next we choose a linear predictor $\eta_i$ as a linear function of all other regressors which are other variables: $mhealth\_sum_i$, $age_i$,$educ_i$,$black_i$,$female_i$,$married_i$ and $inc10_i$. The linear function therefore is:  
  
    $\eta_i=\alpha+\beta_1mhealth\_sum_i+\beta_2age+\beta_3educ+\beta_4black+\beta_5female+\beta_6married+\beta_7inc10$  
   
    Next since probability $\pi_i$ takes value only between 0 and 1 while $\eta_i$ could take any real number, we need a mapping or link function to map $\eta_i$ to $\pi_i$, a natural choice is the sigmoid transformation:  
  
    $\pi_i=\frac{1}{1+e^{-\eta_i}}$ then plug in the expression for $\eta_i$ above into this equation. 
    
2. 
```{r mul logistic,echo=TRUE}
multi_vt = glm(vote96~mhealth_sum + age + educ + black + female + married + inc10,data=vt,family = binomial)
summary(multi_vt)
```
The estimated intercept is -4.304103 with standard error 0.508103. The estimated coefficients of predictors mhealth_sum, age, educ, black, female, married and inc10 are -0.089102, 0.042534, 0.228686, 0.272984, -0.016969, 0.296915 and 0.069614 respectively, and the corresponding standard error 0.023642, 0.004814, 0.029532, 0.202585, 0.139972, 0.153164 and 0.026532. Among the 7 coefficients, 2 are not statistically significant, one is variable female's and and the other one is variable black's, which means these two predictors may not be strong predictors of the response variable in this model. As a whole, the multiple variable model is better than the basic model as the AIC decrease from 1620.7 to 1257.8.   

3. So if predictor black and predictor female are not strong predictors as the previous result shows, we could remove them from the model and run the logistic regression using the other variables:

```{r mul1 logistic,echo=TRUE}
multi1_vt = glm(vote96~mhealth_sum + educ+ age+married+inc10 ,data=vt,family = binomial)
summary(multi1_vt)
```
Now we observe the AIC value drops from 1257.8 to 1255.6, and more importantly, this model is more parsimonious than the previous one. We can then  estimate the accuracy rate, pRE and AUC of this model:
```{r accuracy rate mul, echo=TRUE}
threshold = 0.5
pred_prob = predict(multi1_vt,type="response")
# filter out observations with missing values in at least one column of dataframe vt
tmp_df = vt[complete.cases(vt),]
tab = table(pred_prob>threshold,tmp_df$vote96)
accuracy_rate = (tab['FALSE','0']+tab['TRUE','1'])/(tab['FALSE','0']+tab['TRUE','1']+tab['FALSE','1']+tab['TRUE','0'])
print(accuracy_rate)

multiple_model_error = tab['FALSE','1']+tab['TRUE','0']

print(multiple_model_error)
pred = prediction(pred_prob, tmp_df$vote96)
auc=as.numeric(performance(pred, "auc")@y.values)
print(auc)
```
We find that the accuracy rate increases from 0.677761 by using the basic model to 0.7201717, and the number of error prediction decreases from 426 to 326, so PRE is $(426-326)/426 = 23.47418\%$, and AUC improves from 0.6243087 to 0.758913. All these criterion indicate this multiple variable model is better than the previous basic model. 

Next we can observe that among all predictors in the model, predictor married seems the most sbustantively significant. So keeping all other predictors fixed, we can estimate the effect of years of formal education on the response variable in terms of predicted probability of voter turnout. Assume the values of all other predictors are: mhealth_sum = 3, age = 35, married=1 and inc10 =12k. We can graph the relationship between years of formal education and the probability of voter turnout as below:
```{r prob_educ,echo=TRUE}
expr = function(x){predict(multi1_vt,data.frame(mhealth_sum=3,age=35,educ=x,married=1,inc10=12),type="response")}
curve(expr,from=0,to=20,xlab = "years of formal education",ylab="probability of voter turnout",main="probability of voter turnout v.s. education years")

```
     
From the above we can see a clear trend of increase in probability of voter turnout as the voter's years of formal education increases. Keeping other predictors fixed, someone with 20-year formal education, probably with a PhD degree, is predicted to be 93% sure to vote, but the probabilty drops to around 36% for those who have only 5-year formal education. Therefore voter turnout rate will increase if our education system is improved and thus more people will become aware of the value and benefits of political participation. 

#Estimate a regression model
```{r readdata,echo=TRUE}
tv = read.csv("gss2006.csv",header=TRUE)
```
1.  The probability distribution is the Poisson distribution:   $Pr(tvhours_{i}=t)|\mu_{i})=\frac{\mu^{-tvhours_i}e^{-\mu}}{tvhours_i!}$ where $tvhours_{i}$ is the random varible of number of hours of TV watched per day which can only take values of {0,1,2...},  therefore the repsonse variable $tvhours$ is drawn from a Poisson distribution with rate $\mu$.  
   
    Next we choose a linear predictor $\eta_i$ as a linear function of all other regressors which are other variables: , $age_i$,$childs_i$,$educ_i$,$female_i$,$grass_i$,$hrsrelax_i$, $black_i$,$social\_connect_i$,$voted04_i$,$xmovie_i$,$zodiac_i$. The linear function therefore is:  
  
    $\eta_i=\alpha+\beta_1age_i+\beta_2childs_i+\beta_3educ_i+\beta_4female_i+\beta_5grass_i+\beta_6hrsrelax_i+\beta_7black_i+\beta_8social\_connect_i+\beta_9voted04_i+\beta_{10}xmovie_i+\beta_11zodiac_i$  
   
    Next since the average rate $\mu_i$ only takes postive valuewhile $\eta_i$ could take any real number, we need a mapping or link function to map $\eta_i$ to $\mu_i$, a natural choice is the exponential transformation:  
  
    $\mu_i=e^{\eta_i}$ and then plug in the expression for $\eta_i$ above into this equation. 
    
2. 
```{r poisson,echo=TRUE}
poi_tv = glm(tvhours~.-ind,data=tv,family = "poisson")
summary(poi_tv)
```
```{r poisson2,echo=TRUE}
poi_tv1 = glm(tvhours~educ+grass+hrsrelax+black,data=tv,family = "poisson")
summary(poi_tv1)
```

After initially running the poisson regression with all potential predictors and selecting out redundant variables in the order of big p-value to small p-value, four predictor remained: educ, grass, hrsrelax and black, and the model has AIC 1638. Now the linear predictor:     $\eta_i = \alpha + \beta_1educ+\beta_2grass+\beta_3hrsrelax+\beta_4hrsrelax$  
The estimated intercept is 1.252169 with standard error 0.167757, and the estimated coefficients of educ, grass, hrsrelax and black are -0.039589, -0.111854,  0.043044 and 0.452819 respectively with standard error 0.010915, 0.060572, 0.009264, 0.070783 and p-value 0.000287, 0.064803, 3.38e-06 and 1.58e-10. 

3. Generally, the coefficient of a predictor means the change in log counts of the number of hours of TV watched per day as the predictor increases by 1 unit by fixing other predictors.

Again I want to focus on the effect of years of formal education on the number of hours of TV watched per day since variable educ is one of the sigificant predictors. Assume other three predictors grass = 0, hrsrelax = 2, black = 0. The graph below shows the relationship between predicted number of hours of TV watched per day and years of education:
```{r poi_educ,echo=TRUE}

expr1 = function(x){exp(predict(poi_tv1,data.frame(grass=0,hrsrelax=2,educ=x,black=0)))}
curve(expr1,from=0,to=20,xlab = "years of formal education",ylab="number of hours of TV watched",main="TV watched hours per day v.s. education years",col='blue')
expr2 = function(x){exp(predict(poi_tv1,data.frame(grass=1,hrsrelax=2,educ=x,black=0)))}
curve(expr2,from=0,to=20,xlab = "years of formal education",ylab="number of hours of TV watched",main="TV watched hours per day v.s. education years",col='red',add =TRUE)
legend(15,3.7,legend = c("grass=0","grass=1"),col=c('blue','red'),lty=1,cex = 0.6,text.font=2)

```
    
Therefore we can see a general decreasing trend in number of TV watched hours as people's formal education years increase. A interesting observation is that those respondents who thinks marjiuana should be legalized generally spend fewer hours on watching TV than those who believe marjiuana do. Another obvious difference in TV watched hours is between black respondents and non-black respondent:

```{r poi_educ2,echo=TRUE}

expr3 = function(x){exp(predict(poi_tv1,data.frame(grass=0,hrsrelax=2,educ=x,black=0)))}
curve(expr3,from=0,to=20,xlab = "years of formal education",ylab="number of hours of TV watched",main="TV watched hours per day v.s. education years",col='blue',ylim = c(0,8))

expr4 = function(x){exp(predict(poi_tv1,data.frame(grass=0,hrsrelax=2,educ=x,black=1)))}
curve(expr4,from=0,to=20,xlab = "years of formal education",ylab="number of hours of TV watched",main="TV watched hours per day v.s. education years",col='red',add =TRUE)
legend(15,8,legend = c("non-black","black"),col=c('blue','red'),lty=1,cex = 0.6,text.font=2)

```
      
We can see that black respondents spend more time on watching TV per day than non-white respondents.  
In order to check if the model over or under-dispersed, we run the poisson regression to estimate a quasi-poisson model:
```{r poisson3,echo=TRUE}
poi_tv_quasi = glm(tvhours~educ+grass+hrsrelax+black,data=tv,family = "quasipoisson")
summary(poi_tv_quasi)
```
We can see the dispersion parameter is 1.086044, slightly bigger than 1, which means the original model is weakly over-dispersed. 