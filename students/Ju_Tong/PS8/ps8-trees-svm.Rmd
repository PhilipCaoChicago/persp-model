---
title: "Problem set #8: tree-based methods and support vector machines"
author: "Tong Ju"
date: "**March 5th 2017**"
output:
  github_document:
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE, fig.align = 'center', warning = FALSE)
# getwd()
# setwd("/Users/tongju/Desktop/MAC-Surface/CSS-HW/MACS_2017_Winter/persp-model/students/Ju_Tong/PS8")
# install.packages("gridExtra")
# install.packages("gbm")
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(rcfss)
library(pROC)
library(gbm)
library(pander)
library(e1071)


## To install the ggendro package!!!
# to get the tree graphs with the labels and values, use the forked
# version of ggdendro
# devtools::install_github("bensoltoff/ggdendro")
library(ggdendro)

# Set the parameters
options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())

#import the dataset
biden = read_csv('data/biden.csv')
mh = read_csv('data/mental_health.csv')
simpson = read_csv('data/simpson.csv')


```


# Part 1: Sexy Joe Biden (redux times two) [3 points]

###1.Split the data into a training set (70%) and a validation set (30%). **Be sure to set your seed prior to this part of your code to guarantee reproducibility of results.**

```{r 1-split-data}
set.seed(1234)
# Factorize some string variables in the dataset
biden_fact<-biden %>%
  mutate (female = factor(female, levels =0:1, labels = c("male", "female")),
          dem = factor (dem, levels =0:1, labels = c("non-dem","dem")),
          rep = factor (rep, levels =0:1, labels = c("non-rep", "redp")))

#split the data set to training/test set (70%:30%) as required:
biden_split <- resample_partition(biden_fact, c(test = 0.3, train = 0.7))

```

###2.Fit a decision tree to the training data, with `biden` as the response variable and the other variables as predictors. Plot the tree and interpret the results. What is the test MSE?
    * Leave the control options for `tree()` at their default values

```{r 1-tree1}

# define the MSE() function:
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

# Make the tree model of Biden data (full tree model)
tree_biden <- tree(biden ~ ., data = biden_split$train)

# Plot the tree:
tree_data <- dendro_data(tree_biden)

ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = 'Decision Tree for Warmth towards Biden ', 
       subtitle = 'Default controls, all predictors as independent vairables')

leaf_vals <- leaf_label(tree_data)$yval
test_mse <- mse(tree_biden, biden_split$test)


```    

Based on the training data, the full tree model by using the `biden` as the responsive variable, and the other variables as independent variables is established, with the control parameters defaulted. 

There are three nodes ((internal and terminal) in this decision tree,in which the braches of the tree are dependent are the political affilication of the respendents (Democratic, Republican, and neither Democratic nor Republican). The result of this tree model could be simply interpreted as below:

If the respondent affiliated with Democratic party, then the model estimates the warmth toward Biden to be `r leaf_vals[3]`; 

If the respondent not affiliated with Democratic party, then we proceed down the left branch to the next internal node:

If he/she is Republican, then the model estimates the warmth toward Biden to be `r leaf_vals[2]`; Otherwise, if neither Republican nor Democratic, the estimated warmth is predicted to be `r leaf_vals[1]`.

In addition, the MSE score of this full tree model is `r test_mse`  
    
###3.Now fit another tree to the training data with the following `control` options:
Use cross-validation to determine the optimal level of tree complexity, plot the optimal tree, and interpret the results. Does pruning the tree improve the test MSE?

First, under the controlled condition, the full tree based on the training data is shown below, with 192 terminal nodes. And the MSE value is 481. 
```{r 1-tree-full}

tree_biden <- tree(biden ~ ., data = biden_split$train, 
                   control = tree.control(nobs = nrow(biden_split$train),
                            mindev = 0))

tree_data<-dendro_data(tree_biden)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = 'Decision Tree for Warmth towards Biden (Before Optimization)', 
       subtitle = 'Under controlled condition, all predictors as independent vairables')

full_mse <- mse(tree_biden, biden_split$test)


```    

Then, by using the cross-validation appraoch, I plot all the MSE value for the trees with various terminal nodes number. As a result, we find the minimal cross-validated test MSE is for 4 terminal nodes. 

```{r 1-tree-kfold}
# in this step, the MSE is based on the 10 fold-validation using the whole dataset
# then figure out the optimized number for terminal nodes

# generate 10-fold CV trees
biden_cv <- crossv_kfold(biden_fact, k = 10) %>%
  mutate(tree = map(train, ~ tree(biden ~ ., data = .,
     control = tree.control(nobs = nrow(biden_fact),
                            mindev = 0))))

# calculate each possible prune result for each fold
biden_cv <- expand.grid(biden_cv$.id, 2:15) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,k = Var2) %>%
  left_join(biden_cv, by = ".id") %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         mse = map2_dbl(prune, test, mse)) 


biden_cv %>%
  select(k, mse) %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of terminal nodes",
       y = "Test MSE",
       title = "MSE vs. Terminal Node Number (Cross-validation Approach)")

```    

Therefore, under the controlled condition, I established the optimized tree for the training data with 4 terminal nodes, as follows. 

```{r 1-tree-optimized}
# the tree before the pruning
tree_biden <- tree(biden ~ ., data = biden_split$train, 
                   control = tree.control(nobs = nrow(biden_split$train),
                            mindev = 0))
# set the node number as 4
mod <- prune.tree(tree_biden, best = 4)

tree_data<-dendro_data(mod)

ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = 'Optimized Decision Tree for Warmth towards Biden', 
       subtitle = 'Under controlled condition, all predictors as independent vairables')

opt_mse <- mse(mod, biden_split$test)
```    

In conclusion, the pruning indeed improve the MSE score: before the pruning, the full model has the MSE as 481, while it is reduced to 407 after the optimization (with 4 nodes). In addition, we can interpret this tree as below:

If the respondent affiliated with Democratic party, then we proceed down to right branch to the next internal node: if the respondent is younger than 53.5, the estimated warmth score toward Biden is approximaely 78.64; otherwise, it is 71.86.

If the respondent not affiliated with Democratic party, then we proceed down the left branch to the next internal node:if he/she is Republican, then the model estimates the warmth toward Biden to be 43.23; Otherwise, if neither Republican nor Democratic, the estimated warmth is predicted to be 57.60. 

###4. Use the bagging approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results.

```{r 1-bagging}
biden_bag <- randomForest(biden ~ ., data = biden_split$train,
                          mtry=5,
                          ntree=500)

bagging_mse <- mse(biden_bag , biden_split$test)

data_frame(var = rownames(importance(biden_bag)),
           MeanDecreaseGini = importance(biden_bag)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting Warmth Score toward Biden",
       subtitle = "Bagging Appraoch",
       x = NULL,
       y = "Average decrease in the Gini Index")
```    

The model established by using the bagged approach (trees number = 500, and all predictors should be considered for each split of the tree) can explain only 9.16 variance in the responsive variable considering all the independent variables. In addition, the MSE score is 486, much higher than all the MSE score in the optimized single tree model.  

Although interpreting a bagged model is much more difficult than interpreting a single decision tree, from the above plots of variable importance, we can still clearly see how the responsive variable is influenced by the independent variables in terms of the reduction in Gini index. For the bagged model, age, democratic affliation are the most important predictors. The variable education also has some impact on the warmth score towards Biden. However, the republican affiliation and gender are the unimportant predictors. 

###5. Use the random forest approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results. Describe the effect of $m$, the number of variables considered at each split, on the error rate obtained.


```{r 1-bagging-sample-number}
biden_rf <- randomForest(biden ~ ., data = biden_split$train, 
                          ntree = 500)

rf_mse <- mse(biden_rf , biden_split$test)

seq.int(biden_bag$ntree) %>%
  map_df(~ getTree(biden_bag, k = ., labelVar = TRUE)[1,]) %>%
  count(`split var`) %>%
  knitr::kable(caption = "Variable used to generate the first split in each tree (Bagging)",
               col.names = c("Variable used to split", "Number of training observations"))

seq.int(biden_rf$ntree) %>%
  map_df(~ getTree(biden_rf, k = ., labelVar = TRUE)[1,]) %>%
  count(`split var`) %>%
  knitr::kable(caption = "Variable used to generate the first split in each tree (Random Forest)",
               col.names = c("Variable used to split", "Number of training observations"))

data_frame(var = rownames(importance(biden_rf)),
           `Random forest` = importance(biden_rf)[,1]) %>%
  left_join(data_frame(var = rownames(importance(biden_rf)),
           Bagging = importance(biden_bag)[,1])) %>%
  mutate(var = fct_reorder(var, Bagging, fun = median)) %>%
  gather(model, gini, -var) %>%
  ggplot(aes(var, gini, color = model)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting Warmth Score toward Biden",
       x = NULL,
       y = "Average decrease in the Gini Index",
       color = "Method")
```    

Compared with the bagged model, which considered all the predictors in the splitting (m = p), the random forest model with m = p/3 has much lower MSE, 409 (while bagging model's test MSE score is 486).In addition this random forest model can explain 25.8 % of the variance (also much higher than the bagged model's 9.16%). 

In the above plot, we can conclude that, compared with the bagged model, age is no longer important in the random forest model, while the affilication with republilcan will be much more important. Except `rep`, we can also observe that the average decrease in the Gini index associated with each variable is generally smaller using the random forest method compared to bagging - this is because of the variable restriction imposed when considering splits. To sum up, the choice m = p/3 gave a significant improvement over bagging (m = p) in this example. 


###6. Use the boosting approach to analyze the data. What test MSE do you obtain? How does the value of the shrinkage parameter $\lambda$ influence the test MSE?

In this section, first, I explore the optimal number of boosting iterations for the boosting models with various interaction depth 1, 2, or 4. Then by using the optimized number, I calculate out the MSE for each boosting model. It appears that at the optimized number of boosting iterations, both boosting model with depth2 and depth4 have the similar and small enough MSE. Therefore, in the next step, I determine to set the `interaction.depth` as 4 and `n.trees` as 2080, and subsequently investigate how the $\lambda$ influence the test MSE.
```{r 1-boosting-optimal}
set.seed(1234)


# Define a function to calculate the MSE for each bossting model
mse_biden_boost <-function(model, test, tree_number) {
  yhat.boost <- predict (model, newdata = test, n.trees=tree_number)
  mse <- mean((yhat.boost - (as_tibble(test))$biden)^2)
  return (mse)
}

biden_models <- list("boosting_depth1" = gbm(biden ~ .,
                                               data = biden_split$train,
                                               n.trees = 5000, interaction.depth = 1),
                       "boosting_depth2" = gbm(biden - 1 ~ .,
                                               data = biden_split$train,
                                               n.trees = 5000, interaction.depth = 2),
                       "boosting_depth4" = gbm(biden~ .,
                                               data = biden_split$train,
                                               n.trees = 5000, interaction.depth = 4))

data_frame(depth = c(1, 2, 4),
           model = biden_models[c("boosting_depth1", "boosting_depth2", "boosting_depth4")],
           optimal = map_dbl(model, gbm.perf, plot.it = FALSE),
           mse = c(mse_biden_boost(biden_models$boosting_depth1,biden_split$test, 3248),
                   mse_biden_boost(biden_models$boosting_depth2,biden_split$test, 2551),
                   mse_biden_boost(biden_models$boosting_depth4,biden_split$test, 2080))) %>%
  select(-model) %>%
  knitr::kable(caption = "Optimal number of boosting iterations",
               col.names = c("Depth", "Optimal number of iterations", "MSE"))





``` 

The default shrinkage number is 0.001, so I try different shrinkage number from 0.00025 to 0.4, and plot the MSE vs. the shrinkage number. As the blot below, I found that test MSE score first decrease to reach its minimal value = 402 (when $\lambda$ = 0.002), then increases as the $\lambda$ increases. Since $\lambda$, essentially, is a shrinkage parameter which controls the rate at which boosting learns, very small  $\lambda$ will require very large value of B in order to achieve the good performance. In our case, given the tree number, if the lambda is too small, then the B is not big enough for the model to achieve good result.However, in our case, when the $\lambda$ to a rather large number (larger than 0.1), the MSE also increases, the accuracy of the model decreases.  

In conclusion, based on the optimized number of boosting iterations, I choose the best boosting model as B = 2080, depth =4, $\lambda$ = 0.002, which gave the MSE as 402, the lowest MSE in all the models. 


```{r 1-boosting-shrinkage}

s <- c(0.00025, 0.0005, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4)

MSE<-list()
for (i in s) {
  boost <- gbm(biden~ .,data = biden_split$train,n.trees = 2080, interaction.depth = 4, shrinkage = i)
  MSE <- append(MSE, mse_biden_boost(boost,biden_split$test, 2080))
}

MSE_lambda<-data_frame (shrinkage = s,
            MSE = unlist(MSE))

pander(MSE_lambda)

ggplot(MSE_lambda, aes(x=shrinkage, y=MSE)) +
  geom_line()+
  labs(x = "Shrinkage parameter",
       y = "test MSE",
       title = "MSE vs. Shrinkage parameter (0.00025 ~ 0.4) for Boosting",
       subtitle ="num. of trees = 2080, interaction depth = 4")
  
```    



# Part 2: Modeling voter turnout [3 points]


###1. Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five tree-based models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)

In this section, I have built up 5 trees/forests: 1) singel tree by using all independent variables; 2) bagging approach; 3) random forest; 4) single tree by using the age as the independent variabes; 5) single tree with full nodes. 

#####Model 1: simple tree with all independent variables
```{r 2-tree-1}
set.seed(1234)
# factorize the original data
mh <- read_csv("data/mental_health.csv") %>% 
  mutate(vote96 = factor(vote96, levels = 0:1, labels = c("xvote", "vote")), black = factor(black), female = factor(female), married = factor(married))

na.omit(mh)
#split the data
mh_split <- resample_partition(mh, p = c("test" = .3, "train" = .7))
# Define the error rate function 
err.rate.tree <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])

  pred <- predict(model, newdata = data, type = "class")
  actual <- data[[response]]

  return(mean(pred != actual, na.rm = TRUE))
}
##########################################################

# Model 1) simple tree with all the independent variables
tree_default <- tree(vote96 ~ ., data = mh_split$train)

#Plot tree
tree_data <- dendro_data(tree_default)

ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), alpha = 0.5) +
  geom_text(data = label(tree_data), aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter turnout tree",
       subtitle = "mhealth_sum + age + educ + black + female + married + inc10")

#ROC
fitted <- predict(tree_default, as_tibble(mh_split$test), type = "class")

roc_td <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_td, main = "ROC for model 1")
auc1 <-auc(roc_td)

#Mse
mse1 <- err.rate.tree(tree_default, mh_split$test)


#PRE
real <- as.numeric(na.omit(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- mse1 
PRE1 <- (E1 - E2) / E1

```  

This decision tree is established with full nodes by using all the independent variables. Test error rate is 28.1%, AUC is 0.638 and the PRE is 4.29%, meaning when compared to the NULL model, estimating all with the median data value, this model decreases the error rate by only 4.29%. The tree model stress the age, education, income and mental health status. The tree model can be explained as below:

If age < 44.5, then we proceed down the left branch to the next internal node.

If educ < 12.5 (year), then the model estimates the person don't vote.
If educ >= 12.5, then we proceed down the right branch to the next internal node.
If the mental health index < 4.5, then the model estimates the person vote.
If the mental health index >= 4.5, then the model estimates the person don't vote.
If age >= 44.5, then we proceed down the right branch to the next internal node.

If inc10 < 1.08335 (in $10,000s), then the model estimates the person vote.
If inc10 > 1.08335, the we proceed down the right branch to the next internal node.
If the mental health index < 4.5, then the model estimates the person vote.
If the mental health index >= 4.5, then the model estimates the person vote.


#####Model 2: Bagging approach
```{r 2-tree-2}
set.seed(1234)

# Model 2) bagging
bagging <- randomForest(vote96 ~ ., data = na.omit(as_tibble(mh_split$train)), mtry = 7, ntree = 500)

data_frame(var = rownames(importance(bagging)),
           MeanDecreaseRSS = importance(bagging)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseRSS, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseRSS)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting voter turnout",
       subtitle = "Bagging",
       x = NULL,
       y = "Average decrease in the Gini Index")



#ROC
fitted <- predict(bagging, as_tibble(mh_split$test), type = "prob")[,2]

roc_td <- roc(as.numeric(as_tibble(mh_split$test)$vote96), fitted)
plot(roc_td, main = "ROC for model 2")
auc2 <-auc(roc_td)

#Mse
mse2 <- err.rate.tree(bagging, mh_split$test)


#PRE
real <- as.numeric(na.omit(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- mse2
PRE2 <- (E1 - E2) / E1

```  

Through bagging approach, with all predictor variables, the model emphasizes age, inc10, educ, and mental health and has a test error rate 31.5% (estimated by out-of-bag error estimate), which is higher than the default one and indicates a potential overfitting problem. Also, the AUC us 0.73 and the PRE is -7.26%, meaning when compared to the NULL model, estimating all with the median data value, this model even increases the error rate by 7.26%.


#####Model 3: Random Forest
```{r 2-tree-3}
set.seed(1234)

# Model 3) bagging
rf<- randomForest(vote96 ~ ., data = na.omit(as_tibble(mh_split$train)), ntree = 500)

data_frame(var = rownames(importance(rf)),
           MeanDecreaseRSS = importance(rf)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseRSS, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseRSS)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting voter turnout",
       subtitle = "Random Forest",
       x = NULL,
       y = "Average decrease in the Gini Index")



#ROC
fitted <- predict(rf, as_tibble(mh_split$test), type = "prob")[,2]

roc_td <- roc(as.numeric(as_tibble(mh_split$test)$vote96), fitted)
plot(roc_td, main = "ROC for model 3")
auc3 <-auc(roc_td)

#Mse
mse3 <- err.rate.tree(rf, mh_split$test)


#PRE
real <- as.numeric(na.omit(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- mse3
PRE3 <- (E1 - E2) / E1

```   

Similar as in the bagging appraoch, the random forest also stresses age, income, education and mental health status, with the test error rate as 28.4%, AUC as 0.766, PRE as 3.18%.  

#####Model 4: Simple tree with only age
```{r 2-tree-4}
tree_age <- tree(vote96 ~ age, data = mh_split$train)

#Plot tree
tree_data <- dendro_data(tree_age)

ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), alpha = 0.5) +
  geom_text(data = label(tree_data), aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter turnout tree",
       subtitle = " vote96 ~ age ")

#ROC
fitted <- predict(tree_default, as_tibble(mh_split$test), type = "class")

roc_td <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_td, main = "ROC for model 4")
auc4 <-auc(roc_td)

#Mse
mse4 <- err.rate.tree(tree_age, mh_split$test)


#PRE
real <- as.numeric(na.omit(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- mse4
PRE4<- (E1 - E2) / E1

```      

In this model, with the default condition, I only include the education as the independent variable. The results show that the error rate is 0.293, PRE is 0 (indicating the model has a performance equals to the NULL model), and AUC as 0.638.


#####Model 5: Simple tree with controlled condition
```{r 2-tree-5}
tree_control <- tree(vote96 ~ ., data = mh_split$train, control = tree.control(nobs = nrow(mh_split$train), mindev = 0))

#Plot tree
tree_data <- dendro_data(tree_control)

ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), alpha = 0.5) +
  geom_text(data = label(tree_data), aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter turnout tree",
       subtitle = " all the independent variables, under controlled condition ")

#ROC
fitted <- predict(tree_control, as_tibble(mh_split$test), type = "class")

roc_td <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_td, main = "ROC for model 5")
auc5 <-auc(roc_td)

#Mse
mse5 <- err.rate.tree(tree_control, mh_split$test)


#PRE
real <- as.numeric(na.omit(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- mse5
PRE5<- (E1 - E2) / E1

```      

In this model, with the default condition, I only include the education as the independent variable. The results show that the error rate is 0.293, PRE is 0 (indicating the model has a performance equals to the NULL model), and AUC as 0.614.

To sum up, according to the error rate and PRE, the first single tree model (model 1) is the bset in all five models, sicne it has the lowest error rate and highest PRE value. However, the AUC number of model 1 is not the highest in all five models. From this aspect, the best model is the model 3 (random forests). 
Nevertheless, considering the AUC with single turning point in the sigle tree model, the AUC is not so informative, thus is not so imprtant when comparing the models.  

```{r 2-tree-6}

sum<-data_frame("model"=c("model1","model2","model3","model4","model5"),
           "Error Rate"=c(mse1,mse2,mse3,mse4,mse5),
           "PRE" =c(PRE1,PRE2,PRE3,PRE4,PRE5),
           "AUC" =c(auc1,auc2,auc3,auc4,auc5)
           )

pander(sum)
```      



###2. Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five SVM models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)


####Model 1:  Linear Kernel with age

```{r 2-SVM-1}
library(e1071)
set.seed(1234)

mh_split <- resample_partition(na.omit(mh), p = c(test = 0.3, train = 0.7))

mh_lin_tune <- tune(svm, vote96 ~ age, data =  as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_lin_tune)
# Best 
mh_lin <- mh_lin_tune$best.model
summary(mh_lin)

# ROC

fitted <- predict(mh_lin, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_line, main = "ROC of Voter Turnout - Linear Kernel, Partial Model")

auc1<-auc(roc_line)

#PRE
real <- na.omit(as.numeric(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.292
PRE1 <- (E1 - E2) / E1


```      

Using linear kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100), the model gets the best cost level at 1 to 7 and has a 10-fold CV error rate 29.2%.  In addition, the AUC us 0.691 and the PRE is 12.9% (the model MSE is estimated by the 10-fold error rate), meaning when compared to the NULL model, estimating all with the median data value, this model decreases the error rate by 12.9%. 


####Model 2:  Linear Kernel with all independent variables

```{r 2-SVM-2}
set.seed(1234)
mh_lin_tune2 <- tune(svm, vote96 ~ ., data =  as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_lin_tune2)
# Best 
mh_lin2 <- mh_lin_tune2$best.model
summary(mh_lin2)

# ROC

fitted <- predict(mh_lin2, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_line2 <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_line2, main = "ROC of Voter Turnout - Linear Kernel, Full Model")

auc2<-auc(roc_line2)

#PRE
real <- na.omit(as.numeric(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.264
PRE2 <- (E1 - E2) / E1


```      

Using linear kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100), the model gets the best cost level at 3 to 6 and has a 10-fold CV error rate 26.4 %.  In addition, the AUC us 0.733 and the PRE is 21.3% (the model MSE is estimated by the 10-fold error rate), meaning when compared to the NULL model, estimating all with the median data value, this model decreases the error rate by 21.3%. Based on PRE, AUC and error rate, this full model is better than the partial model above. 

####Model 3:  Polynomial kernel (full model)

```{r 2-SVM-3}

set.seed(1234)

mh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_poly_tune)

#Best
mh_poly <- mh_poly_tune$best.model
summary(mh_poly)

# ROC

fitted <- predict(mh_poly, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_poly, main = "ROC of Voter Turnout - Polinominal Kernel, Full Model")

auc3<-auc(roc_poly)

#PRE
real <- na.omit(as.numeric(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.263
PRE3 <- (E1 - E2) / E1


```      

Using linear kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100), the model gets the best cost level at 4 and has a 10-fold CV error rate 26.3 %.  In addition, the AUC us 0.702 and the PRE is 21.5% (the model MSE is estimated by the 10-fold error rate), meaning when compared to the NULL model, estimating all with the median data value, this model decreases the error rate by 9.92%. It appears this polynominal kernal is worse than the linear one. 

####Model 4:  Radial kernel (full model)

```{r 2-SVM-4}
set.seed(1234)


mh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_rad_tune)

mh_rad <- mh_rad_tune$best.model
summary(mh_rad)
# ROC

fitted <- predict(mh_rad, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_rad, main = "ROC of Voter Turnout - Radial Kernel, Full Model")

auc4<-auc(roc_rad)

#PRE
real <- na.omit(as.numeric(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.22
PRE4 <- (E1 - E2) / E1


```      

Using linear kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100), the model gets the best cost level at 4 and has a 10-fold CV error rate 22.0 %.  In addition, the AUC us 0.738 and the PRE is 12.9% (the model MSE is estimated by the 10-fold error rate), meaning when compared to the NULL model, estimating all with the median data value, this model decreases the error rate by 34.4%. This model is stil not as good as the linear full model.

####Model 5:  Sigmoid kernel (full model)

```{r 2-SVM-5}
set.seed(1234)


mh_sig_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "sigmoid",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_sig_tune)

mh_sig <- mh_sig_tune$best.model
summary(mh_sig)
# ROC

fitted <- predict(mh_sig, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_sig <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_sig, main = "ROC of Voter Turnout - Radial Kernel, Full Model")

auc5<-auc(roc_sig)

#PRE
real <- na.omit(as.numeric(as_tibble(mh_split$test)$vote96))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.247
PRE5 <- (E1 - E2) / E1


```      

Using linear kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100), the model gets the best cost level at 3 and has a 10-fold CV error rate 26.3 %.  In addition, the AUC us 0.603 and the PRE is 24.7% (the model MSE is estimated by the 10-fold error rate), meaning when compared to the NULL model, estimating all with the median data value, this model decreases the error rate by 24.7%. This model is even worse than the radial kernal. 

```{r 2-SVM-sum}



#plot(roc_line2, print.auc = TRUE, col = "red", main ="ROC of Voter Turnout - 5 models")
#plot(roc_poly, print.auc = TRUE, col = "blue", print.auc.y = .4, add = TRUE)
#plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
#plot(roc_sig, print.auc = TRUE, col = "green", print.auc.y = .2, add = TRUE)
#plot(roc_line, print.auc = TRUE, col = "black",print.auc.y = .1, add = TRUE)


sum2<-data_frame("model"=c("model1","model2","model3","model4","model5"),
           "Error Rate"=c(0.329, 0.297, 0.302, 0.292, 0.319),
           "PRE" =c(PRE1,PRE2,PRE3,PRE4,PRE5),
           "AUC" =c(auc1,auc2,auc3,auc4,auc5)
           )

pander(sum2)

```      


Based on the data summary in the above table, model 2 and model 4 both have the alomst similar and small error rate in five models. However, model 4 has a little bit larger PRE than model 2,and the model 4 has the highest AUC value. From the plot above, it is also obvious that for model 4, there are more area under the curve than the other curves. Therefore, the model 4(radial) is the best one, then model 2(full linear), then the model 3 (polynominal), the model 5(sigmodial), the worse one is the model 1(partial linear). However, the SVM models is good at prediction rather than interpretation and do not provide clear ways for interpreting the relative importance and influence of individual predictors on the separating hyperplane. 



# Part 3: OJ Simpson [4 points]
You can make full use of any of the statistical learning techniques to complete this part of the assignment:

* Linear regression
* Logistic regression
* Generalized linear models
* Non-linear linear models
* Tree-based models
* Support vector machines
* Resampling methods

Select methods that are appropriate for each question and **justify the use of these methods**.

###1. What is the relationship between race and belief of OJ Simpson's guilt? Develop a robust statistical learning model and use this model to explain the impact of an individual's race on their beliefs about OJ Simpson's guilt.

In this section, I plan to employ the logistic regression, single tree and random forest models, for their ability of providing clearer interpretations about beliefs of OJ Simpson's guilt explained by an individual's race (include black and hispanic). I also split the data into 30% testing and 70% training sets for cross validating their fittness.

####Single Tree
```{r 3-race-tree}
set.seed(1234)
simpson <- read_csv("data/simpson.csv") %>%
  mutate_each(funs(as.factor(.)), guilt, dem, rep, ind, female, black, hispanic, educ, income)
#Split data
simpson_split <- resample_partition(simpson, c(test = 0.3, train = 0.7))

#Grow tree
simpson_tree_default <- tree(guilt ~ black + hispanic, data = simpson_split$train)

#Plot tree
tree_data <- dendro_data(simpson_tree_default)

ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), alpha = 0.5) +
  geom_text(data = label(tree_data), aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Simpson guilt opinion tree",
       subtitle = "black + hispanic")

#ROC
fitted <- predict(simpson_tree_default, as_tibble(simpson_split$test), type = "class")

roc_t <- roc(as.numeric(as_tibble(simpson_split$test)$guilt), as.numeric(fitted))
plot(roc_t)

auc1<-auc(roc_t)

#Accuracy
MSE1 <- err.rate.tree(simpson_tree_default, simpson_split$test)

#PRE
real <- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(real != median(real)))
E2 <- MSE1
PRE1 <- (E1 - E2) / E1


```      


As for the single tree model with default setting, it gives us a 17.02% test error rate, a 0.4341 PRE, and a 0.744 AUC, exactly the same as we got from the logistic model. Basically, the tree model uses only black to estimate the guilt belief.

If the person is not black, the model estimates that the person would believe Simpson is guilty.
If the person is black, the model estimates that the person would believe Simpson is not guilty.


####Random Forest

```{r 3-race-forest}
set.seed(1234)

simpson_rf <- randomForest(guilt ~ black + hispanic, data = na.omit(as_tibble(simpson_split$train)), ntree = 500)
simpson_rf

data_frame(var = rownames(importance(simpson_rf)),
           MeanDecreaseRSS = importance(simpson_rf)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseRSS, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseRSS)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting opinion on Simpson guilty",
       subtitle = "Random forest",
       x = NULL,
       y = "Average decrease in the Gini Index")

#ROC
fitted <- predict(simpson_rf, na.omit(as_tibble(simpson_split$test)), type = "prob")[,2]

roc_rf <- roc(na.omit(as_tibble(simpson_split$test))$guilt, fitted)
plot(roc_rf)

auc2<-auc(roc_rf)

#PRE
real <- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.1843
MSE2<-E2
PRE2 <- (E1 - E2) / E1



```      

As for the random forest model with default setting and 500 trees, it gives us a 19.1% test error rate (estimated by out-of-bag error estimate) and a 38.71% PRE, both are worse than the previous two models. However, the random forest model has a 0.745 AUC at a similar level as the previous model doed. Regarding the predictor importance, the black has a way higher average decrease in the Gini index than hispanic, which indicates black's importance and confirms the results from the previous model.

#### Logistic model

```{r 3-race-logistic}
set.seed(1234)

getProb <- function(model, data){
  data <- data %>% 
    add_predictions(model) %>% 
    mutate(prob = exp(pred) / (1 + exp(pred)),
           pred_bi = as.numeric(prob > .5))
  return(data)
}



model_logistic <- glm(guilt ~ black + hispanic, data = simpson_split$train, family = binomial)
summary(model_logistic)

df_logistic_test <- getProb(model_logistic, as.data.frame(simpson_split$test))

#ROC
auc3 <- auc(df_logistic_test$guilt, df_logistic_test$pred_bi)


#Accuracy
accuracy3 <- mean(df_logistic_test$guilt == df_logistic_test$pred_bi, na.rm = TRUE)


#PRE
real <- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 1 - accuracy3
MSE3<-E2
PRE3 <- (E1 - E2) / E1


```      

In the logistic model by using the `black` and `haspanic` as two independent variables, it gives the 17.00% test error rate, a 0.434 PRE, and a 0.744 AUC. As shown above, both two independent variables included in the model have statistically significant relationships with the guilt, with black (p-value < 2e-16) at a 99.9% confidence level and hispanic (p-value = 0.058) at a 90% confidence level. Both of them have negative relationship with the responsive variable, which means that when the respondent is black (parameter est. = -3.0529) or hispanic (parameter est. = -0.5388), the predected probability of believing Simpson's guilty has a differential increase. Black has a even stronger power than hispanic as it has a smaller p-value and a larger parameter absolute value. The amount of the change in the probability depends on the initial value of the changing independent variable. To interpret the co-effecient in this model: being as a black people, the odds of thinking Simpson guilty will only be exp(-3.0529)=4.72% than those non-black people; being as a haspanic people, the odds of thinking Simpson guilty will only be exp(-0.5388)=58.3% than those non-haspanic people.


```{r 3-race-logistic2}
logistic_grid <- as.data.frame(simpson_split$test) %>%
  data_grid(black, hispanic) %>%
  add_predictions(model_logistic) %>% 
  mutate(prob = exp(pred) / (1 + exp(pred)))

ggplot(logistic_grid, aes(black, pred, group = factor(hispanic), color = factor(hispanic))) +
  geom_line() +
  scale_color_discrete(name = "Hispanic or not (hispanic = 1)") +
  labs(title = "Log-odds of guilt belief",
       subtitle = "by race",
       x = "Black or not (black = 1)",
       y = "Log-odds of voter turnout")

ggplot(logistic_grid, aes(black, prob, group = factor(hispanic), color = factor(hispanic))) +
  geom_line() +
  scale_color_discrete(name = "Hispanic or not (hispanic = 1)") +
  labs(title = "Predicted probability of guilt belief",
       subtitle = "by race",
       x = "Black or not (black = 1)",
       y = "Predicted probability of voter turnout")

```    

The above graphs illustrate the relationship between black, hispanic status, and the belief of Simpson's guilty. In the graph Log-odds of guilt belief, we observe the mentioned negative relationship between race and guilt belief log-odds. The log-odds goes down when people are black, as the lines with a negative slope. In addition, hispanic people have a line below the non-hispanic people. That is, The log-odds of guilt belief is lower for hispanic people. This could be because the the similarity between the respondents and Simpson in terms of race. 

While both the logistic model and tree model perform well, I'll choose the logistic model as my final model, since its interpretability in terms of single relationship direction, the strength of effect, and the specific amount of effect in estimation. I thus redo the logistic model with a 100-time 10-fold cross validation to examine its robustness.

```{r 3-race-validation}


sum3<-data_frame("model"=c("model1","model2","model3"),
           "Error Rate"=c(MSE1, MSE2, MSE3),
           "PRE" =c(PRE1,PRE2,PRE3),
           "AUC" =c(auc1,auc2,auc3)
           )

pander(sum3)


fold_model_mse <- function(df, k){
  cv10_data <- crossv_kfold(df, k = k)
  cv10_models <- map(cv10_data$train, ~ glm(guilt ~ black + hispanic, family = binomial, data = .))
  cv10_prob <- map2(cv10_models, cv10_data$train, ~getProb(.x, as.data.frame(.y)))
  cv10_mse <- map(cv10_prob, ~ mean(.$guilt != .$pred_bi, na.rm = TRUE))
  return(data_frame(cv10_mse))
}

set.seed(1234)
mses <- rerun(100, fold_model_mse(simpson, 10)) %>%
  bind_rows(.id = "id")

ggplot(data = mses, aes(x = "MSE (100 times 10-fold)", y = as.numeric(cv10_mse))) +
  geom_boxplot() +
  labs(title = "Boxplot of MSEs - logistic model",
       x = element_blank(),
       y = "MSE value")

mse_100cv10 <- mean(as.numeric(mses$cv10_mse))
mseSd_100cv10 <- sd(as.numeric(mses$cv10_mse))
mse_100cv10
mseSd_100cv10


```    

In the final step, by using the cross validation, I intend to examine the robustness of my logistic model. The model gets a `r mse_100cv10 * 100`% average error rate, which is still pretty good, with a small std of the error rate at `r mseSd_100cv10`.



###2. How can you predict whether individuals believe OJ Simpson to be guilty of these murders? Develop a robust statistical learning model to predict whether individuals believe OJ Simpson to be either probably guilty or probably not guilty and demonstrate the effectiveness of this model using methods we have discussed in class.

In this section, the  SVM with linear, polynomimal, radial, sigmoid kernel and single tree models were examined for their ability to predict the beliefs of OJ Simpson's guilt explained by all the available predictors. 

####SVM, Linear Kernel
```{r 3-2-0}

set.seed(1234)

simpson_lin_tune <- tune(svm, guilt ~ dem + rep + age + educ + female + black + hispanic + income, data = na.omit(as_tibble(simpson_split$train)),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(simpson_lin_tune)

simpson_lin <- simpson_lin_tune$best.model
summary(simpson_lin)

#Best
simpson_lin <- simpson_lin_tune$best.model
summary(simpson_lin)

#ROC
fitted <- predict(simpson_lin, as_tibble(simpson_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(simpson_split$test)$guilt, fitted$decision.values)
plot(roc_line)

auc(roc_line)

#PRE
real <- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.1905
PRE <- (E1 - E2) / E1
PRE

```  

Using linear kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100), the model gets the best cost level at 0.01, and a 19.1% 10-fold CV error rate. Also, the AUC us 0.796 and the PRE is 36.6% (the model MSE is estimated by the 10-fold error rate).


####SVM, polynominal Kernel
```{r 3-2-1}

set.seed(1234)

simpson_poly_tune <- tune(svm, guilt ~ dem + rep + age + educ + female + black + hispanic + income, data = na.omit(as_tibble(simpson_split$train)),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(simpson_poly_tune)

simpson_poly <- simpson_poly_tune$best.model
summary(simpson_poly)

#Best
simpson_poly <- simpson_poly_tune$best.model
summary(simpson_poly)

#ROC
fitted <- predict(simpson_poly, as_tibble(simpson_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(simpson_split$test)$guilt, fitted$decision.values)
plot(roc_poly)

auc(roc_poly)

#PRE
real <- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.1986
PRE <- (E1 - E2) / E1
PRE

```  

Using polynomial kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100) and different degree levels (3, 4, and 5), the model gets the best cost level at 10, degree level at 3, and a 19.9% 10-fold CV error rate. Also, the AUC as 0.766 and the PRE is 34.00% (the model MSE is estimated by the 10-fold error rate). Generally, the model is slightly worse than the linear one.

####SVM, radial Kernel
```{r 3-2-2}

set.seed(1234)

simpson_rad_tune <- tune(svm, guilt ~ dem + rep + age + educ + female + black + hispanic + income, data = na.omit(as_tibble(simpson_split$train)),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(simpson_rad_tune)

simpson_rad <- simpson_rad_tune$best.model
summary(simpson_rad)

#Best
simpson_rad <- simpson_rad_tune$best.model
summary(simpson_rad)

#ROC
fitted <- predict(simpson_rad, as_tibble(simpson_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(simpson_split$test)$guilt, fitted$decision.values)
plot(roc_rad)

auc(roc_rad)

#PRE
real <- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.1905
PRE <- (E1 - E2) / E1
PRE

```  

Using polynomial kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100) and different degree levels (3, 4, and 5), the model gets the best cost level at 10, degree level at 3, and a 19.86% 10-fold CV error rate. Also, the AUC as 0.771 and the PRE is 36.6% (the model MSE is estimated by the 10-fold error rate). Generally, the model is slightly worse than the linear one.

####SVM, sigmodial Kernel
```{r 3-2-sig}

set.seed(1234)

simpson_sig_tune <- tune(svm, guilt ~ dem + rep + age + educ + female + black + hispanic + income, data = na.omit(as_tibble(simpson_split$train)),
                    kernel = "sigmoid",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(simpson_sig_tune)

simpson_sig <- simpson_sig_tune$best.model
summary(simpson_sig)

#Best
simpson_sig <- simpson_sig_tune$best.model
summary(simpson_sig)

#ROC
fitted <- predict(simpson_sig, as_tibble(simpson_split$test), decision.values = TRUE) %>%
  attributes

roc_sig <- roc(as_tibble(simpson_split$test)$guilt, fitted$decision.values)
plot(roc_sig)

auc(roc_sig)

#PRE
real <- na.omit(as.numeric(as_tibble(simpson_split$test)$guilt))
E1 <- mean(as.numeric(real != median(real)))
E2 <- 0.191
PRE <- (E1 - E2) / E1
PRE


```  

Using polynomial kernel, with all predictor variables and tested at different cost levels (0.001, 0.01, 0.1, 1, 5, 10, and 100). the model gets the best cost level at 4 with a 19.1% 10-fold CV error rate. Also, the AUC us 0.782 and the PRE is 36.5% (the model MSE is estimated by the 10-fold error rate). Generally, the model is still worse than the linear one.

```{r 3-2-summary}


# plot all the ROC curves:
#plot(roc_line, print.auc = TRUE, col = "blue", print.auc.x = .2, main="ROC for Guilt of OJ Simpson - 4 models ")
#plot(roc_poly, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .4, add = TRUE)
#plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
#plot(roc_sig, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .2, add = TRUE)


# summarize:
sum4<-data_frame("model"=c("model1","model2","model3","model4"),
           "Error Rate"=c(0.1905, 0.1986, 0.1905, 0.1910),
           "PRE" =c(0.366, 0.340, 0.366, 0.365),
           "AUC" =c(0.796, 0.766, 0.771, 0.782)
           )

pander(sum4)
```  

![](graphics/2.png)

According to the error rate, PRE and AUC, I think the model1 (linear kernel) is the best model I got. It has the lowest Error rate, and highest PRE as well as the highest AUC, which indicates this model has the best performance among all the four models. 
