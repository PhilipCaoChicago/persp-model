---
title: "PS 8: tree-based methods and support vector machines"
author: "Esha Banerjee"
date: "5 March 2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(gridExtra)
library(grid)
library(titanic)
#install.packages("pROC")
library(pROC)
library(gbm)
library(caret)
library(ggdendro)
library(e1071)

theme_set(theme_minimal())
```

#### Part 1: Sexy Joe Biden (redux times two)
```{r}
# read in data
biden <- read_csv("biden.csv")
```
# Split the data into a training set (70%) and a validation set (30%). Be sure to set your seed prior to this part of your code to guarantee reproducibility of results.

```{r}
# For reproducibility
set.seed(123) 
# Split data
biden_split <- resample_partition(biden, c(test = .3, train = .7))
```


# Fit a decision tree to the training data, with biden as the response variable and the other variables as predictors. Plot the tree and interpret the results. What is the test MSE? Leave the control options for tree() at their default values

```{r}
set.seed(123)
biden_tree <- tree(biden ~ ., data = biden_split$train)

# plot tree
plot(biden_tree, col='black', lwd=2.5)
title("Decision Tree for Biden Scores", sub = 'All predictors, Default Controls')
text(biden_tree, col='black')

# function to get MSE
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

mse_biden_1 <- mse(biden_tree,biden_split$test)
mse_biden_1

```

We evaluate the model with the testing data and find that the mean squared error is `r mse_biden_1`.

The model shows that being a Democrat is the strongest predictor of feelings of warmth toward Joe Biden, being a Republican is the second-strongest predictor. These splits indicate that party affiliation is the most important factor in predicting an individual's feelings of warmth toward Joe Biden.


# Now fit another tree to the training data with the following control options: tree(control = tree.control(nobs = # number of rows in the training set,mindev = 0)). Use cross-validation to determine the optimal level of tree complexity, plot the optimal tree, and interpret the results. Does pruning the tree improve the test MSE?

```{r}
set.seed(123) # For reproducibility

biden_tree_2 <- tree(biden ~ ., data = biden_split$train,
     control = tree.control(nobs = nrow(biden_split$train),
                            mindev = 0))

rounds = 50

mse_list_biden_50 = vector("numeric", rounds - 1)
leaf_list_biden_50 = vector("numeric", rounds - 1)

for(i in 2:rounds) {
    biden_mod = prune.tree(biden_tree_2, best=i)

    mse_val = mse(biden_mod,biden_split$test)
    mse_list_biden_50[[i-1]] = mse_val
    leaf_list_biden_50[[i-1]] = i
}

mse_df_biden_50 = as.data.frame(mse_list_biden_50)
mse_df_biden_50$branches = leaf_list_biden_50

ggplot(mse_df_biden_50, aes(branches, mse_list_biden_50)) +
       geom_line(color='black',size=1) +
       labs(title = "Comparing Regression Trees for Warmth Toward Joe Biden",
       subtitle = "Using Validation Set",
       x = "Number of nodes",
       y = "Mean Squared Error") + 
       theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1))

mse_test <- mse(biden_tree_2,biden_split$test)
mse_test
```

Using cross validation, we find that the MSE is lowest for a tree with 6 nodes. 

```{r}
biden_pruned6 <- prune.tree(biden_tree_2, best=6)
mse_biden_pruned <- mse(biden_pruned6,biden_split$test)

plot(biden_pruned6, col='black', lwd=2.5)
title("Decision Tree for Biden Scores", sub = 'Only 6 nodes')
text(biden_pruned6, col='black')

mse_biden_pruned

```
Pruning to 6 nodes reduces the MSE from `r mse_biden_1` (which was obtained using all defaults) to `r mse_biden_pruned`.However `r mse_test` which was obtained in this set after splitting and before pruning gave a high mse for the test data implying that it overfitted during training with the 70 % data. The tree indicates that for Democrats, age is the next most important variable. Among Republicans age is important followed by education but educationis a factor only for voters aged 43.5 years and above. Gender, strangely has no effect. 

# Use the bagging approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results.
```{r}
set.seed(123)

biden <- read.csv('biden.csv')
biden$Party[biden$dem == 1] = 'Democrat'
biden$Party[biden$dem == 0 & biden$rep == 0] = 'Independent'
biden$Party[biden$rep == 1] = 'Republican'

biden_split <- resample_partition(biden, c(test = 0.3, train = 0.7))

biden_bag_data_train <- biden_split$train %>%
                       tbl_df()%>%
                       select(-Party) %>%
                       mutate_each(funs(as.factor(.)), dem, rep) %>%
                       na.omit

biden_bag_data_test  <- biden_split$test %>%
  tbl_df()%>%
                      select(-Party) %>%
                      mutate_each(funs(as.factor(.)), dem, rep) %>%
                      na.omit

# estimate model
(bag_biden <- randomForest(biden ~ ., data = biden_bag_data_train, mtry = 5, ntree = 500, importance=TRUE))
# find MSE
mse_bag_biden <- mse(bag_biden, biden_bag_data_test)
mse_bag_biden


```

The MSE for the model with bagging is `r mse_bag_biden` , which is much higher than we had for the pruned tree with `r mse_biden_pruned`. The % variation explained is also very low, at 10.76%.



```{r}
set.seed(123)
bag_biden_importance = as.data.frame(importance(bag_biden))

ggplot(bag_biden_importance, mapping=aes(x=rownames(bag_biden_importance), y=IncNodePurity)) +
       geom_bar(stat="identity", aes(fill=IncNodePurity)) +
       labs(title = "Average Increased Node Purity Across 500 Regression Trees",
       subtitle = "Predicted Warmth Toward Joe Biden",
       x = "Variable",
       y = "Mean Increased Node Purity") + 
       theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1), legend.position = 'none') 

```
The variable importance plot shows that age and Democrat are the two most important variables as these yield the greatest average decrease in node impurity across 500 bagged regression trees. Despite the higher test MSE, the bagged tree model is likely a better model than the pruned tree above because the bagged model uses bootstrapping to create 500 different training sets, whereas the pruned tree above uses only a single training set.It can thus address variances based on the composition of the sets better.  Here too, gender is the least important variable. The bagged model accounts only for 10.76% of the variance in feelings of warmth toward Joe Biden.

#### Use the random forest approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results. Describe the effect of $m$, the number of variables considered at each split, on the error rate obtained.
```{r}
set.seed(123)

(biden_rf <- randomForest(biden ~ ., data = biden_bag_data_train,mtry =2,ntree = 500))
mse_rf_biden <- mse(biden_rf, biden_bag_data_test)
mse_rf_biden

```
The random forest model gives a test MSE of `r mse_rf_biden`, which is much lower than the one returned by bagging `r mse_bag_biden`. Random forest also explains variance (26.83%) in the data compared to the bagged model (10.76%). Still, with the % var explained is low, so that there are probably other unknown variables that effect feelings of warmth for Joe Biden.

The notable decrease in MSE is attributable to the effect of limiting the variables available every split to only randomly-selected predictors. This ensures that the trees in the random forest model are uncorrelated to each other, the variance in the final models is lower, and hence the test MSE is lower.

Plotting the importance of the predictors:


```{r}
rf_biden_importance = as.data.frame(importance(biden_rf))

ggplot(rf_biden_importance, mapping=aes(x=rownames(rf_biden_importance), y=IncNodePurity)) +
       geom_bar(stat="identity", aes(fill=IncNodePurity)) +
       labs(title = "Average Increased Node Purity Across 500 Regression Trees",
       subtitle = "Predicted Warmth Toward Joe Biden",
       x = "Variable",
       y = "Mean Increased Node Purity") + 
       theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1), legend.position = 'none') 


```
The random forest model estimates that Democrat is the sinlge most important predictor of feelings toward Joe Biden and that Republican is next in line. As was the case with the bagging model, gender is the least important predictor.

#### Use the boosting approach to analyze the data. What test MSE do you obtain? How does the value of the shrinkage parameter $\lambda$ influence the test MSE?

We first run the boosting model using depths of 1,2 and 4 respoectively, to find the optimal number of iterations for lowest MSE.
```{r}

set.seed(123)
biden_models <- list("boosting_depth1" = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 10000, interaction.depth = 1),
                       "boosting_depth2" = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 10000, interaction.depth = 2),
                       "boosting_depth4" = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 10000, interaction.depth = 4))
data_frame(depth = c(1, 2, 4),
           model = biden_models[c("boosting_depth1", "boosting_depth2", "boosting_depth4")],
           optimal = map_dbl(model, gbm.perf, plot.it = FALSE)) %>%
  select(-model) %>%
  knitr::kable(caption = "Optimal number of boosting iterations",
               col.names = c("Depth", "Optimal number of iterations"))
biden_boost_1 = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 3302, interaction.depth = 1)

biden_boost_2 = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 2700, interaction.depth = 2)

biden_boost_4 = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 2094, interaction.depth = 4)
predict.gbm <- function (object, newdata, n.trees, type = "link", single.tree = FALSE, ...) {
  if (missing(n.trees)) {
    if (object$train.fraction < 1) {
      n.trees <- gbm.perf(object, method = "test", plot.it = FALSE)
    }
    else if (!is.null(object$cv.error)) {
      n.trees <- gbm.perf(object, method = "cv", plot.it = FALSE)
    }
    else {
      n.trees <- length(object$train.error)
    }
    cat(paste("Using", n.trees, "trees...\n"))
    gbm::predict.gbm(object, newdata, n.trees, type, single.tree, ...)
  }
}
mse_1 = mse(biden_boost_1,biden_bag_data_test)
mse_1
mse_2 = mse(biden_boost_2,biden_bag_data_test)
mse_2
mse_4 = mse(biden_boost_4,biden_bag_data_test)
mse_4
```

The boosting model with a depth of 1 has a test MSE of `r mse_1`; for the model with a depth of 2, it is `r mse_2` and for the model with a depth of 4 it is `r mse_4`. The boosting approach yields the lowest MSE for trees with two splits compared to those with one or four splits. These values are much better than those obtained by bagging and random forest models.

Next, we increase the value of the $\lambda$ from the default of .001 to .1:
```{r}
set.seed(123)

biden_boost_1 = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 3302, interaction.depth = 1,shrinkage=0.02)

biden_boost_2 = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 2700, interaction.depth = 2,shrinkage=0.02)

biden_boost_4 = gbm(as.numeric(biden) - 1 ~ .,
                                               data = biden_bag_data_train,
                                               n.trees = 2094, interaction.depth = 4,shrinkage=0.02)

mse_1 = mse(biden_boost_1,biden_bag_data_test)
mse_1
mse_2 = mse(biden_boost_2,biden_bag_data_test)
mse_2
mse_4 = mse(biden_boost_4,biden_bag_data_test)
mse_4
```
We notice that all the MSE values have increased.Shrinkage is used for reducing, or shrinking, the impact of each additional fitted base-learner (tree). It reduces the size of incremental steps and thus penalizes the importance of each consecutive iteration. So since we increased the step size, the negative impact of an erroneous boosting iteration could not be rectified and we end up with a high MSE.

# Part 2: Modeling voter turnout
#### Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five tree-based models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)

```{r}
mh <- read_csv("mental_health.csv") %>%
  na.omit()%>%
  mutate_each(funs(as.factor(.)), vote96, black, female, married) 

set.seed(123)
mh_split <- resample_partition(mh, p = c("test" = .3, "train" = .7))
```
```{r}
mh_tree <- tree(vote96 ~ educ, data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)
tree_err

roc_tree1 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree1)

auc(roc_tree1)
```


```{r}
mh_tree <- tree(vote96 ~ educ + mhealth_sum, data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)
tree_err

roc_tree2 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree2)

auc(roc_tree2)
```


```{r}
mh_tree <- tree(vote96 ~ educ + mhealth_sum + age, data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)
tree_err

roc_tree3 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree3)

auc(roc_tree3)
```

```{r}
mh_tree <- tree(vote96 ~ educ + mhealth_sum + age + inc10, data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)
tree_err

roc_tree4 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree4)

auc(roc_tree4)
```


```{r}
mh_tree <- tree(vote96 ~ ., data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)
tree_err

roc_tree5 <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree5)

auc(roc_tree5)
```


```{r}
plot(roc_tree1, print.auc = TRUE, col = "blue", print.auc.x = .2)
plot(roc_tree2, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(roc_tree3, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(roc_tree4, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(roc_tree5, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .1, add = TRUE)
```
Chosing a combination of different variables: education for first model, education and mental health score for the second, education, age and mental health score in the third, education, age, income and mental health score in the fourth and all variables in the fifth. The area under the curve is highest at 0.621 for the model containing education, age and mental health score. While the error is slightly higher (0.30) than the last two (~ 0.29), it can be overlooked since the other variables are not better predictors.

Looking at the tree for the third model, age is the most important factor, followed by education and mental health score. We can interpret the tree for model 3 (shown below) using hypothetical observations. 
For younger people, education is the more dominant factor than mental health score when it comes to voting. People younger than 39.5 years are most likely to vote if they have a high education of more then 12.5 years. 
Older people are more likely to have voted, even if they have received only some education.

```{r}
mh_tree <- tree(vote96 ~ educ + mhealth_sum + age, data = as_tibble(mh_split$train))
mh_tree

plot(mh_tree)
text(mh_tree, pretty = 0)
```
#### Use cross-validation techniques and standard measures of model fit to compare and evaluate at least five SVM models of voter turnout. Select the best model and interpret the results using whatever methods you see fit.
```{r}
mh_lin_tune <- tune(svm, vote96 ~ educ + age + mhealth_sum, data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))

mh_lin <- mh_lin_tune$best.model
summary(mh_lin)


fitted <- predict(mh_lin, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes


roc_line <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)

auc(roc_line)
plot(roc_line, main = "ROC of Voter Turnout - Linear Kernel, Partial Model")

```
Area under the curve: 0.7468

```{r}
mh_lin_all <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))

mh_lall <- mh_lin_all$best.model
summary(mh_lall)
fitted <- predict(mh_lall, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes


roc_line_all <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)

auc(roc_line_all)
plot(roc_line_all, main = "ROC of Voter Turnout- Linear Kernel, Total Model")

```
Area under the curve: 0.7502
```{r}
mh_poly_tune <- tune(svm, vote96 ~ age + educ + mhealth_sum, data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))

mh_poly <- mh_poly_tune$best.model
summary(mh_poly)

fitted <- predict(mh_poly, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
auc(roc_poly)
plot(roc_poly, main = "ROC of Voter Turnout - Polynomial Kernel, Partial Model")

```

Area under the curve: 0.7411

```{r}
mh_poly_all <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))

mh_poly <- mh_poly_all$best.model
summary(mh_poly)
fitted <- predict(mh_poly, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly_all <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
auc(roc_poly_all)
plot(roc_poly_all, main = "ROC of Voter Turnout - Polynomial Kernel, Total Model")
```

Area under the curve: 0.7395

```{r}
mh_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))

mh_rad <- mh_rad_tune$best.model
summary(mh_rad)


fitted <- predict(mh_rad, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
auc(roc_rad)
plot(roc_rad, main= "ROC of Voter Turnout - Radial Kernel, Total Model")

```
Area under the curve: 0.7466

```{r}
plot(roc_line, print.auc = TRUE, col = "blue", print.auc.x = .2)
plot(roc_line_all, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(roc_poly, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(roc_poly_all, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .1, add = TRUE)
```

Looking at the area under the curve, the best model is the linear one with all the variables. Its error however is high, This model has a cost of 1, so the margins are narrow around the linear hyperplane. As we can see from the plot below, the error hovers around 0.32
```{r}
plot(mh_lin_all)
```
# Part 3: OJ Simpson

#### What is the relationship between race and belief of OJ Simpson's guilt? Develop a robust statistical learning model and use this model to explain the impact of an individual's race on their beliefs about OJ Simpson's guilt.
```{r}
oj = read.csv('simpson.csv')
oj <- oj %>%
  na.omit()
```

Starting off with linear regression just to get a basic idea, it is seen that the variable black is highly significant in predicting the belief in Simpson's guilt.Dealing with the non-binary variables: age, educationand income, age can be significant but its not substantive, education and income seem to be significant which is intuitive since education and income are often influenced by race.
```{r}
# age, educ, income left
oj_binaries <- lm(guilt ~ dem + rep + black + hispanic + female, data = oj)
oj_age <- lm(guilt ~ age, data = oj)
oj_educ <- lm (guilt ~ educ, data = oj )
oj_income <- lm (guilt ~ income, data = oj )
summary (oj_age)
summary (oj_income)
summary (oj_educ)
summary (oj_binaries)
tidy(oj_binaries)
oj_binaries1 <- lm(guilt ~ black + hispanic, data = oj)
summary (oj_binaries1)
ggplot(oj, aes(black, guilt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "black",
       y = "Guilt")

blah <- lm(guilt ~ age + income + educ + female, data = oj)
```
Plotting moddels considering the variables, we find it is safe to not consider the factors other than black and hispanic for estimation purposes.
```{r}
oj %>%
  add_predictions(blah) %>%
  add_residuals(blah) %>%
  {.} -> grid
gridblack <- filter(grid, black == 1)
gridhispanic <- filter(grid, hispanic == 1)
gridother <- filter(grid, black == 0 & hispanic == 0)
ggplot(grid, aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method ="lm", aes(y = resid , color = 'Black'), data = gridblack, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = 'Hispanic'), data = gridhispanic, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = 'Other'), data = gridother, size = 1) +
  scale_colour_manual("", values = c("Black"="blue","Hispanic"="red", "Other"="green")) +
  labs(title = "Predicted Value and Residuals of model with age,income, gender, education",
        x = "Predicted Guilt",
        y = "Residuals") +
  theme_minimal()

blah1 = lm(guilt ~ age + income + educ + female + black + hispanic, data = oj)
blah2 = lm(guilt ~ age + income + educ + female + dem + rep, data = oj)
blah3 = lm(guilt ~ age + income + educ + dem + rep + black + hispanic, data = oj)
oj %>%
  add_predictions(blah1) %>%
  add_residuals(blah1) %>%
  {.} -> grid
griddem <- filter(grid, dem == 1)
gridrep <- filter(grid, rep == 1)
gridother <- filter(grid, dem == 0 & rep == 0)
ggplot(grid, aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method ="lm", aes(y = resid , color = 'Dem'), data = griddem, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = 'Rep'), data = gridrep, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = 'Other'), data = gridother, size = 1) +
  scale_colour_manual("", values = c("Dem"="blue","Rep"="red", "Other"="green")) +
  labs(title = "Predicted Value and Residuals of model with age,income, gender, education, race",
        x = "Predicted Guilt",
        y = "Residuals") +
  theme_minimal()


oj %>%
  add_predictions(blah2) %>%
  add_residuals(blah2) %>%
  {.} -> grid
gridblack <- filter(grid, black == 1)
gridhispanic <- filter(grid, hispanic == 1)
gridother <- filter(grid, black == 0 & hispanic == 0)
ggplot(grid, aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method ="lm", aes(y = resid , color = 'Black'), data = gridblack, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = 'Hispanic'), data = gridhispanic, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = 'Other'), data = gridother, size = 1) +
  scale_colour_manual("", values = c("Black"="blue","Hispanic"="red", "Other"="green")) +
  labs(title = "Predicted Value and Residuals of model with age,income, gender, education, party",
        x = "Predicted Guilt",
        y = "Residuals") +
  theme_minimal()

oj %>%
  add_predictions(blah3) %>%
  add_residuals(blah3) %>%
  {.} -> grid
gridfemale <- filter(grid, female == 1)
gridmale <- filter(grid, female == 0)
ggplot(grid, aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method ="lm", aes(y = resid , color = 'F'), data = gridfemale, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = 'M'), data = gridmale, size = 1) +
  #geom_smooth(method ="lm", aes(y = resid, color = 'Other'), data = gridother, size = 1) +
  scale_colour_manual("", values = c("F"="blue","M"="red")) +
  labs(title = "Predicted Value and Residuals of model with age,income, party, education, race",
        x = "Predicted Guilt",
        y = "Residuals") +
  theme_minimal()
```
```{r}
blah4 = lm(guilt ~ educ + black + hispanic, data = oj)
oj %>%
  add_predictions(blah4) %>%
  add_residuals(blah4) %>%
  {.} -> grid
grid1 <- filter(grid, income == "UNDER $15,000")
grid2 <- filter(grid, income == "$15,000-$30,000")
grid3 <- filter(grid, income == "$30,000-$50,000")
grid4 <- filter(grid, income == "$50,000-$75,000")
grid5 <- filter(grid, income == "OVER $75,000")
ggplot(grid, aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method ="lm", aes(y = resid , color = "UNDER $15,000"), data = grid1, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = "$15,000-$30,000"), data = grid2, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = "$30,000-$50,000"), data = grid3, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = "$50,000-$75,000"), data = grid4, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = "OVER $75,000"), data = grid5, size = 1) +
  scale_colour_manual("", values = c("UNDER $15,000"="blue","$15,000-$30,000"="red", "$30,000-$50,000"="green", "$50,000-$75,000"="pink", "OVER $75,000" = "yellow" )) +
  labs(title = "Predicted Value and Residuals of model with education, race",
        x = "Predicted Guilt",
        y = "Residuals") +
  theme_minimal()

blah5 = lm(guilt ~ income + black + hispanic, data = oj)
oj %>%
  add_predictions(blah5) %>%
  add_residuals(blah5) %>%
  {.} -> grid
grid1 <- filter(grid, educ == "NOT A HIGH SCHOOL GRAD")
grid2 <- filter(grid, educ == "HIGH SCHOOL GRAD")
grid3 <- filter(grid, educ == "SOME COLLEGE(TRADE OR BUSINESS)")
grid4 <- filter(grid, educ == "COLLEGE GRAD AND BEYOND")
#grid5 <- filter(grid, income == "OVER $75,000")
ggplot(grid, aes(x = pred, y = resid)) +
  geom_point() +
  geom_smooth(method ="lm", aes(y = resid , color = "NOT A HIGH SCHOOL GRAD"), data = grid1, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = "HIGH SCHOOL GRAD"), data = grid2, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = "SOME COLLEGE(TRADE OR BUSINESS)"), data = grid3, size = 1) +
  geom_smooth(method ="lm", aes(y = resid, color = "COLLEGE GRAD AND BEYOND"), data = grid4, size = 1) +
  #geom_smooth(method ="lm", aes(y = resid, color = "OVER $75,000"), data = grid5, size = 1) +
  scale_colour_manual("", values = c("NOT A HIGH SCHOOL GRAD"="blue","HIGH SCHOOL GRAD"="red", "SOME COLLEGE(TRADE OR BUSINESS)"="green", "COLLEGE GRAD AND BEYOND"="pink")) +
  labs(title = "Predicted Value and Residuals of model with income, race",
        x = "Predicted Guilt",
        y = "Residuals") +
  theme_minimal()
```
Concentrating on race:
```{r}

oj$Opinion <- factor(oj$guilt, levels = c(0,1), labels = c("Probably not guilty", "Probably guilty"))
 
ggplot(oj, aes(x=black, fill=Opinion)) + geom_bar(position = "dodge") + 
       ylab("Frequency count of respondents") +
       xlab("Race") +
       ggtitle("Opinion of Simpson Guilt Based on Race") +
       theme(plot.title = element_text(hjust = 0.5),
       panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank(),
       panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1)) +
       scale_x_continuous(breaks = c(0,1), labels = c("Not Black", "Black"))

```
High proportion of non-black people believe OJ Simpson to be guilty. 
Considering a logistic regression, since we are using dichotomous variables to predict a dichotomous outcome (belief or no-belief in guilt), it seems to be the most logical choice.
```{r}
logit_guilty1 <- glm(guilt ~ black, data = oj, family = binomial)
logit_guilty2 <- glm(guilt ~ hispanic, data = oj, family = binomial)
logit_guilty3 <- glm(guilt ~ black + hispanic, data = oj, family = binomial)
summary(logit_guilty1)
summary(logit_guilty2)
summary(logit_guilty3)


logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}

prob2odds <- function(x){
  x / (1 - x)
}

prob2logodds <- function(x){
  log(prob2odds(x))
}

guilt_pred <- oj %>%
  add_predictions(logit_guilty1) %>%
  mutate(prob = logit2prob(pred)) %>%
  mutate(odds = prob2odds(prob)) %>%
  mutate(logodds = prob2logodds(prob))



ggplot(guilt_pred, aes(black)) +
  geom_point(aes(y = guilt)) +
  geom_line(aes(y = prob), color = "blue", size = 1) +
  labs(x = "Black",
       y = "Probability of guilt")

#Logistic regression & chi-squared, phi-coeff


accuracy1 <- oj %>%
  add_predictions(logit_guilty1) %>%
  mutate(pred = logit2prob(pred),
         prob = pred,
         pred = as.numeric(pred > .5))
accuracy2 <- oj %>%
  add_predictions(logit_guilty2) %>%
  mutate(pred = logit2prob(pred),
         prob = pred,
         pred = as.numeric(pred > .5))
accuracy3 <- oj %>%
  add_predictions(logit_guilty3) %>%
  mutate(pred = logit2prob(pred),
         prob = pred,
         pred = as.numeric(pred > .5))

accuracy_rate1 <- 100*mean(accuracy1$guilt == accuracy1$pred, na.rm = TRUE)
accuracy_rate2 <- 100*mean(accuracy2$guilt == accuracy2$pred, na.rm = TRUE)
accuracy_rate3 <- 100*mean(accuracy3$guilt == accuracy3$pred, na.rm = TRUE)
accuracy_rate1
accuracy_rate2
accuracy_rate3



# function to calculate PRE for a logistic regression model
PRE <- function(model){
  # get the actual values for y from the data
  y <- model$y

  # get the predicted values for y from the model
  y.hat <- round(model$fitted.values)

  # calculate the errors for the null model and your model
  E1 <- sum(y != median(y))
  E2 <- sum(y != y.hat)

  # calculate the proportional reduction in error
  PRE <- 100*(E1 - E2) / E1
  return(PRE)
}

PRE(logit_guilty1)
PRE(logit_guilty2)
PRE(logit_guilty3)
auc1 <- auc(accuracy1$guilt, accuracy1$prob)
auc1
auc2 <- auc(accuracy2$guilt, accuracy2$prob)
auc2
auc3 <- auc(accuracy3$guilt, accuracy3$prob)
auc3
```

The model with both black and hispanic as factors have the highest area under the curve but it is only marginally more than the one with only black in it. Also, there is no significant error reduction from black only to black + hispanic model. Beingblack reduces the log-odds of an individuals belief in OJ's guilt by -3.1022, i.e.,it lowers   likelihood of believing in OJ's guilt.

Race is a highly dominant factor, so using a decision tree makes sense. It is also easy to interpret.
```{r}
set.seed(123) # For reproducibility
oj = read.csv('simpson.csv')
oj = oj[(!is.na(oj$guilt)), ]
oj$Opinion = factor(oj$guilt, levels = c(0,1), labels = c("Innocent", "Guilty"))
oj_split7030 = resample_partition(oj, c(test = 0.3, train = 0.7))
oj_train70 = oj_split7030$train %>%
                tbl_df()
oj_test30 = oj_split7030$test %>%
               tbl_df()

oj_data_train = oj_train70 %>%
                select(-guilt) %>%
                mutate_each(funs(as.factor(.)), dem, rep) %>%
                na.omit

oj_data_test = oj_test30 %>%
               select(-guilt) %>%
               mutate_each(funs(as.factor(.)), dem, rep) %>%
               na.omit

# estimate model
oj_tree <- tree(Opinion ~ ., data = oj_data_train)

# plot tree
tree_data <- dendro_data(oj_tree)

ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()+
  labs(title = "Decision Tree for OJ's Guilt",
       subtitle = 'All predictors, Default Controls')
ptree

```
The decision tree clearly shows that being black is the single most important predictor of belief in Simpson's guilt.
Using the Random forest approach with all variables,
```{r}
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

(rf_oj = randomForest(Opinion ~ ., data = oj_data_train, mtry = 3,ntree = 2000))

```
The error rate is only 19.1%which is pretty good. This is comparable to the values from using logistic regression too (Logistic regression makes most sense in this classification problem). Validating our belief in race to be a guiding factor:
```{r}
rf_oj_importance = as.data.frame(importance(rf_oj))

ggplot(rf_oj_importance, mapping=aes(x=rownames(rf_oj_importance), y=MeanDecreaseGini)) +
       geom_bar(stat="identity", aes(fill=MeanDecreaseGini)) + 
       labs(title = "Mean Decrease in Gini Index Across 2000 Random Forest Regression Trees",
       subtitle = "Predicted Opinion of Simpson Guilt",
       x = "Variable",
       y = "Mean Decrease in Gini Index") + 
       theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5),
             panel.border = element_rect(linetype = "solid", color = "grey70", fill=NA, size=1.1), legend.position = 'none') 


```
it yields the highest decrease in Gini Index. 

Using linear SVM with all variables
```{r}
set.seed(123)
simpson_lin_tune <- tune(svm, Opinion ~ ., data = na.omit(as_tibble(oj_data_train)),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(simpson_lin_tune)

simpson_lin <- simpson_lin_tune$best.model
summary(simpson_lin)

#Best
simpson_lin <- simpson_lin_tune$best.model
summary(simpson_lin)

#ROC
fitted <- predict(simpson_lin, as_tibble(oj_data_test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(oj_data_test)$Opinion, fitted$decision.values)
plot(roc_line)

auc(roc_line)

```

Area under the curve: 0.7841. So, it doesnt better our accuracy from Random Forest.
Cross validation using logistic regression with black being the only variable and random forest with all variables both seem to be good approaches, but I would choose random forest as the better one as it will lead to better results for different seeds. We know from previous work that the cross-validation (70:30) split becomes highly dictated by set composition.
