---
title: 'Perspectives on Computational Modeling: Problem Set #8'
author: "Xingyun Wu"
date: "2017/3/5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(titanic)
library(rcfss)
library(pROC)
library(gbm)
library(ggdendro)
library(dplyr)
library(e1071)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

```{r read-in data, include=FALSE}
bidenData <- read.csv("biden.csv")
mhlthData <- na.omit(read.csv("mental_health.csv"))
simpsonData <- read.csv("simpson.csv")
```

## Part 1: Sexy Joe Biden

1. The `biden` data is splitted as required.
```{r problem1_1, echo=FALSE}
set.seed(1234)
biden_split <- resample_partition(bidenData, c(test = 0.3, train = 0.7))
```

2. 
```{r problem1_segment, echo = FALSE}
# hackish function to get line segment coordinates for ggplot
partition.tree.data <- function (tree, label = "yval", add = FALSE, ordvars, ...) 
{
  ptXlines <- function(x, v, xrange, xcoord = NULL, ycoord = NULL, 
                       tvar, i = 1L) {
    if (v[i] == "<leaf>") {
      y1 <- (xrange[1L] + xrange[3L])/2
      y2 <- (xrange[2L] + xrange[4L])/2
      return(list(xcoord = xcoord, ycoord = c(ycoord, y1, 
                                              y2), i = i))
    }
    if (v[i] == tvar[1L]) {
      xcoord <- c(xcoord, x[i], xrange[2L], x[i], xrange[4L])
      xr <- xrange
      xr[3L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[1L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else if (v[i] == tvar[2L]) {
      xcoord <- c(xcoord, xrange[1L], x[i], xrange[3L], 
                  x[i])
      xr <- xrange
      xr[4L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[2L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else stop("wrong variable numbers in tree.")
  }
  if (inherits(tree, "singlenode")) 
    stop("cannot plot singlenode tree")
  if (!inherits(tree, "tree")) 
    stop("not legitimate tree")
  frame <- tree$frame
  leaves <- frame$var == "<leaf>"
  var <- unique(as.character(frame$var[!leaves]))
  if (length(var) > 2L || length(var) < 1L) 
    stop("tree can only have one or two predictors")
  nlevels <- sapply(attr(tree, "xlevels"), length)
  if (any(nlevels[var] > 0L)) 
    stop("tree can only have continuous predictors")
  x <- rep(NA, length(leaves))
  x[!leaves] <- as.double(substring(frame$splits[!leaves, "cutleft"], 
                                    2L, 100L))
  m <- model.frame(tree)
  if (length(var) == 1L) {
    x <- sort(c(range(m[[var]]), x[!leaves]))
    if (is.null(attr(tree, "ylevels"))) 
      y <- frame$yval[leaves]
    else y <- frame$yprob[, 1L]
    y <- c(y, y[length(y)])
    if (add) {
      # lines(x, y, type = "s", ...)
    }
    else {
      a <- attributes(attr(m, "terms"))
      yvar <- as.character(a$variables[1 + a$response])
      xo <- m[[yvar]]
      if (is.factor(xo)) 
        ylim <- c(0, 1)
      else ylim <- range(xo)
      # plot(x, y, ylab = yvar, xlab = var, type = "s", ylim = ylim,
      #      xaxs = "i", ...)
    }
    data_frame(x = x, y = y)
  }
  else {
    if (!missing(ordvars)) {
      ind <- match(var, ordvars)
      if (any(is.na(ind))) 
        stop("unmatched names in vars")
      var <- ordvars[sort(ind)]
    }
    lab <- frame$yval[leaves]
    if (is.null(frame$yprob)) 
      lab <- format(signif(lab, 3L))
    else if (match(label, attr(tree, "ylevels"), nomatch = 0L)) 
      lab <- format(signif(frame$yprob[leaves, label], 
                           3L))
    rx <- range(m[[var[1L]]])
    rx <- rx + c(-0.025, 0.025) * diff(rx)
    rz <- range(m[[var[2L]]])
    rz <- rz + c(-0.025, 0.025) * diff(rz)
    xrange <- c(rx, rz)[c(1, 3, 2, 4)]
    xcoord <- NULL
    ycoord <- NULL
    xy <- ptXlines(x, frame$var, xrange, xcoord, ycoord, 
                   var)
    xx <- matrix(xy$xcoord, nrow = 4L)
    yy <- matrix(xy$ycoord, nrow = 2L)

    return(list(data_frame(xmin = xx[1L,],
                           ymin = xx[2L,],
                           xmax = xx[3L,],
                           ymax = xx[4L,]),
                data_frame(x = yy[1L,],
                           y = yy[2L,],
                           label = lab)))
    if (!add) 
      plot(rx, rz, xlab = var[1L], ylab = var[2L], type = "n", 
          xaxs = "i", yaxs = "i", ...)
    segments(xx[1L, ], xx[2L, ], xx[3L, ], xx[4L, ])
    text(yy[1L, ], yy[2L, ], as.character(lab), ...)
  }
}
```

```{r problem1_2_model, echo = FALSE}
biden_train <- biden_split$train %>%
  as_tibble() %>%
  mutate(female = factor(female, levels = 0:1, labels = c("Male", "Female")), 
         dem = factor(dem, levels = 0:1, labels = c("Non-Democrat", "Democrat")),
         rep = factor(rep, levels = 0:1, labels = c("Non-Republican", "Republican")))
```

```{r problem1_2, echo=FALSE}
biden_tree1_2 <- tree(biden ~ female + age + dem + rep + educ, data = biden_split$train)
summary(biden_tree1_2)

mod1_2 <- prune.tree(biden_tree1_2, best = 3)
summary(mod1_2)
```

```{r problem1_2_plot, echo = FALSE}
# plot tree
tree_data <- dendro_data(mod1_2)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()


# plot region space
preg <- ggplot(biden_split$train$data, aes(dem, rep)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod1_2)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod1_2)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(biden_split$train$data$dem), max(biden_split$train$data$rep)),
                  ylim = c(min(biden_split$train$data$rep), max(biden_split$train$data$rep)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod1_2$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

2. `biden` is the response variable, and all the other variables are predictors. Contents below show the results and plot of the tree.
  Only two predictors are actually used in tree construction: `dem` and `rep`. 
```{r problem1_2_test, echo=FALSE}
biden_train <- biden_split$train$data %>%
  as_tibble() %>%
  mutate(female = factor(female, levels = 0:1, labels = c("Male", "Female")), 
         dem = factor(dem, levels = 0:1, labels = c("Non-Democrat", "Democrat")),
         rep = factor(rep, levels = 0:1, labels = c("Non-Republican", "Republican")))

# estimate model
biden_tree <- tree(biden ~ female + age + dem + rep + educ, data = biden_train)

# plot unpruned tree
mod <- biden_tree
summary(mod)

tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Biden theremometer tree",
       subtitle = "female + age + dem + rep + educ")
```
  The test MSE of this model is 401.
```{r problem1_2_test_mse, echo=FALSE}
# MSE function
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

# convert the qualitative variables in testing data
biden_test <- biden_split$test$data %>%
  as_tibble() %>%
  mutate(female = factor(female, levels = 0:1, labels = c("Male", "Female")), 
         dem = factor(dem, levels = 0:1, labels = c("Non-Democrat", "Democrat")),
         rep = factor(rep, levels = 0:1, labels = c("Non-Republican", "Republican")))

mse(mod, biden_test)
```


3. According to the cross-validation approach with LOOCV, the optimal level of tree complexity is 4. Although adding a second-order polynomial would increase the MSE, compared with the MSE of the model only having the linear term, we could get lower MSE (also compared to the MSE with only the linear term) after adding the fourth-order polynomial. However, the MSE would greatly increase after adding the fifth-order polynomial. So the optimal level of tree complexity is 4.
```{r problem1_3_complexity, echo = FALSE}
# use cross-validation to deter mine the optimal level of tree complexity
loocv_data <- crossv_kfold(bidenData, k = nrow(bidenData))

cv_error <- vector("numeric", 5)
terms <- 1:5

for(i in terms){
  loocv_models <- map(loocv_data$train, ~ lm(biden ~ poly(female+age+dem+rep+educ, i), data = .))
  loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)
  cv_error[[i]] <- mean(loocv_mse)
}

cv_mse <- data_frame(terms = terms,
           cv_MSE = cv_error)
cv_mse

# plot
ggplot(cv_mse, aes(terms, cv_MSE)) +
  geom_line() +
  labs(title = "Comparing quadratic linear models",
       subtitle = "Using LOOCV",
       x = "Highest-order polynomial",
       y = "Mean Squared Error")
```

  According to the table and plot, among the predictors put into the model, only `dem`, `rep` and `age` are actually used by the tree. 
```{r problem1_3_model, echo = FALSE}
# estimate model
biden_tree1_3 <- tree(biden ~ female+age+dem+rep+educ, data = biden_train,
     control = tree.control(nobs = nrow(biden_train),
                            mindev = 0))

mod1_3 <- prune.tree(biden_tree1_3, best = 4)
#mod1_3 <- biden_tree1_3
summary(mod1_3)

tree_data2 <- dendro_data(mod1_3)
ggplot(segment(tree_data2)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data2), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data2), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Biden theremometer tree",
       subtitle = "female + age + dem + rep + educ")
```

  The test MSE of this model is 398, which is lower than the test MSE of the previous model. So pruning the tree slightly improves the test MSE.
```{r problem1_3_testMSE, echo=FALSE}
mse(mod1_3, biden_test)
```


4. 
```{r problem1_4_model, echo = FALSE}
biden_rf_data <- biden_split$train$data %>%
    select(-age, -educ) %>%
    mutate_each(funs(as.factor(.)), female, dem, rep) %>%
    na.omit

(biden_bag <- randomForest(biden ~ ., data = biden_rf_data,
                             mtry = 5, ntree = 500))
```
```{r problem1_4_importance, echo = FALSE}
importance(biden_bag)
```
  And the test MSE of this model is 396.
```{r problem1_4_testMSE, echo=FALSE}
# preparing the testing data
biden_rf_data_test <- biden_split$test$data %>%
    select(-age, -educ) %>%
    mutate_each(funs(as.factor(.)), female, dem, rep) %>%
    na.omit

mse(biden_bag, biden_rf_data_test)
```

5. The test MSE is 407, which is larger than the tree generated with the bagging approach.
```{r problem1_5_model, echo=FALSE}
(biden_rf <- randomForest(biden ~ ., data = biden_rf_data,
                            ntree = 500))
summary(biden_rf)
```
```{r problem1_5_testMSE, echo = FALSE}
mse(biden_rf, biden_rf_data_test)
```

```{r problem1_5_variable_used, echo = FALSE}
seq.int(biden_rf$ntree) %>%
  map_df(~ getTree(biden_rf, k = ., labelVar = TRUE)[1,]) %>%
  count(`split var`) %>%
  knitr::kable(caption = "Variable used to generate the first split in each tree",
               col.names = c("Variable used to split", "Number of training observations"))
```
```{r problem1_5_error_rate, echo = FALSE}
err.rate.rf <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  
  pred <- predict(model, newdata = data, type = "response")
  actual <- data[[response]]
  
  return(mean(pred != actual, na.rm = TRUE))
}
err.rate.rf(biden_rf, biden_rf_data)
```

6. 
```{r problem1_6, echo = FALSE}
set.seed (1234)
biden_boost=gbm(biden~.,data=biden_split$train, distribution="gaussian", n.trees=5000, interaction.depth=4)
summary(biden_boost)
```

```{r problem1_6_testMSE, echo=FALSE}
#mse(biden_boost, biden_split$test)
```


## Part 2: Modeling voter turnout

Models description:
  In this part, the response variable would be `vote96`, and the predictors would be all the other variables: `mhealth_sum`, `age`, `educ`, `black`, `female`, `married` and `inc10`.

### Preparation: split the data into a training set (70%) and a validation set (30%).
```{r problem2_1_split, echo=FALSE}
set.seed(1234)
mhlth_split <- resample_partition(mhlthData, c(test = 0.3, train = 0.7))
```

### 1. Five tree-based models of voter turnout

#### 1.1 classification tree
  Using a 10-fold CV with ramdem seed = 1234, I decide that the optimal tree size is 7. When the complexity reaches 5, the test MSE obtains its lowest point among the attempts. In addition, since simply increasing the complexity could cause over-fitting problem, I would not increase the tree size to numbers that would not lower the test MSE any more.
```{r problem2_1_complexity, echo=FALSE}
set.seed(1234)
# generate 10-fold CV trees
mhlth_cv <- crossv_kfold(mhlthData, k = 10) %>%
  mutate(tree = map(train, ~ tree(vote96 ~ mhealth_sum + age + educ + black + female + married + inc10, data = .,
     control = tree.control(nobs = nrow(mhlthData),
                            mindev = 0))))

# calculate each possible prune result for each fold
mhlth_cv <- expand.grid(mhlth_cv$.id, 2:10) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(mhlth_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         mse = map2_dbl(prune, test, mse))

mhlth_cv %>%
  select(k, mse) %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of terminal nodes",
       y = "Test MSE")
```


```{r problem2_1_model, echo=FALSE}
mhlth <- mhlth_split$train %>%
  as_tibble() %>%
  mutate(vote96 = factor(vote96, levels = 0:1, labels = c("not-voted", "voted")),
         black = factor(black, levels = 0:1, labels = c("non-black", "black")),
         female = factor(female, levels = 0:1, labels = c("male", "female")),
         married = factor(married, levels = 0:1, labels = c("not-married", "married")))

# estimate model
mhlth_tree <- tree(vote96 ~ mhealth_sum + age + educ + black + female + married + inc10, data = mhlth,
                     control = tree.control(nobs = nrow(mhlth),
                            mindev = .001))

# plot unpruned tree
mod2_1 <- prune.tree(mhlth_tree, best=5)
summary(mod2_1)

tree_data2_1 <- dendro_data(mod2_1)
mhlth_tree_data <- dendro_data(mod2_1)
ggplot(segment(mhlth_tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data2_1), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data2_1), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter Turnout",
       subtitle = "mental health + educ")
```
  According to the computation below, the test MSE of this model is 0.295.
```{r problem2_1_fit, echo=FALSE}

```

#### 1.2: Bagging approach

```{r problem2_2_model, echo=FALSE}
mhlth_rf_data <- mhlth_split$train$data %>%
    select(-mhealth_sum, -age, -educ, -inc10) %>%
    mutate_each(funs(as.factor(.)), black, female, married) %>%
    na.omit

(mhlth_bag <- randomForest(as.factor(vote96) ~ ., data = mhlth_rf_data,
                             mtry = 3, ntree = 500))
```

#### 1.3 Random forest approach
```{r problem2_3_model, echo=FALSE}
(mhlth_rf <- randomForest(as.factor(vote96) ~ ., data = mhlth_rf_data,
                            ntree = 500))
```

#### 1.4 Boosting: depth = 1
```{r problem2_4_model, echo=FALSE}
boosting_depth1 = gbm(as.numeric(vote96) ~ ., 
                      data = mhlth_split$train, 
                      n.trees = 10000, 
                      interaction.depth = 1)
```


#### 1.5 Boosting: depth = 3
```{r problem2_5_model, echo=FALSE}
boosting_depth2 = gbm(as.numeric(vote96) ~ ., 
                      data = mhlth_split$train, 
                      n.trees = 10000, 
                      interaction.depth = 3)
```

#### 1.6 model comparison
```{r problem2_comparison1_set_models, echo=FALSE}
mhlth_split <- resample_partition(mhlth_rf_data, p = c("test" = .3,
                                                           "train" = .7))

mhlth_models <- list("bagging" = randomForest(vote96 ~ ., data = mhlth_split$train,
                                                mtry = 7, ntree = 10000),
                       "boosting_depth1" = gbm(as.numeric(vote96)  ~ .,
                                               data = mhlth_split$train,
                                               n.trees = 10000, interaction.depth = 1),
                       "boosting_depth2" = gbm(as.numeric(vote96)  ~ .,
                                               data = mhlth_split$train,
                                               n.trees = 10000, interaction.depth = 3),
                       "boosting_depth4" = gbm(as.numeric(vote96)  ~ .,
                                               data = mhlth_split$train,
                                               n.trees = 10000, interaction.depth = 5))
```


```{r problem2_comparison1, echo=FALSE}
boost_test_err <- data_frame(bagging = predict(mhlth_models$bagging,
                                               newdata = as_tibble(mhlth_split$test$data),
                                               predict.all = TRUE)[[2]] %>%
                               apply(2, function(x) x != as_tibble(mhlth_split$test$data)$vote96) %>%
                               apply(2, mean),
                             
                             boosting_depth1 = predict(mhlth_models$boosting_depth1,
                                                       newdata = as_tibble(mhlth_split$test$data),
                                                       n.trees = 1:10000) %>%
                               apply(2, function(x) round(x) == as.numeric(as_tibble(mhlth_split$test$data)$vote96)) %>%
                               apply(2, mean),
                             
                             boosting_depth3 = predict(mhlth_models$boosting_depth3,
                                                       newdata = as_tibble(mhlth_split$test$data),
                                                       n.trees = 1:10000) %>%
                               apply(2, function(x) round(x) == as.numeric(as_tibble(mhlth_split$test$data)$vote96)) %>%
                               apply(2, mean),
                             
                             boosting_depth5 = predict(mhlth_models$boosting_depth5,
                                                       newdata = as_tibble(mhlth_split$test$data),
                                                       n.trees = 1:10000) %>%
                               apply(2, function(x) round(x) == as.numeric(as_tibble(mhlth_split$test$data)$vote96)) %>%
                               apply(2, mean))

boost_test_err %>%
  mutate(id = row_number()) %>%
  mutate_each(funs(cummean(.)), bagging:rf_mtry4) %>%
  gather(model, err, -id) %>%
  mutate(model = factor(model, levels = names(mhlth_models),
                        labels = c("Bagging",
                                   "Boosting: depth = 1",
                                   "Boosting: depth = 3",
                                   "Boosting: depth = 5"))) %>%
  ggplot(aes(id, err, color = model)) +
  geom_line() +
  scale_color_brewer(type = "qual", palette = "Dark2") +
  labs(x = "Number of trees",
       y = "Test classification error",
       color = "Model")
```


### 2. Five SVM models

#### Preparation
  Read-in the voter turnout data and split it into a training set (70%) and a validation set(30%). We would use 10-fold CV on the training set to determine the optimal cost parameter.
```{r preparation2, echo=FALSE, include=FALSE}
(mh <- read_csv("mental_health.csv") %>%
  mutate_each(funs(as.factor(.)), vote96, black, female, married) %>%
  na.omit)
mh_split <- resample_partition(mh, p = c("test" = .3, "train" = .7))
```

#### 2.1 Linear kernel

```{r problem2_6_model, echo=FALSE}
mh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
#summary(mh_lin_tune)
mh_lin <- mh_lin_tune$best.model
#summary(mh_lin)
fitted <- predict(mh_lin, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_line)
```

```{r problem2_6_evaluation, echo=FALSE}
auc(roc_line)
```

#### 2.2 Polynomial kernel
```{r problem2_7, echo=FALSE}
mh_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
#summary(mh_poly_tune)
mh_poly <- mh_poly_tune$best.model
#summary(mh_poly)

fitted <- predict(mh_poly, as_tibble(mh_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(mh_split$test)$vote96, fitted$decision.values)
plot(roc_poly)
```

```{r problem2_7_evaluation, echo=FALSE}
auc(roc_poly)
```

#### 2.3 Logistic regression
```{r problem2_8_model, echo=FALSE}
mh_logit <- glm(vote96 ~ ., data = as_tibble(mh_split$train), family = binomial)
#summary(mh_logit)
fitted <- predict(mh_logit, as_tibble(mh_split$test), type = "response")
logit_err <- mean(as_tibble(mh_split$test)$vote96 != round(fitted))

roc_logit <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_logit)
```

```{r problem2_8_evaluation, echo=FALSE}
auc(roc_logit)
```

#### 2.4 Desicion tree
```{r problem2_9_model, echo=FALSE}
mh_tree <- tree(vote96 ~ ., data = as_tibble(mh_split$train))

fitted <- predict(mh_tree, as_tibble(mh_split$test), type = "class")
tree_err <- mean(as_tibble(mh_split$test)$vote96 != fitted)

roc_tree <- roc(as.numeric(as_tibble(mh_split$test)$vote96), as.numeric(fitted))
plot(roc_tree)
```

```{r problem2_9_evaluation, echo=FALSE}
auc(roc_tree)
```

#### 2.5 Random forest
```{r problem2_10_model, echo=FALSE}
mh_rf <- randomForest(vote96 ~ ., data = as_tibble(mh_split$train))

fitted <- predict(mh_rf, as_tibble(mh_split$test), type = "prob")[,2]

roc_rf <- roc(as_tibble(mh_split$test)$vote96, fitted)
plot(roc_rf)
```

```{r problem2_10_evaluation, echo=FALSE}
auc(roc_rf)
```

#### Comparison
```{r problem2_comparison2, echo=FALSE}
plot(roc_line, print.auc = TRUE, col = "blue", print.auc.x = .2)
plot(roc_poly, print.auc = TRUE, col = "blue", print.auc.x = .2)
plot(roc_logit, print.auc = TRUE, col = "red", print.auc.x = .2, print.auc.y = .4, add = TRUE)
plot(roc_tree, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(roc_rf, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .1, add = TRUE)
```


## Part 3: OJ Simpson

### 1. The relationship between race and belief of OJ Simpson's guilt
  Since this question already has required resopnse variable `guilt` and predictor `black`, I would use logistic regression with resampling methods for this part.

#### 1.1 logistic regression with the whole data


#### 1.2 logistic regression with 10-fold cross-validation


#### 1.3 logistic regression with bootstrap approach


### 2. Predict whether individuals believe OJ Simpson to be guity
  Since this question does not have specific predictors, I would use tree-based models for this part.
