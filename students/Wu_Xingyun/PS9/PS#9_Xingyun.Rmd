---
title: "PS#9_Xingyun"
author: "Xingyun Wu"
date: "2017/3/15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(rcfss)
library(pROC)
library(grid)
library(gridExtra)
library(FNN)
library(kknn)
library(tree)
library(e1071)
library(ggdendro)
library(randomForest)
library(gbm)
library(pander)
#library(tidytext)
#library(tm)
#library(topicmodels)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```


## Part 1: Attitudes towards feminists

### 1. Import and split data

  The data has been imported and spitted as required.
  
```{r read_data1, echo=FALSE, include=FALSE}
data1 <- read.csv('feminist.csv')

# mark the qualitative variables
data1<-data1 %>%
  mutate (female = factor (female, levels =0:1, labels = c("male","female")),
          inc = factor (income, levels = 1: 25, labels = c("0","3","5","7.5","10","11","12.5","15","17","20","22","25","30","35","40","45","50","60","75","90","100","110","120","135","150"))) %>%
  mutate (inc=as.numeric(as.character(inc)))%>%
  # omit missing values
  na.omit()

# data split
set.seed(1234)
data1_split <- resample_partition(data1, p = c("test" = .3, "train" = .7))
data1_train <- as_tibble(data1_split$train)
data1_test <- as_tibble(data1_split$test)
```


### 2. KNN models

  The response variable of my model is `feminist`, and the predictors I choose are `female`, `age`, `educ` and `income`. The plot below shows the relation between test MSE for KNN models and values of k.
  
```{r problem1_2, echo=FALSE}
# define the function calculating MSE
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

# generate KNN models with K=5, 10, 15, 20, 25,..., 100
mse1_2 <- data_frame(k = seq(5, 100, by = 5), 
                      knn = map(k, ~ knn.reg(select(data1_train, -female, -age, -educ, -income), y = data1_train$feminist, test = select(data1_test, -female, -age, -educ, -income), k = .)), 
                      mse = map_dbl(knn, ~ mean((data1_test$feminist - .$pred)^2))) 

# plot the MSE on different k value
ggplot(mse1_2, aes(k, mse)) +
  geom_line() +
  geom_point() +
  labs(title = "KNN for Feminist Score",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)

knn_mse1_2<-min(mse1_2$mse)
knn_mse1_2
```

  According to the plot, the test MSE increases with the increase of k for KNN models. Thus, it would be the model with `k = 5` that produces the lowest test MSE, which is 5.81.
  
  
### 3. Weighted KNN models

  Settings of response variable and predictors are the same as the previous question. The plot below shows the relationship between the test MSE and k of weighted KNN models.
  
```{r problem1_3, echo=FALSE}
# generate weighted KNN models with K=5, 10, 15, 20, 25,..., 100
mse1_3 <- data_frame(k = seq(5, 100, by = 5), 
                      wknn = map(k, ~ kknn(feminist ~ female + age + educ + inc, train = data1_train, test = data1_test, k = .)), 
                      mse_wknn = map_dbl(wknn, ~ mean((data1_test$feminist - .$fitted.values)^2)))

# estimate the MSE for LM
mse_lm <- lm(feminist ~ female + age + educ + inc, data = data1_train) %>%
  mse(.,data1_test)

# plot the MSE on different k value
ggplot(mse1_3, aes(k, mse_wknn)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  labs(title = "Weighted KNN for Feminist Score",
       x = "K",
       y = "Test mean squared error") +
  expand_limits(y = 0)

knn_mse1_3<-min(mse1_3$mse_wknn)
knn_mse1_3
```

  The plot shows that the test MSE decreases with the increase of k. The lowest test MSE is 447, which occurs when `k = 100`.
  However, both the plot and the lowest test MSE shows that the weighted KNN models does not perform well. Their lowest test MSE is much higher than the lowest test MSE for the previous KNN models. In addition, the plot shows that their test MSE is not improved, compared to the test MSE of the OLS model.
  
  
### 4. Comparison

  As is required, I calculated the test MSE for the equivalent linear regression, decision tree, boosting, and random forest methods using the same combination of varialbes as before. And compare them to the test MSE for the best KNN/wKNN models.
  Note that the model with boosting method is with `depth = 1`.
  
```{r problem1_4_models, echo=FALSE}
# decision tree model
tree <- tree(feminist ~ female + age + educ + inc, data = data1_train)
mse_tree <- mse(tree, data1_test)

# function to calculate the MSE for each boosting model
mse_boost <-function(model, test, tree_number) {
  yhat.boost <- predict (model, newdata = test, n.trees=tree_number)
  mse <- mean((yhat.boost - (as_tibble(test))$feminist)^2)
  return (mse)
}
# boosting model
boost <- gbm(feminist ~ female + age + educ + inc, data = data1_train, distribution="gaussian", n.trees = 5000, interaction.depth = 1)
mse_bst <- mse_boost(boost, data1_test, 5000)

# random forest model
rf<- randomForest(feminist ~ female + age + educ + inc, data = data1_train, ntree = 500)
mse_rf <- mse(rf, data1_test)
```

```{r problem1_4_comparison, echo=FALSE}
cat('The test MSE of the best KNN model is: ', knn_mse1_2, '\n')
cat('The test MSE of the best wKNN model is: ', knn_mse1_3, '\n')
cat('The test MSE of equivalent linear regression model is: ', mse_lm, '\n')
cat('The test MSE of equivalent decision tree is: ', mse_tree, '\n')
cat('The test MSE of equivalent boosting method is: ', mse_bst, '\n')
cat('The test MSE of equivalent random forest is: ', mse_rf, '\n')
```

  According to the comparison above, the KNN model performs best, with much lower test MSE. And the other five models have very similar test MSEs. Note that the weighted KNN model does not perform better than the other methods. It seems like the traditional KNN model beats the other five models in this case.
  The traditional KNN model performs better because it relaxes assumptions about the functional form of $f$. It uses the data to estimate $f$ directly, so it could get close to the data points and avoid overcomplexity. But it depends on data whether traditional KNN model or weighted KNN model would perform better.



## Part 2: Voter turnout and depression

### 1. Import and split data

  The `mental_health` data is imported and splitted as required.

```{r problem2_1, echo=FALSE}
data2 <- read.csv('mental_health.csv')

# mark the qualitative variables
data2<-data2 %>%
  #mutate (vote96 = factor (vote96, levels = 0:1, labels = c("not_voted","voted")),
#          black = factor (black, levels = 0:1, labels = c("not_black", "black")),
#         female = factor (female, levels = 0:1, labels = c("male", "female")),
#          married = factor(married, levels = 0:1, labels = c("not_married", "married"))) %>%
#  mutate (inc10=as.numeric(inc10), 
#          mhealth_sum=as.numeric(mhealth_sum))%>%
  # omit missing values
  na.omit()

# data split
set.seed(1234)
data2_split <- resample_partition(data2, p = c("test" = .3, "train" = .7))
data2_train <- as_tibble(data2_split$train)
data2_test <- as_tibble(data2_split$test)
```

```{r problem2_1_functions, echo=FALSE}
# Define the error rate function for trees
err.rate.tree <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])

  pred <- predict(model, newdata = data, type = "class") 
  actual <- data[[response]]

  return(mean(pred != actual, na.rm = TRUE))
}

# Define logit2prob():
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}

```


### 2. Test error rate for KNN models

  The response variable in this part is `vote96`, and the predictors are `mhealth_sum`, `age`, `educ` and `inc10`. I choose these predictors because they represent important features of individuals. `mhealth_sum` represents mental state that would influence behavior. `age` is a very important demographic feature. `educ` and `inc10` are good explanatory variables to locate individuals' social status.

```{r problem2_2_model, echo=FALSE}
set.seed(1234)

# estimate the MSE for GLM
mh_glm <- glm(vote96 ~ age + inc10 + mhealth_sum + educ, data = data2_train, family = binomial) 
# estimate the error rate for this model:
x<- data2_test %>%
  add_predictions(mh_glm) %>%
  mutate (pred = logit2prob(pred),
          prob = pred,
          pred = as.numeric(pred > 0.5))
err.rate.glm <-mean(x$vote96 != x$pred)

# estimate the MSE for KNN K=1,2,...,10
mse_knn <- data_frame(k = 1:10,
                      knn_train = map(k, ~ class::knn(select(data2_train, -vote96),
                                                test = select(data2_train, -vote96),
                                                cl = data2_train$vote96, k = .)),
                      knn_test = map(k, ~ class::knn(select(data2_train, -vote96),
                                                test = select(data2_test, -vote96),
                                                cl = data2_train$vote96, k = .)),
                      mse_train = map_dbl(knn_train, ~ mean(data2_test$vote96 != .)),
                      mse_test = map_dbl(knn_test, ~ mean(data2_test$vote96 != .)))

ggplot(mse_knn, aes(k, mse_test)) +
  geom_line() +
  geom_hline(yintercept = err.rate.glm, linetype = 2) +
  labs(x = "K",
       y = "Test error rate",
       title = "KNN on Vote Turnout") +
  expand_limits(y = 0)

hm_knn_mse<-min(mse_knn$mse_test)
hm_knn_mse
```

  According to the plot, the KNN model does not perform better than GLM. The model with `k = 8` produces the lowest test MSE, which is `r hm_knn_mse`.


### 3. Weighted KNN models

```{r problem2_3, echo=FALSE}
set.seed(1234)
## estimate the MSE for weighted KNN models:

# estimate the MSE for KNN K=1,2,...,10
# note here we need to convert the fitted.values, probablity to 0 or 1. 
mse_wknn <- data_frame(k = 1:10,
                      wknn = map(k, ~ kknn(vote96 ~., train = data2_train, test = data2_test, k =.)),
                      mse_test_wknn = map_dbl(wknn, ~ mean(data2_test$vote96 != as.numeric(.$fitted.values > 0.5))))

mse_wknn_mh <- min(mse_wknn$ mse_test_wknn)

err<-mse_wknn %>%
  left_join(mse_knn, by = "k") %>%
  select(k, mse_test_wknn, mse_test) %>%
  gather(method,mse, -k) %>%
  mutate(method = factor(method, levels =c("mse_test_wknn","mse_test"), labels = c("Weighted KNN","KNN")))

err %>%
  ggplot(aes(k, mse, color = method)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = err.rate.glm, linetype = 2) +
  labs(title = "Test MSE for linear regression vs. KNN, on Vote Turnout",
       subtitle = "Traditional and weighted KNN",
       x = "K",
       y = "Test mean squared error",
       method = NULL) +
  expand_limits(y = 0) +
  theme(legend.position = "bottom")

cat("The lowest MSE is: ", mse_wknn_mh)
```

  According to the plot, with the increase of `k`, the weighted KNN models performs better than the traditional KNN model. The lowest test error rate of KNN models is produced by the model with `k = 10`, and the test MSE is `r mse_wknn_mh`.
  

### 4. Comparison

```{r problem2_4, echo=FALSE}
set.seed(1234)

# the equivalent GLM has already been estimated, with test error rate named err.rate.glm

# the equivalent decision tree
# Single decision tree model:
# data preparation
mh_rm_na_fac<- mh_rm_na %>%
  mutate (vote96 = factor(vote96, levels = 0:1, label =c("no_vote", "vote")))
mh_split <- resample_partition(mh_rm_na_fac, c(test = 0.3, train = 0.7))
mh_train <- as_tibble(mh_split$train)
mh_test <- as_tibble(mh_split$test)
# decision tree
tree_mh <- tree(vote96 ~ age + inc10 + mhealth_sum + educ, data = mh_train)
error_tree <- err.rate.tree(tree_mh, mh_test)

# random forest
rf<- randomForest(vote96 ~ age + inc10 + mhealth_sum + educ, data = mh_train, ntree = 500)
error_rf <- err.rate.tree(rf, mh_test)

# the equivalent boosting model
mh_split <- resample_partition(mh_rm_na, c(test = 0.3, train = 0.7))
mh_train <- as_tibble(mh_split$train)
mh_test <- as_tibble(mh_split$test)
boost_mh = gbm (vote96 ~ age + inc10 + mhealth_sum + educ, data = mh_train, n.trees = 10000, interaction.depth = 4, distribution = "bernoulli")  
error<-list()
for (i in 100:1000) {
  e<- mean(round(predict(boost_mh,newdata = mh_test,n.trees = i)) != mh_test$vote96)
  error<-append(error, e)
}

err_boost<- data_frame("tree" = 100:1000,
                      "error_rate" = unlist(error))

err_boost500 <- min(err_boost$error_rate)

# SVM
mh_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(mh_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(mh_lin_tune)

# Best
mh_lin <- mh_lin_tune$best.model
summary(mh_lin)
```

  According to the comparison, the best model is, with the test error rate as.


## Part 3: Colleges

  In the `College` dataset, only the variable `Private` is a factor variable. And it prevents the PCA analysis from running. So I set it as a numeric variable.
  The first two principal components are shown in the plot below.

```{r problem3_data, echo=FALSE}
# to run PCA, treat the variable "Private" as numerical variable
data3 <- read.csv("College.csv")%>%
  mutate(Private = as.numeric(Private))

pr.out <- prcomp(data3, scale = TRUE)
biplot(pr.out, scale = 0, cex = .8, xlabs=rep(".", nrow(data3)))
pr.out$rotation
```

  According to the table, the variables `Top10perc`, `Top25perc`, `Outstate`, `PhD`, `Terminal` and `Expend` are highly correlated. Their vectors' directions and their length are very similar, all between (0.30, 0.37), on the first principal component dimension. On the second principal component, the strongly correlated variables are `Private`, `Apps`, `Accept`, `Enroll`, `F.Undergrad` and `P.Undergrad`. Their vectors' length are very similar. It makes sense.
  The first principal component represents the quality of the colleges (expenditure, and popularity are also explanatory factors of quality :)). And the second principal component represents how large the population is.


## Part 4: Clustering states

```{r problem4_data, echo=FALSE}
data4 <- read.csv("USArrests")

```


