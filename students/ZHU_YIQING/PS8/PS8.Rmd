---
title: "Problem set #8: tree-based methods and support vector machines"
author: "Yiqing Zhu"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(tree)
library(randomForest)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(titanic)
library(pROC)
library(gbm)
library(ggdendro)
library(e1071)
library(devtools)
library(ggplot2)
library(rcfss)
devtools::install_github("bensoltoff/ggdendro")

options(digits = 3)
theme_set(theme_minimal())

options(na.action = na.warn)
set.seed(1234)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
```

# Part 1: Sexy Joe Biden (redux times two)

**1. Split the data into a training set (70%) and a validation set (30%).**

```{r}
data1 <- read_csv("biden.csv") %>%
    mutate_each(funs(as.factor(.)), female, dem, rep) %>%
    na.omit
data1_split <- resample_partition(data1, c(test = 0.3, train = 0.7))
```

**2. Fit a decision tree to the training data, with biden as the response variable and the other variables as predictors. Plot the tree and interpret the results. What is the test MSE?**

```{r}
biden_tree <- tree(biden ~ ., data = data1_split$train)

# plot tree
tree_data <- dendro_data(biden_tree)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

ptree
```
```{r}
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}
mse_1 = mse(biden_tree, data1_split$test)
```

If the person is a democrat, then the model estimates his/her feeling thermometer for Biden to be approximately 74.51. If the person is a republican, then the model estimate he/she has a feeling thermometer to be approximately 43.23. If the person is an independent, the model estimate he/she has a feeling thermometer to be approximately 57.6. 
The test MSE is `r mse_1`.

**3. Now fit another tree to the training data with specific control options. Use cross-validation to determine the optimal level of tree complexity, plot the optimal tree, and interpret the results. Does pruning the tree improve the test MSE?**
```{r}
biden_tree2 <- tree(biden ~ ., data = data1_split$train, 
                    control = tree.control(nobs = nrow(data1_split$train),
                              mindev = 0))

# plot tree
tree_data2 <- dendro_data(biden_tree2)
ptree2 <- ggplot(segment(tree_data2)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data2), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data2), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

ptree2
```
```{r}
# generate 10-fold CV trees
biden_cv <- crossv_kfold(data1, k = 10) %>%
  mutate(tree = map(train, ~ tree(biden ~ ., data = .,
     control = tree.control(nobs = nrow(data1),
                            mindev = 0))))

# calculate each possible prune result for each fold
biden_cv <- expand.grid(biden_cv$.id, 2:10) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(biden_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         mse = map2_dbl(prune, test, mse))

biden_cv %>%
  select(k, mse) %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of terminal nodes",
       y = "Test MSE")
```

From the plot above, I decide the optimal level of tree complexity is with three terminal nodes. Here is the optimal tree:

```{r}
mod <- prune.tree(biden_tree2, best = 3)

# plot tree
tree_data3 <- dendro_data(mod)
ptree3 <- ggplot(segment(tree_data3)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data3), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data3), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

ptree3
```

This plot is the same as the one in exercise 2. If the person is a democratic, then the model estimates his/her feeling thermometer for Biden to be approximately 74.51. If the person is a republican, then the model estimate he/she has a feeling thermometer to be approximately 43.23. If the person is an independent, the model estimate he/she has a feeling thermometer to be approximately 57.6. 

```{r}
mse_2 = mse(biden_tree2, data1_split$test)
mse_3 = mse(mod, data1_split$test)
```

The test MSE is `r mse_3`. Compared to the test MSE of full_grown tree `r mse_2`, pruning the tree does improve the test MSE.

**4. Use the bagging approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results.**

```{r}
(biden_bag <- randomForest(biden ~ ., data = data1_split$train, importance = TRUE,
                           mtry = 5, ntree = 500))
mse_4 = mse(biden_bag, data1_split$test)
```

I obtain the test MSE `r mse_4`.

```{r}
data_frame(var = rownames(importance(biden_bag)),
           MeanDecreasemse = importance(biden_bag)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreasemse, fun = median)) %>%
  ggplot(aes(var, MeanDecreasemse)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting feeling thermometer for Biden",
       subtitle = "Bagging",
       x = NULL,
       y = "Degree of Importance (Average decrease in the MSE)")
```

For regression trees, bigger values are better, in other words, the above plot indicates that when including the `dem` variable, the model test MSE will decrease more than 80%, when including the `rep` vairable, the model test MSE will decrease more than 20% percent, and the variable `age`, `educ`, and `female` don't affect the MSE decrease much. Therefore, for the biden bagged model, `dem` and `rep` are the most important predictors, whereas `age`, `educ`, and `female` are relatively unimportant.

**5. Use the random forest approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.**

```{r}
(biden_rf <- randomForest(biden ~ ., data = data1_split$train, importance = TRUE, ntree = 500))
mse_5 = mse(biden_rf, data1_split$test)
```

The test MSE I obtain is `r mse_5`.

```{r}
data_frame(var = rownames(importance(biden_rf)),
           MeanDecreasemse = importance(biden_rf)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreasemse, fun = median)) %>%
  ggplot(aes(var, MeanDecreasemse)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting feeling thermometer for Biden",
       subtitle = "Random Forest",
       x = NULL,
       y = "Degree of Importance (Average decrease in the MSE)")
```

Simialr as part 4, when including the `dem` variable, the model test MSE will decrease about 29%, when including the `rep` vairable, the model test MSE will decrease about 27% percent, and the variable `age`, `educ`, and `female` don't affect the MSE decrease much. Therefore, for the biden random forest model, `dem` and `rep` are the most important predictors, whereas `age`, `educ`, and `female` are relatively unimportant.

The m (No. of variables tried at each split) = 1, compared to m = 5 in the bagging model, we can see the error rate has decreased.

**6. Use the boosting approach to analyze the data. What test MSE do you obtain? How does the value of the shrinkage parameter λ influence the test MSE?**

```{r}
boost_mse <- function(model, input_data){
  pred <- predict(model, input_data, n.trees = 10000)
  actual_index <- input_data$idx
  actual <- input_data$data$biden[actual_index]
  mse <- (mean((pred - actual)^2))
  return(mse)
}
```

```{r}
set.seed(1234)
biden_boost_shrink1 <- gbm(biden ~ ., data = data1_split$train, shrinkage = 0.0001)
biden_boost_shrink2 <- gbm(biden ~ ., data = data1_split$train, shrinkage = 0.001)
biden_boost_shrink3 <- gbm(biden ~ ., data = data1_split$train, shrinkage = 0.01)
biden_boost_shrink4 <- gbm(biden ~ ., data = data1_split$train, shrinkage = 0.1)

mse_6 <- boost_mse(biden_boost_shrink1, data1_split$test)
mse_7 <- boost_mse(biden_boost_shrink2, data1_split$test)
mse_8 <- boost_mse(biden_boost_shrink3, data1_split$test)
mse_9 <- boost_mse(biden_boost_shrink4, data1_split$test)
```

I got the following test MSE:
```{r}
boostmse <- matrix(c(mse_6, mse_7, mse_8, mse_9), ncol=4, byrow=TRUE)
colnames(boostmse) <- c("Shrinkage = 0.0001", "Shrinkage = 0.001", "Shrinkage = 0.01",
                        "Shrinkage = 0.1")
rownames(boostmse) <- c("test MSE")
boostmse <- as.table(boostmse)
boostmse
```

With the value of the shrinkage parameter λ increasing, the test MSE decreases.


# Part 2: Modeling voter turnout

**1. Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five tree-based models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)**

```{r}
data2 <- read_csv("mental_health.csv") %>%
  mutate(vote96 = factor(vote96, levels = 0:1, labels = c("Not Voted", "Voted")),
         black = factor(black, levels = 0:1, labels = c("Not Black", "Black")),
         married = factor(married, levels = 0:1, labels = c("Not Married", "Married")),
         female = factor(female, levels = 0:1, labels = c("Not Female", "Female"))) %>%
  na.omit

data2_split <- resample_partition(data2, p = c("test" = .3, "train" = .7))
```

```{r}
err.rate.tree <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  
  pred <- predict(model, newdata = data, type = "class")
  actual <- data[[response]]
  
  return(mean(pred != actual, na.rm = TRUE))
}

tree_model <- function(variable, model, input_data){

set.seed(1234)
# generate 10-fold CV trees
vote_cv <- input_data %>%
  crossv_kfold(k = 10) %>%
  mutate(tree = map(train, ~ tree(variable, data = .,
                                  control = tree.control(nobs = nrow(input_data),
                                  mindev = .001))))

# calculate each possible prune result for each fold
vote_cv <- expand.grid(vote_cv$.id,
                          seq(from = 2, to = ceiling(length(model$frame$yval) / 2))) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(vote_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.misclass(.x, best = .y)),
         mse = map2_dbl(prune, test, err.rate.tree))

# plot
vote_cv %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(title = "Voter turnout tree",
       subtitle = variable,
       x = "Number of terminal nodes",
       y = "Test error rate")
}
```

First model: vote96 ~ mhealth_sum

```{r}
tree1 <- tree(vote96 ~ mhealth_sum, data = data2, 
              control = tree.control(nobs = nrow(data2), mindev = 0))
variable1 <- "vote96 ~ mhealth_sum"
tree_model(variable1, tree1, data2)
```

From the above plot, I can find the optimal level of tree complexity is with four terminal nodes.

Here is the plot of the optimal tree:
```{r}
mod <- prune.tree(tree1, best = 4)
tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter Turnout tree",
       subtitle = "mhealth_sum")
```

Second model: vote96 ~ mhealth_sum + age

```{r}
tree2 <- tree(vote96 ~ mhealth_sum + age, data = data2, 
              control = tree.control(nobs = nrow(data2), mindev = 0))
variable2 <- "vote96 ~ mhealth_sum + age"
tree_model(variable2, tree2, data2)
```

From the above plot, I can find the optimal level of tree complexity is with two or three terminal nodes.

Here is the plot of the optimal tree:
```{r}
mod <- prune.tree(tree2, best = 3)
tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter Turnout tree",
       subtitle = "mhealth_sum + age")
```

Third model: vote96 ~ mhealth_sum + age + educ

```{r}
tree3 <- tree(vote96 ~ mhealth_sum + age + educ, data = data2, 
              control = tree.control(nobs = nrow(data2), mindev = 0))
variable3 <- "vote96 ~ mhealth_sum + age + educ"
tree_model(variable3, tree3, data2)
```

From the above plot, I can find the optimal level of tree complexity is with six terminal nodes.

Here is the plot of the optimal tree:
```{r}
mod <- prune.tree(tree3, best = 6)
tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter Turnout tree",
       subtitle = "mhealth_sum + age + educ")
```

Fourth model: vote96 ~ mhealth_sum + age + educ + black

```{r}
tree4 <- tree(vote96 ~ mhealth_sum + age + educ + black, data = data2, 
              control = tree.control(nobs = nrow(data2), mindev = 0))
variable4 <- "vote96 ~ mhealth_sum + age + educ + black"
tree_model(variable4, tree4, data2)
```

From the above plot, I can find the optimal level of tree complexity is with six terminal nodes.

Here is the plot of the optimal tree:
```{r}
mod <- prune.tree(tree4, best = 6)
tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter Turnout tree",
       subtitle = "mhealth_sum + age + educ + black")
```

Fifth model: vote96 ~ mhealth_sum + age + educ + black + inc10

```{r}
tree5 <- tree(vote96 ~ mhealth_sum + age + educ + black + inc10, data = data2, 
              control = tree.control(nobs = nrow(data2), mindev = 0))
variable5 <- "vote96 ~ mhealth_sum + age + educ + black + inc10"
tree_model(variable5, tree5, data2)
```

From the above plot, I can find the optimal level of tree complexity is with seventeen terminal nodes.

Here is the plot of the optimal tree:
```{r}
mod <- prune.tree(tree5, best = 17)
tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter Turnout tree",
       subtitle = "mhealth_sum + age + educ + black + inc10")
```

Let's use Random Forest to select the best model among the five above.

```{r}
(tree_rf1 <- randomForest(vote96 ~ mhealth_sum, data = data2,
                          ntree = 500))
```

```{r}
(tree_rf2 <- randomForest(vote96 ~ mhealth_sum + age, data = data2,
                          ntree = 500))
```

```{r}
(tree_rf3 <- randomForest(vote96 ~ mhealth_sum + age + educ, data = data2,
                          ntree = 500))
```

```{r}
(tree_rf4 <- randomForest(vote96 ~ mhealth_sum + age + educ + black, data = data2,
                          ntree = 500))
```

```{r}
(tree_rf5 <- randomForest(vote96 ~ mhealth_sum + age + educ + black + inc10, data = data2,
                          ntree = 500))
```

According to the OOB estimate of error rate for the five models above, I find that the third model (vote96 ~ mhealth_sum + age + educ) is the best, with the lowest OOB estimate of error rate.

Let's recall the optimal tree for this model:
```{r}
mod <- prune.tree(tree3, best = 6)
tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Voter Turnout tree",
       subtitle = "mhealth_sum + age + educ")
```

If the person's age is below 44.5, we go to the left branch of the model; then if he/she has education year more than 14.5, the model estimates he/she voted, if he/she has education year less than 14.5, we again go to the left branch of the tree; if he/she has education year less than 11.5, the model estimates he/she not voted, if he/she has education year more than 11.5, the model estimates he/she voted. If the person's age is above 44.5, we go to the right branch of the model; then if he/she has education year more than 12.5, the model estimates he/she voted; if he/she has education year less than 12.5, we go to the left branch of the tree; then if he/she has mental health index less than 4.5, the model estimates he/she voted, if he/she has mental health index more than 4.5, the model also estimates he/she voted. Actually in this model, all people aged above 44.5 are estimated to have voted.

**2. Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five SVM models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)**

First model: Linear kernel

```{r}
set.seed(1234)
vote_lin_tune <- tune(svm, vote96 ~ ., data = as_tibble(data2_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(vote_lin_tune)
```

```{r}
vote_lin <- vote_lin_tune$best.model
summary(vote_lin)
```

```{r}
fitted <- predict(vote_lin, as_tibble(data2_split$test), decision.values = TRUE) %>%
  attributes

roc_line <- roc(as_tibble(data2_split$test)$vote96, fitted$decision.values)
plot(roc_line)
```

```{r}
auc(roc_line)
```

Observing the linear kernel with all predictor variables, the model gets the best cost level at 0.1 and has a 10-fold CV error rate 28.9%, and the AUC is 0.769, indicating that we have 74.6% chance to get the correct prediction under this model.


Second model: Polynomial kernel

```{r}
set.seed(1234)
vote_poly_tune <- tune(svm, vote96 ~ ., data = as_tibble(data2_split$train),
                       kernel = "polynomial",
                       range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(vote_poly_tune)
```

```{r}
vote_poly <- vote_poly_tune$best.model
summary(vote_poly)
```

```{r}
fitted <- predict(vote_poly, as_tibble(data2_split$test), decision.values = TRUE) %>%
  attributes

roc_poly <- roc(as_tibble(data2_split$test)$vote96, fitted$decision.values)
plot(roc_poly)
```

```{r}
auc(roc_poly)
```

Observing the polynomial kernel with all predictor variables, the model gets the best cost level at 1 and has a 10-fold CV error rate 29.2%, and the AUC is 0.749, indicating that we have 74.9% chance to get the correct prediction under this model.


Third model: Radial kernel

```{r}
set.seed(1234)
vote_rad_tune <- tune(svm, vote96 ~ ., data = as_tibble(data2_split$train),
                    kernel = "radial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(vote_rad_tune)
```

```{r}
vote_rad <- vote_rad_tune$best.model
summary(vote_rad)
```

```{r}
fitted <- predict(vote_rad, as_tibble(data2_split$test), decision.values = TRUE) %>%
  attributes

roc_rad <- roc(as_tibble(data2_split$test)$vote96, fitted$decision.values)
plot(roc_rad)
```

```{r}
auc(roc_rad)
```

Observing the radial kernel with all predictor variables, the model gets the best cost level at 5 and has a 10-fold CV error rate 28.7%, and the AUC is 0.75, indicating that we have 75% chance to get the correct prediction under this model.


Forth model: Radial kernel

```{r}
set.seed(1234)
vote_lin_tune2 <- tune(svm, vote96 ~ mhealth_sum + inc10 + black + age, data = as_tibble(data2_split$train),
                    kernel = "linear",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100)))
summary(vote_lin_tune2)
```

```{r}
vote_lin2 <- vote_lin_tune2$best.model
summary(vote_lin2)
```

```{r}
fitted <- predict(vote_lin2, as_tibble(data2_split$test), decision.values = TRUE) %>%
  attributes

roc_line2 <- roc(as_tibble(data2_split$test)$vote96, fitted$decision.values)
plot(roc_line2)
```

```{r}
auc(roc_line2)
```

Observing the linear kernel with predictor variables mhealth_sum, inc10, black, and age, the model gets the best cost level at 1 and has a 10-fold CV error rate 31.5%, and the AUC is 0.715, indicating that we have 71.5% chance to get the correct prediction under this model.


Model 5: polynomial kernel with different degrees

```{r}
set.seed(1234)
vote_poly_tune2 <- tune(svm, vote96 ~ ., data = as_tibble(data2_split$train),
                    kernel = "polynomial",
                    range = list(cost = c(.001, .01, .1, 1, 5, 10, 100), degree = c(2, 4, 6)))
summary(vote_poly_tune2)
```

```{r}
vote_poly2 <- vote_poly_tune2$best.model
summary(vote_poly2)
```

```{r}
fitted <- predict(vote_poly2, as_tibble(data2_split$test), decision.values = TRUE) %>%
  attributes

roc_poly2 <- roc(as_tibble(data2_split$test)$vote96, fitted$decision.values)
plot(roc_poly2)
```

```{r}
auc(roc_poly2)
```

Observing the polynomial kernel with all predictor variables and some different degree levels, the model gets the best cost level at 1 and best degree at 2. It has a 10-fold CV error rate 27.6%, and the AUC is 0.76, indicating that we have 76% chance to get the correct prediction under this model.

```{r}
plot(roc_line, print.auc = TRUE, col = "blue")
plot(roc_poly, print.auc = TRUE, col = "red", print.auc.y = .4, add = TRUE)
plot(roc_rad, print.auc = TRUE, col = "orange", print.auc.y = .3, add = TRUE)
plot(roc_line2, print.auc = TRUE, col = "brown", print.auc.y = .2, add = TRUE)
plot(roc_poly2, print.auc = TRUE, col = "pink", print.auc.y = .1, add = TRUE)
```

Comparing all five model's AUC as above, the first linear kernel model performed the best.


# Part 3: OJ Simpson

```{r}
data3 <- read_csv("simpson.csv") %>%
    mutate(#guilt = factor(guilt, levels = 0:1, labels = c("Not Guilt", "Guilt")),
           dem = factor(dem, levels = 0:1, labels = c("Not Dem", "Dem")),
           rep = factor(rep, levels = 0:1, labels = c("Not Rep", "Rep")),
           ind = factor(ind, levels = 0:1, labels = c("Not Ind", "Ind")),
           female = factor(female, levels = 0:1, labels = c("Male", "Female")),
           black = factor(black, levels = 0:1, labels = c("Not Black", "Black")),
           hispanic = factor(hispanic, levels = 0:1, labels = c("Not Hispanic", "Hispanic"))) %>%
    na.omit

data3_split <- resample_partition(data3, c(test = 0.3, train = 0.7))
```


**1. What is the relationship between race and belief of OJ Simpson's guilt? Develop a robust statistical learning model and use this model to explain the impact of an individual's race on their beliefs about OJ Simpson's guilt.**

Let's use the logistic regression model to estimate the relationship between race and belief.

```{r}
guilt_lg <- glm(guilt ~ black + hispanic, data = data3_split$train, family = binomial)
summary(guilt_lg)
tidy(guilt_lg)
```

```{r}
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}

x_accuracy <- data3 %>%
  add_predictions(guilt_lg) %>%
  mutate(pred = logit2prob(pred),
         prob = pred,
         pred = as.numeric(pred > .5))

accuracy <- mean(x_accuracy$guilt == x_accuracy$pred, na.rm = TRUE)
```

```{r}
auc <- auc(x_accuracy$guilt, x_accuracy$pred)
auc
```

The logistic model gives `r accuracy` accuracy and  `r auc` AUC, and the p-value for each predictor is very small, which means that the model is pretty good. The model indicates that the an individual's race and their beliefs about OJ Simpson's guilt are negatively related. If the person is black, then the log-odds of perceiving Simpson as guilty would decrease by 2.991; if the person is Hispanic, then the log-odds of perceiving Simpson as guilty would decrease by 0.555.


**2. How can you predict whether individuals believe OJ Simpson to be guilty of these murders? Develop a robust statistical learning model to predict whether individuals believe OJ Simpson to be either probably guilty or probably not guilty and demonstrate the effectiveness of this model using methods we have discussed in class.**

Let's use k-fold cross validation method to estimate an optimal tree model for predicting whether individuals believe OJ Simpson to be guilty of these murders.

```{r}
simpson <- read_csv("simpson.csv") %>%
    mutate(guilt = factor(guilt, levels = 0:1, labels = c("Innocent", "Guilty")),
           dem = factor(dem, levels = 0:1, labels = c("Not Dem", "Dem")),
           rep = factor(rep, levels = 0:1, labels = c("Not Rep", "Rep")),
           ind = factor(ind, levels = 0:1, labels = c("Not Ind", "Ind")),
           female = factor(female, levels = 0:1, labels = c("Male", "Female")),
           black = factor(black, levels = 0:1, labels = c("Not Black", "Black")),
           hispanic = factor(hispanic, levels = 0:1, labels = c("Not Hispanic", "Hispanic"))) %>%
    na.omit

simpson_split <- resample_partition(simpson, c(test = 0.3, train = 0.7))

# estimate model
simpson_tree <- tree(guilt ~ ., data = simpson,
                     control = tree.control(nobs = nrow(simpson),
                            mindev = .001))
mod = simpson_tree

err.rate.tree <- function(model, data) {
  data <- as_tibble(data)
  response <- as.character(model$terms[[2]])
  
  pred <- predict(model, newdata = data, type = "class")
  actual <- data[[response]]
  
  return(mean(pred != actual, na.rm = TRUE))
}

# generate 10-fold CV trees
simpson_cv <- simpson %>%
  na.omit() %>%
  crossv_kfold(k = 10) %>%
  mutate(tree = map(train, ~ tree(guilt ~ ., data = .,
     control = tree.control(nobs = nrow(simpson),
                            mindev = .001))))

# calculate each possible prune result for each fold
simpson_cv <- expand.grid(simpson_cv$.id,
                          seq(from = 2, to = ceiling(length(mod$frame$yval) / 2))) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(simpson_cv, by= ".id") %>%
  mutate(prune = map2(tree, k, ~ prune.misclass(.x, best = .y)),
         mse = map2_dbl(prune, test, err.rate.tree))

simpson_cv %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(title = "Simpson Guilt tree",
       subtitle = "guilt ~ .",
       x = "Number of terminal nodes",
       y = "Test error rate")
```

Using the k-fold cross validation method, we find the optimal tree model should have four terminal nodes. Let's visualize it.

```{r}
mod <- prune.tree(simpson_tree, best = 4)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

ptree
```

In this model, if the person is black, he/she will think Simpson innocent; if the person is not black, we will go to the left branch. Then if the person is aged less than 19.5, he/she will think Simpson innocent; if he/she is aged more than 19.5, he/she will think Simpson is guilty, no matter he/she is republican or not.

Using the following random forest method, we can justify the validation of this model.
```{r}
randomForest(guilt ~ rep + age + black, type = 'classification', data = na.omit(as_tibble(data3)))

```






















