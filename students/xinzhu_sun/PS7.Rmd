---
title: "PS7"
author: "Xinzhu Sun"
date: "2/27/2017"
output:
  github_document:
    toc: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, warning = FALSE, error = FALSE)
```
````{r library}
library(dplyr)
library(ggplot2)
library(tidyr)
library(modelr)
library(broom)
library(purrr)
library(readr)
set.seed(1234)
options(digits = 3)
library(tidyverse)
library(gam)
```

## Part 1: Sexy Joe Biden
```{r MSE the traditional approach}
data <- read_csv("data/biden.csv")
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}
lin_Biden <- lm(biden ~ age + female + educ + dem + rep, data = data) 
lin_mse <- mse(lin_Biden, data)
summary(lin_Biden)
```
1.The traning MSE of the model using the traditional approach is `r lin_mse`.
``` {r, MSE the validation set approach}
data_split <- resample_partition(data, c(test = 0.3, train = 0.7))
lin_Biden_train <- lm(biden ~ age + female + educ + dem + rep, data = data_split$train) 
lin_mse_val <- mse(lin_Biden_train, data_split$test)
```
2.The test MSE of the model using the validation set approach is `r lin_mse_val`. This is higher than the training MSD from step 1.
``` {r, repeat the validation set approach}
mseCal <- function(i) {
  #100 different splits
  if (i == 30) {
    i <- 30.1
  }
  r <- i / 100
  split <- resample_partition(data, c(test = r, train = 1 - r))
  lin_model <- lm(biden ~ age + female + educ + dem + rep, data = split$train)
  mse(lin_model, split$test)
}
set.seed(1234)
df <- data.frame(index = 1:100)
df$mse <- unlist(lapply(df$index, mseCal))
ggplot(data = df, aes(x = index, mse)) +
  geom_smooth() +
  geom_point() +
  labs(title="MSE vs Percentage of elements in the testing set",  x ="Percentage of elements in the testing set", y = "MSE") 
```

3.We can see from the plot that most partitions result in an MSE of 400, but high percentages result in higher MSE, low percentages result in lower MSE. This is due to over fitting and under fitting respectively. 
``` {r, MSE the LOOCV approach}
loocv_data <- crossv_kfold(data, k = nrow(data))
loocv_models <- map(loocv_data$train, ~ lm(biden ~ age + female + educ + dem + rep, data = .))
loocv_mse <- map2_dbl(loocv_models, loocv_data$test, mse)
loocv_mean_mse <- mean(loocv_mse)
```
4.The test MSE of the model using the LOOCV approach is `r loocv_mean_mse`. It's lower than the test MSE from step 2 and higher than the training MSE from step 1.
``` {r, MSE the $10$-fold cross-validation approach}
mseFoldCal <- function(i) {
  tenFold_data <- crossv_kfold(data, k = 10)
  tenFold_models <- map(tenFold_data$train, ~ lm(biden ~ age + female + educ + dem + rep, data = .))
  tenFold_mse <- map2_dbl(tenFold_models, tenFold_data$test, mse)
  tenFold_mean_mse <- mean(tenFold_mse)
}
set.seed(1234)
ten_fold_df <- data.frame(index = 1:100)
ten_fold_df$mse <- unlist(lapply(ten_fold_df$index, mseFoldCal))
```
5.The test MSE using the $10$-fold cross-validation approach is `r ten_fold_df$mse[1]`. This is very close to the test MSE of the model using the LOOCV approach, lower than the test MSE from step 2 and higher than the training MSE from step 1.
``` {r repeat the $10$-fold cross-validation approach}
qplot(mse, data = ten_fold_df, geom="histogram", main = "Histogram of model MSE using the $10$-fold cross-validation approach", binwidth = .1, xlab = 'MSE', ylab ='Count')
```

6.The histogram of the test MSE using the $10$-fold cross-validation approach is always very close to 398 and within the interval of [396,400] under different iterations. This is very similar to the results using the LOOCV approach.
``` {r, using the bootstrap}
biden_boot <- data %>%
  modelr::bootstrap(1000) %>%
  mutate(model = map(strap, ~ lm(biden ~ age + female + educ + dem + rep, data = .)),
         coef = map(model, tidy))
biden_boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(est.boot = mean(estimate),
            se.boot = sd(estimate, na.rm = TRUE))
```
7.The results are very close to each other. Comparing to the initial one, using bootstrap leads to std of age 0.0006 higher; parameter of dem 0.014 higher, std 0.45 higher; parameter of educ 0.008 higher, std 0.0036 lower; std of female 0.0062 higher; parameter of rep 0.0185 lower, std 0.1338 higher. In general, the standard error of bootstrap approach is higher.

## Part 2: College(bivariate)
``` {r, linear}
data <- read.csv("data/college.csv")
lin_coll <- lm(Outstate ~ ., data = data)
summary(lin_coll)
```
Let's look at those values that are significant, say, Private, Apps, Accept, Room.Board, perc.alumni, Expend, Grad.Rate.
``` {r, linear_7}
lin_7 <- lm(Outstate ~ Room.Board + Private + perc.alumni + Expend + Grad.Rate + Apps + Accept, data = data)
summary(lin_7)
```
The $R^2$ is 0.741 here.
``` {r, Private}
lin_Private <- lm(Outstate ~ Private, data = data)
summary(lin_Private)
```
``` {r, Apps}
lin_Apps <- lm(Outstate ~ Apps, data = data)
summary(lin_Apps)
```
``` {r, Accept}
lin_Accept <- lm(Outstate ~ Accept, data = data)
summary(lin_Accept)
```
``` {r, Room.Board}
lin_Room.Board <- lm(Outstate ~ Room.Board, data = data)
summary(lin_Room.Board)
```
``` {r,  perc.alumni}
lin_perc.alumni <- lm(Outstate ~ perc.alumni, data = data)
summary(lin_perc.alumni)
```
``` {r, Expend}
lin_Expend <- lm(Outstate ~ Expend, data = data)
summary(lin_Expend)
```
``` {r, Grad.Rate}
lin_Grad.Rate <- lm(Outstate ~ Grad.Rate, data = data)
summary(lin_Grad.Rate)
```
Among them, Private, Room.Board, perc.alumni, Expend and Grad.Rate are still significant and have relatively large $R^2$(around 0.3-0.4). 
To see whether the relationship is linear, lets plot:
``` {r, Room.Board 2}
RBDF <- add_predictions(data, lin_Room.Board)
RBDF <- add_residuals(RBDF, lin_Room.Board)
ggplot(RBDF, aes(x = pred, y = resid)) +
  geom_smooth() +
  geom_point() +
  labs(title="Linear model regression for Room.Board",  x ="Predicted Room.Board", y = "Residuals") 
```

The relationship between Room.Board and outstate is linearly.
``` {r, perc.alumni 2}
paDF <- add_predictions(data, lin_perc.alumni)
paDF <- add_residuals(paDF, lin_perc.alumni)
ggplot(paDF, aes(x = pred, y = resid)) +
  geom_smooth() +
  geom_point() +
  labs(title="Linear model regression for perc.alumni",  x ="Predicted perc.alumni", y = "Residuals") 
```

The relationship between perc.alumni and outstate is linearly.
``` {r, Expend 2}
expendDF <- add_predictions(data, lin_Expend)
expendDF <- add_residuals(expendDF, lin_Expend)
ggplot(expendDF, aes(x = pred, y = resid)) +
  geom_smooth() +
  geom_point() +
  labs(title="Linear model regression for Expend",  x ="Predicted expenditure", y = "Residuals") 
```

The plot shows that the model is mostly accurate for expenditures around $10 000 USD$ but gets worse as the expenditure increases. This likely due to a non-linear term becoming dominant. 
``` {r, Grad.Rate 2}
GRDF <- add_predictions(data, lin_Grad.Rate)
GRDF <- add_residuals(GRDF, lin_Grad.Rate)
ggplot(GRDF, aes(x = pred, y = resid)) +
  geom_smooth() +
  geom_point() +
  labs(title="Linear model regression for Grad.Rate",  x ="Predicted Grad.Rate", y = "Residuals") 
```

When Grad.Rate is arround 10000 the model is mostly accurate while when it goes higher or lower, the worse the fit. This is likely due to overfitting or underfitting problem.

So lets try a polynomial fit, to find the polynomial we will use 10-fold Cross-vakudation.
``` {r Expend 3}
set.seed(1234)
tenFold_data <- crossv_kfold(data, k = 10)

polyMSE <- function(d) {
  tenFold_models <- map(tenFold_data$train, ~ lm(Outstate ~ poly(Expend, d), data = .))
  tenFold_mse <- map2_dbl(tenFold_models, tenFold_data$test, mse)
  tenFold_mean_mse <- mean(tenFold_mse)
}

tenFoldDF <- data.frame(index = 1:10)
tenFoldDF$mse <- unlist(lapply(1:10, polyMSE))

ggplot(tenFoldDF, aes(index, mse)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(title="MSE vs polynomial fit degree for Expend",  x ="Degree", y = "MSE") 
```

We can see that the lowest MSE is obtained for a polynomial of degree 3. So lets SE what that model leads to.
``` {r Expend 4}
poly3_Expend <- lm(Outstate ~ poly(Expend , 3), data = data)
summary(poly3_Expend)
```
$R^2$ is 0.603 now, that's a much better fit. Lets plot it:
``` {r Expend 5}
expendDF <- add_predictions(data, poly3_Expend)
expendDF <- add_residuals(expendDF, poly3_Expend)
ggplot(expendDF, aes(x = pred, y = resid)) +
  geom_smooth() +
  geom_point() +
  labs(title="3rd order polynomial model regression for Expend",  x ="Predicted expenditure", y = "Residuals") 
```

Now lets look at the Grad.Rate:
``` {r Grad.Rate 3}
set.seed(1234)
tenFold_data <- crossv_kfold(data, k = 10)

polyMSE <- function(d) {
  tenFold_models <- map(tenFold_data$train, ~ lm(Outstate ~ poly(Grad.Rate, d), data = .))
  tenFold_mse <- map2_dbl(tenFold_models, tenFold_data$test, mse)
  tenFold_mean_mse <- mean(tenFold_mse)
}

tenFoldDF <- data.frame(index = 1:10)
tenFoldDF$mse <- unlist(lapply(1:10, polyMSE))

ggplot(tenFoldDF, aes(index, mse)) +
  geom_line() +
  geom_point() +
  scale_y_log10() +
  labs(title="MSE vs polynomial fit degree for Grad.Rate",  x ="Degree", y = "MSE") 
```

We can see that the lowest MSE is obtained for a polynomial of degree 4. So lets SE what that model leads to.
``` {r Grad.Rate 4}
poly4_Grad.Rate <- lm(Outstate ~ poly(Grad.Rate, 4), data = data)
summary(poly4_Grad.Rate)
```
Its $R^2$ doesn't increase too much.
``` {r, linear_7_new}
lin_7_new <- lm(Outstate ~ Private + Room.Board + perc.alumni +  poly(Expend , 3) + Grad.Rate + Apps + Accept, data = data)
summary(lin_7_new)
```
$R^2$ increases to 0.777 now, but Accept is not as significant as before anymore.

This model has none of the edge effects of the linear fit and is a much better fit for the data all around. Since that went well, lets try a nonlinear model for the worst fit Accept, but this time a spline. First we will find the optimal number of knots with 3rd order piece wise polynomials:
``` {r Accept 2}
set.seed(1234)
tenFold_data <- crossv_kfold(data, k = 10)
polyMSE <- function(n) {
  tenFold_models <- map(tenFold_data$train, ~ glm(Outstate ~ bs(Accept, df = n), data = .))
  tenFold_mse <- map2_dbl(tenFold_models, tenFold_data$test, mse)
  tenFold_mean_mse <- mean(tenFold_mse)
}

tenFoldDF <- data.frame(index = 1:10)
tenFoldDF$mse <- unlist(lapply(1:10, polyMSE))

ggplot(tenFoldDF, aes(index, mse)) +
  geom_line() +
  geom_point() +
  labs(title="MSE vs number of knots",  x ="Number of knots", y = "MSE")
```

We see the minimum is at 9, but that 5 is almost as low so we will try that.
``` {r Accept 3}
spline5_Accept <- glm(Outstate ~ bs(Accept, df = 5), data = data)
summary(spline5_Accept)
```
Doesn't report $R^2$ , lets do futher regression
``` {r, linear_7_final}
lin_7_final <- lm(Outstate ~ Private + Room.Board + perc.alumni +  poly(Expend, 3) + Grad.Rate + Apps + poly(Accept, 5), data = data)
summary(lin_7_final)
```
$R^2$ doesn't improve to much and the model from Accept aspect is still a bad fit. Maybe because Accept itself has very little predictive power.

## Part 3: College(GAM)
1.Split the data into a training set and a test set
``` {r, split data}
data <- read.csv("data/college.csv")
data_split <- resample_partition(data, c(test = 0.3, train = 0.7))
```
``` {r, OLS}
lin_college <- lm(Outstate ~ Private + Room.Board + PhD + perc.alumni + Expend + Grad.Rate, data = data_split$train)
summary(lin_college)
```
2.All variables are significant and that together $R^2$ of the model is 0.751. 

Let's look at the residuals plot:
``` {r residuals plot}
fullDF <- add_predictions(data, lin_college)
fullDF <- add_residuals(fullDF, lin_college)
ggplot(fullDF, aes(x = pred, y = resid)) +
  geom_smooth() +
  geom_point() +
  labs(title="Residuals vs predicted values for the full linear fit",  x ="Predicted expenditure", y = "Residuals")
```

The error appears to be largest for large predicted values.

3.let's see if a a GAM can fix the problem above. We will use the sum of loess fits for our continuous variables since the loess fits is a nice smooth fit and should work with most distributions.
``` {r, GAM}
college_gam <- gam(Outstate ~ lo(perc.alumni) + lo(PhD) + lo(Expend) + lo(Grad.Rate) + lo(Room.Board) + Private , data = data_split$train)
summary(college_gam)
```
Doesn't report $R^2$, lets look at the residuals plot
``` {r}
gamDF <- add_predictions(data, college_gam)
gamDF <- add_residuals(gamDF, college_gam)

ggplot(gamDF, aes(x = pred, y = resid)) +
  geom_smooth() +
  geom_point() +
  labs(title="Residuals vs predicted values for the full linear fit",  x ="Predicted expenditure", y = "Residuals")
```

We can see it provides better fit and much less edge effect, it looks like the GAM model has much lower error.

Lets look at a few of the individual components:
``` {r perc.alumni Q3}
college_gam_terms <- preplot(college_gam, se = TRUE, rug = FALSE)

data_frame(x = college_gam_terms$`lo(perc.alumni)`$x,
           y = college_gam_terms$`lo(perc.alumni)`$y,
           se.fit = college_gam_terms$`lo(perc.alumni)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       x = "Percentage of donating alumni",
       y = expression(f[1](perc.alumni)))
```

The higher the percentage of alumni donations, the higher the out-of-state tuition.
``` {r PhD}
data_frame(x = college_gam_terms$`lo(PhD)`$x,
           y = college_gam_terms$`lo(PhD)`$y,
           se.fit = college_gam_terms$`lo(PhD)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       x = "Percentage of faculty with PhDs",
       y = expression(f[1](PhD)))
```

The higher rates of faculty with PhD, the higher the out-of-state tuition in general. But note that there is a local maxima around 30%.
``` {r Private Q3}
data_frame(x = college_gam_terms$Private$x,
           y = college_gam_terms$Private$y,
           se.fit = college_gam_terms$Private$se.y) %>%
  unique %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit,
         x = factor(x, levels = c('No', 'Yes'), labels = c("Public", "Private"))) %>%
  ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "GAM of out-of-state tuition",
       x = NULL,
       y = expression(f[3](gender)))

```
Being private has a statistical significant and large effect on  increasing out-of-state tuition.

``` {r, model fit}
lin_mse <- mse(lin_college, data_split$test)
summary(lin_mse)
gam_mse <- mse(college_gam, data_split$test)
summary(gam_mse)
```
4.The MSE for the linear model is 3850000 while for the GAM is 3860000. There isn't difference. Going to the much more complicated GAM yields only 0.26% more MSE. This is likely due to the GAME being over fitted, using polynomials or less other less complicated components in the GAM than loess fits could likely improve the MSE by reducing the levels of over fitting.

5.The ANOVA test of GAM in 3-3 indicates that Expend is with a high likelihood non-linear.Lets look at the plot of Expend
``` {r}
data_frame(x = college_gam_terms$`lo(Expend)`$x,
           y = college_gam_terms$`lo(Expend)`$y,
           se.fit = college_gam_terms$`lo(Expend)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of out-of-state tuition",
       x = "Expenditure per student",
       y = expression(f[1](Expend)))
```

As in the plot the expenditure per student is non-linear and this is consistent with the results in Part 2.


