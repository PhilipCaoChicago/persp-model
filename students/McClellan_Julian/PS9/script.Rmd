---
title: "Problem Set 9 | MACS 301"
author: "Julian McClellan"
date: "Due 3/6/17"
output:
  html_document: default
  pdf_document: 
    latex_engine: lualatex
---
```{r setup, echo = FALSE, include = FALSE, message = FALSE}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(titanic)
library(rcfss)
library(pROC)
library(grid)
library(gridExtra)
library(FNN)
library(kknn)
library(purrr)
library(tree)
library(gbm)
library(randomForest)
library(caret)
library(ggdendro)
library(cowplot)

knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE
                      )
options(digits = 3)
theme_set(theme_minimal())
```

# Attitudes towards feminists

#### 1. Split the data into a training set (70%) and a validation set (30%). *Be sure to set your seed prior to this part of your code to guarantee reproducibility of results.*

We utilize 70% of the data as training data, and the remaining 30% as testing data.

```{r echo = TRUE}
fem_df <- read_csv('data/feminist.csv') 
  # mutate_each(funs(as.factor(.)), income, female, dem, rep)
  
set.seed(1234) # For reproducibility
fem_split <- resample_partition(fem_df, c(test = .3, train = .7))

fem_train <- fem_df[fem_split$train$idx, ]
fem_test <- fem_df[fem_split$test$idx, ]
```

***

#### 2. Calculate the test MSE for KNN models with K = 5, 10, 15, …, 100, using whatever combination of variables you see fit. Which model produces the lowest test MSE?

```{r}
calc_mse <- function(model, data, response = 'feminist'){
  if (any(class(model) %in% c('lm', 'tree','randomForest'))) {
    x <- modelr:::residuals(model, data)
    mse <- mean(x ^ 2, na.rm = TRUE)
  } else if (any(class(model) == 'knnReg')) {
                             # Need to access response vector directly.
    mse <- mean((model$pred - data$data[data$idx, ][[response]]) ^ 2)
    return(mse)
  } else if (any(class(model) == 'kknn')) {# Weighted KNN
    mse <- mean((model$fitted.values - data$data[data$idx, ][[response]]) ^ 2)
      # mean((model$fitted.values - data$data[data$idx, ][[response]]) ^ 2)
    return(mse)
  } else if(any(class(model) == 'gbm')) {
    mse <- mean((data$data[data$idx, ][[response]] - 
                     predict(model, newdata = data, n.trees = model$n.trees)) ^ 2)
    return(mse)
  }
}


data_frame(k_vals = seq(5, 100, by = 5),
           knn_models = map(k_vals, ~ 
                              knn.reg(as.data.frame(select(fem_train, -feminist)), 
                                      y = as.vector(as.data.frame(select(fem_train, feminist))), 
                                                    test = as.data.frame(select(fem_test, -feminist)),
                                                    k = .)
                            ),
           mse = map_dbl(knn_models, ~ mean((fem_test$feminist - .$pred) ^ 2))
) %>% 
{.} -> df

min_mse_k <- df$k_vals[which.min(df$mse)]
best_knn_model <- df$knn_models[[which.min(df$mse)]]

df %>%
  ggplot(aes(k_vals, mse)) +
    geom_line() +
    scale_x_continuous(breaks = append(c(25, 50, 75, 100), min_mse_k)) +
    geom_vline(aes(xintercept = min_mse_k), color = 'red', linetype = 'dashed') +
    annotate('text', x = 44, y = 500, label = sprintf('Minimum MSE at %d neighbors in KNN Regression', min_mse_k)) + 
     labs(title = 'Test MSE of KNN Regression on Feminist Warmth Score',
         subtitle = 'All Predictors Used | Values of k: 5, 10, 15, . . . 100',
         y = 'Test MSE',
         x = 'Number of Neighbors (k)',
         color = '')
```

***

#### 3. Calculate the test MSE for weighted KNN models with K = 5, 10, 15, …, 100 using the same combination of variables as before. Which model produces the lowest test MSE?

```{r}
data_frame(k_vals = seq(5, 100, by = 5),
           knn_models = map(k_vals, ~ 
                              kknn(feminist ~ ., train = fem_train, 
                                   test = fem_test, k = .)
                            ),
           mse = map_dbl(knn_models, ~ mean((fem_test$feminist - .$fitted.values) ^ 2))
) %>% 
{.} -> df

min_mse_k <- df$k_vals[which.min(df$mse)]
best_wknn_model <- df$knn_models[[which.min(df$mse)]]

df %>%
  ggplot(aes(k_vals, mse)) +
    geom_line() +
    labs(title = 'Test MSE of Weighted KNN Regression on Feminist Warmth Score',
         subtitle = 'All Predictors Used | Values of k: 5, 10, 15, . . . 100',
         y = 'Test MSE',
         x = 'Number of Neighbors (k)', 
         color = '') +
    scale_x_continuous(breaks = append(c(25, 50, 75, 100), min_mse_k)) +
    geom_vline(aes(xintercept = min_mse_k), color = 'red', linetype = 'dashed') +
    annotate('text', x = 60, y = 500, label = sprintf('Minimum MSE at %d neighbors in KNN Regression', min_mse_k)) +
    annotate('text', x = 61, y = 480, label = 'Test MSE appears to be monotonically decreasing with k')
```

***

#### 4. Compare the test MSE for the best KNN/wKNN model(s) to the test MSE for the equivalent linear regression, decision tree, boosting, and random forest methods using the same combination of variables as before. Which performs the best? Why do you think this method performed the best, given your knowledge of how it works?

```{r}
set.seed(1234) # For Random Forest Reproducibility
df <- data_frame(model = list('Best KNN (knn)' = best_knn_model, 'Best wKNN (kknn)' = best_wknn_model,
                              'Linear Regression (lm) ' = lm(feminist ~ ., data = fem_split$train),
                              'Decision Tree (tree)' = tree(feminist ~ ., data = fem_split$train),
                              'Boosting (2000 Trees)' = gbm(feminist ~ ., data = fem_split$train, distribution = 'gaussian',
                                                n.trees = 2000, interaction.depth = 2), 
                              'Random Forest (500 Trees)' = randomForest(feminist ~ ., data = fem_split$train,
                                                                         importance = TRUE, ntree = 500)),
                 mse = map_dbl(model, ~ calc_mse(., data = fem_split$test))
)

stats = list('min_mse' = min(df$mse), 'min_mse_model_name' = names(df$model[which.min(df$mse)]))
tdf <- data_frame(model = list('Best KNN' = best_knn_model, 'best wKNN' = best_wknn_model),
                  vals = c(5, 4))

df %>%
  ggplot(aes(names(model), mse)) +
    geom_col(aes(fill = names(model)), width = 1, show.legend = FALSE) +
    coord_flip() + 
    labs(title = 'Test MSE for Feminist Warmth Score for Various Methods (All Predictors)',
         subtitle = sprintf('Best Method: %s (%.3f Test MSE)', stats$min_mse_model_name, stats$min_mse),
         x = '',
         y = 'Test MSE',
      fill = 'Method') +
    theme(plot.title = element_text(hjust = 2))
```

I think that the boosting method, with 2000 trees worked, because the process of boosting iteratively works to reduce the size of residuals with each tree created. Additionally, I tried to choose a number of trees that was high enough to significantly reduce the residuals on the training set, but low enough to prevent over-fitting.

***
***

# Voter turnout and depression 

##### 1. Split the data into a training and test set (70/30).

```{r, echo = TRUE}
mhealth_df <- na.omit(read_csv('data/mental_health.csv'))
  # mutate_each(funs(as.factor(.)), income, female, dem, rep)
  
set.seed(1234) # For reproducibility
mhealth_split <- resample_partition(mhealth_df, c(test = .3, train = .7))

mhealth_train <- mhealth_df[mhealth_split$train$idx, ]
mhealth_test <- mhealth_df[mhealth_split$test$idx, ]
```

***

#### 2. Calculate the test error rate for KNN models with K = 1, 2, …, 10, using whatever combination of variables you see fit. Which model produces the lowest test MSE?

```{r}
data_frame(k_vals = seq(5, 100, by = 5),
           knn_classifiers = map(k_vals, ~ 
                              class::knn(as.data.frame(select(mhealth_train, -vote96)), 
                                      cl = as.data.frame(select(mhealth_train, vote96))$vote96, 
                                                    test = as.data.frame(select(mhealth_test, -vote96)),
                                                    k = .)
                            ),
           test_err = map_dbl(knn_classifiers, ~ mean(unlist(.) != mhealth_test$vote96, na.rm = TRUE))
) %>% 
{.} -> df

min_test_err_k <- df$k_vals[which.min(df$test_err)]
best_knn_classifier <- df$knn_classifiers[[which.min(df$test_err)]]

df %>%
  ggplot(aes(k_vals, test_err)) +
    geom_line() +
    geom_vline(aes(xintercept = min_test_err_k), color = 'red', linetype = 'dashed') +
    annotate('text', x = 45, y = .315, label = sprintf('Minimum Test Error Rate at %d neighbors in KNN Classification', min_test_err_k)) + 
    scale_x_continuous(breaks = append(c(25, 50, 75, 100), min_test_err_k)) +
    labs(title = 'Test Error of KNN Classification on Voting in 1996',
         subtitle = 'All Predictors Used | Values of k: 5, 10, 15, . . . 100',
         y = 'Test Error Rate',
         x = 'Number of Neighbors (k)') 
```

*** 

#### 3. Calculate the test error rate for weighted KNN models with K = 1, 2, …, 10 using the same combination of variables as before. Which model produces the lowest test error rate?

```{r}
data_frame(k_vals = seq(5, 100, by = 5),
           knn_classifiers = map(k_vals, 
                                 ~ kknn(vote96 ~ ., train = mutate(mhealth_train, vote96 = factor(vote96)),
                                   test = mutate(mhealth_test, vote96 = factor(vote96)),
                                   k = .)
                            ),
           test_err = map_dbl(knn_classifiers, ~ mean(mhealth_test$vote96 != .$fitted.values))
) %>% 
{.} -> df

min_test_err_k <- df$k_vals[which.min(df$test_err)]
best_wknn_classifier <- df$knn_classifiers[[which.min(df$test_err)]]

df %>%
  ggplot(aes(k_vals, test_err)) +
    geom_line() +
    geom_vline(aes(xintercept = min_test_err_k), color = 'red', linetype = 'dashed', show.legend = TRUE) +
    annotate('text', x = 45, y = .296, label = sprintf('Minimum Test Error Rate at %d neighbors in KNN Classification', min_test_err_k)) +     scale_x_continuous(breaks = append(c(25, 50, 75, 100), min_test_err_k)) +
    labs(title = 'Test Error of KNN Classification on Voting in 1996',
         subtitle = 'All Predictors Used | Values of k: 5, 10, 15, . . . 100',
         y = 'Test Error Rate',
         x = 'Number of Neighbors (k)',
         color = '')
```

***

#### 4. Compare the test error rate for the best KNN/wKNN model(s) to the test error rate for the equivalent logistic regression, decision tree, boosting, random forest, and SVM methods using the same combination of variables as before. Which performs the best? Why do you think this method performed the best, given your knowledge of how it works?

```{r}
calc_test_err <- function(model, data, response = 'vote96'){
  actual <- data$data[data$idx, ][[response]]
  
  if (class(model) == 'factor') { # For output of class::knn
    test_err <- mean(model != data$data[data$idx, ][[response]], na.rm = TRUE)
    
  } else if (class(model) == 'tree') {
    pred <- predict(model, data, type = 'class')
    test_err <- mean(pred != actual, na.rm = TRUE)
    
  } else if (any(class(model) == 'kknn')) {# Weighted KNN
    test_err <- 
      mean((as.numeric(levels(model$fitted.values))[model$fitted.values] - data$data[data$idx, ][[response]]) ^ 2)
      # mean((model$fitted.values - data$data[data$idx, ][[response]]) ^ 2)

  } else if (any(class(model) == 'gbm')) {
    # From tree methods notes
    test_err <- predict(model, newdata = as_tibble(data), type = 'response', n.trees = model$n.trees) %>%
      (function(x) round(x) != data$data[data$idx, ][[response]]) %>%
      mean()
    
  } else if (any(class(model) == 'randomForest')){
    pred_factor <- predict(model, data, type = 'class')
    pred <- as.numeric(levels(pred_factor))[pred_factor]

    test_err <- mean(pred != actual, na.rm = TRUE)
  } else if (all(class(model) == c('glm', 'lm'))){
    probs <- predict(model, data, type = 'response')
    pred <- ifelse(probs > .5, 1, 0)
    test_err <- mean(pred != actual, na.rm = TRUE)
  }
  if (exists('test_err')){
    return(test_err)
  } else {
    print(class(model))
  }
  
  
}

set.seed(1234) # For Random Forest Reproducibility
df <- data_frame(model = list('Best KNN (class::knn)' = best_knn_classifier, 'Best wKNN (kknn)' = best_wknn_classifier,
                              'Logistic Regression (glm) (.5 threshold) ' = glm(vote96 ~ ., data = mhealth_split$train, family = binomial),
                              'Decision Tree (tree)' = tree(factor(vote96) ~ ., data = mhealth_split$train),
                              'Boosting (2000 Trees)' = gbm(vote96 ~ ., data = mhealth_split$train, 
                                                            distribution = 'gaussian',
                                                n.trees = 2000, interaction.depth = 2), 
                              'Random Forest (500 Trees)' = randomForest(factor(vote96) ~ ., data = mhealth_split$train,
                                                                         importance = TRUE, ntree = 500))
                 ,
                 test_err = map_dbl(model, ~ calc_test_err(., data = mhealth_split$test))
)

stats = list('min_test_err' = min(df$test_err), 'min_test_err_model_name' = names(df$model[which.min(df$test_err)]))

df %>%
  ggplot(aes(names(model), test_err)) +
    geom_col(aes(fill = names(model)), width = 1, show.legend = FALSE) +
    coord_flip() + 
    labs(title = 'Test Error for Voting in 1996 for Various Classifiers (All Predictors)',
         subtitle = sprintf('Best Method: %s (%.3f Test Error)', stats$min_test_err_model_name, stats$min_test_err),
         x = '',
         y = 'Test Error',
      fill = 'Method') +
    theme(plot.title = element_text(hjust = 2))
```

As one can see from the graph above, the wKNN classifier performed the best. Why did it perform the best? Well, given a vector of dependent variables to predict from, wKNN weights the nearest neighbors in the training sample to make a prediction. Thus, it must have been the case that the closest neighbors for any given dependent variables were highly predictive of voting in 1996.

***

# Colleges.

```{r}
college_df <- read_csv('data/College.csv') %>%
  mutate(Private = ifelse(Private == 'Yes', 1, 0))

pr_out <- prcomp(college_df, scale = TRUE)

biplot(pr_out, scale = 0, cex = .6)
```

The bi-plot shows us that a lot of the Universities lie along the negative axes of the principal components.

In regards to interpreting the loadings of the first two principal components, this bi-plot looks a little hard to interpret, there are a lot of variables at play, so let's just extract the loadings of the first and second principal components.

```{r}
print('First Principal Component')
pr_out$rotation[, 1]

print('Second Principal Component')
pr_out$rotation[, 2]
```

Looking at the first principal component, the variables with the highest magnitude loadings are `PhD`, `Terminal`, `Top10perc`, `Top25perc`, `Outstate`, `Expend` and `Grad.Rate`. Thus, it seems that the percent of faculty with PhD's or with terminal degrees, percent of the student body in the top 25% or 10% of their high school class, the percent of the student body from out of state, the cost of the university, and the graduation rate of the university seem to move together, i.e. they are correlated.

Looking at the Second Principal Component, the variables with the highest magnitude loadings are `Private`, `Apps`, `Accept`, `Enroll`, `F.Undergrad`, and `P.Undergrad`. Thus, it seems that whether the university is private or not, the number of apps received, the number of new students accepted, the number of new students enrolled, the number of full-time undergraduates, and the percent of full-time undergraduates seem to move together, i.e. they are correlated.

***
***

# Clustering States.

#### 1. Perform PCA on the dataset and plot the observations on the first and second principal components.

```{r}
crime_df <- read_csv('data/USArrests.csv')

pr_out <- prcomp(x = select(crime_df, -State), scale = TRUE)

biplot(pr_out, scale = 0, cex = .6)
```

As was discussed in ISLR and in class, the first Principal Component roughly corresponds to level of violent crime, while the second roughly corresponds with Urban Population.

***

#### 2. Perform K-means clustering with K = 2. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.

```{r}
add_kmeans_clusters <- function(df, num_clusters, orig_data, on_orig = TRUE){
  set.seed(1234) # Cluster reproducibility 
  if (on_orig){
    orig_data <- select(orig_data, -State)
    cluster_ids <- factor(kmeans(orig_data, num_clusters)$cluster)
  } else { 
    cluster_ids <- factor(kmeans(select(df, -State), num_clusters)$cluster)
  }
  return(mutate(df, cluster_id = cluster_ids))
}


pca2_df <- select(as_data_frame(pr_out$x), PC1:PC2) %>%
  mutate(State = crime_df$State)
num_clusters <- 2

pca2_df %>% add_kmeans_clusters(num_clusters, crime_df) %>%
  ggplot(aes(PC1, PC2, color = cluster_id, label = State)) +
    geom_text() + 
    labs(title = sprintf('K-means clustering with K = %d on Original Data', num_clusters),
         color = 'Cluster ID')
```

The clustering, performed on the original variables, but plotted on the first 2 component vectors, shows a mostly clear grouping between 2 groups of States. Since the Clustering appears to be a split on the first component vector, in general we can say that the the 1st Cluster ID features states with differing (upon inspection, lower) rates of violent crimes `Rape`, `Murder`, and `Assault`, than those in the 2nd Cluster ID.

***

#### 3. Perform K-means clustering with K = 4. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.

```{r}
num_clusters <- 4

pca2_df %>% add_kmeans_clusters(num_clusters, crime_df) %>%
  ggplot(aes(PC1, PC2, color = cluster_id, label = State)) +
    geom_text() + 
    labs(title = sprintf('K-means clustering with K = %d on Original Data', num_clusters),
         color = 'Cluster ID')
```

This graph shows 4 roughly distinct clusters. Much like the previous graph, the differences between the clusters seems to be mostly on the first principal component, i.e. the rate of violent crime in the `Murder`, `Rape`, and `Assault` variables.

***

#### 4. Perform K-means clustering with K = 3. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.

```{r}
num_clusters <- 3

pca2_df %>% add_kmeans_clusters(num_clusters, crime_df) %>%
  ggplot(aes(PC1, PC2, color = cluster_id, label = State)) +
    geom_text() + 
    labs(title = sprintf('K-means clustering with K = %d on Original Data', num_clusters),
         color = 'Cluster ID')
```

In this graph, we can see 3 roughly distinct clusters, once again the clusters seem to be split on the first principal component.

***

#### 5. Perform K-means clustering with K = 3 on the first two principal components score vectors, rather than the raw data. Describe your results and compare them to the clustering results with K = 3 based on the raw data.

```{r}
num_clusters <- 3

pca2_df %>% add_kmeans_clusters(num_clusters, crime_df, FALSE) %>%
  ggplot(aes(PC1, PC2, color = cluster_id, label = State)) +
    geom_text() + 
    labs(title = sprintf('K-means clustering with K = %d on First 2 Principal Components', num_clusters),
         color = 'Cluster ID')
```

Refreshingly, this graph seems to represent 3 perfectly distinct clusters. This time, the clusters seem to be split on the 2nd as well as the 1st principal component. This graph seems to have the highest quality clusters, at least visually speaking, which makes sense, if we are clustering on Euclidean distance, and are clustering on the exact axes we are visualizing on (`PC1`, `PC2`), then the clustering should hold up better to visual inspection than the the previous graphs we have seen, which were clustered on the variables that form the loadings of `PC1` and `PC2`.

***

#### 6. Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}
state_names <- select(crime_df, State)$State
crime_dat <- as.matrix(select(crime_df, - State))
rownames(crime_dat) <- state_names

hc_complete <- hclust(dist(crime_dat), method = 'complete')

hc1 <- ggdendrogram(hc_complete, labels = TRUE) + 
  labs(title = '50 States Hierarchical Clustering',
       y = 'Euclidean Distance')

hc1
```

***

#### 7. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r}
cutree(hc_complete, k = 3) %>%
  data_frame(State = names(.), clust_id = .)
```

***

#### 8. Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation 1. What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

```{r}
hc_complete <- hclust(dist(scale(crime_dat)), method = 'complete')

hc2 <- ggdendrogram(hc_complete, labels = TRUE) + 
  labs(title = '50 States Hierarchical Clustering on Scaled Variables',
       y = 'Euclidean Distance')

hc2
hc1
```

Looking at the above two plots, scaling the variables has two noticeable effects. Firstly, the y-axis, the Euclidean distance from the complete linkage method, is much smaller with scaled variables. Secondly, some of the clusterings are different, Alaska merges with Mississippi and South Carolina without scaling the variables, but with Alabama, Louisiana, Georgia, Tennessee, North Carolina, Mississippi, and South Carolina when the variables are scaled. (Some clusterings stay the same though).

In my opinion, the variables *should* be scaled before inter-observation dissimilarities are computed. Unless the variables have the same standard deviation, those variables with larger and smaller standard deviations will have, respectively, exaggerated and diminished effects on the dissimilarity measure. For instance, if there are two variables, the first with a standard deviation of 1000, and the second with a standard deviation of 10, and under complete linkage the dissimilarity between a given two clusters is 200 with respect to the first variable, and 20, with respect to the second, in reality, the difference between the two clusters in terms of the first variable is actually quite small relative to the standard deviation of that variable, while the difference in terms of the second variable is quite large, twice the size of the standard deviation of that variable. However, without scaling, the dissimilarity contributed by the difference in the first variable will be much larger than that of the second, which does not reflect the reality of the closeness in the 1st variable, and the dissimilarity in the second variable! Under scaling, this issue would not occur, as dissimilarity is taken with respect to the standard deviation of each variable.

