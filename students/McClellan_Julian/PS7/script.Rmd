---
title: "Problem Set 7 | MACS 301"
author: "Julian McClellan"
date: "February 27, 2017"
output:
  html_document: default
  pdf_document:
    latex_engine: lualatex
---

```{r setup, message = FALSE, echo = FALSE}
library(ggplot2)
library(tidyverse)
library(broom)
library(modelr)
library(pROC)
library(stargazer)
library(modelr)
library(splines)
library(gam)
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      echo = FALSE)
df.biden = read.csv('data/biden.csv')
df.college = read.csv('data/College.csv')
```

# Part 1: Sexy Joe Biden 

### 1. Estimate the training MSE of the model using the traditional approach.

```{r biden_lm, results = 'asis'}
lm.biden.all <- lm(biden ~ ., data = df.biden)

# Based off of resampling class notes
calc_mse <- function(model, data){
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

mse.train_only <- calc_mse(lm.biden.all, df.biden)
stargazer(lm.biden.all, type = 'html')
```
The mean squared error of a linear model using all of the predictors, and calculated on all of the (training) data is `r mse.train_only`.

***

### 2. Estimate the test MSE of the model using the validation set approach.

```{r vset_approach}
set.seed(69)
biden_split <- resample_partition(df.biden, c(test = .3, train = .7))

lm.biden.all.train <- lm(biden ~ ., data = biden_split$train)

mse.test_only <- calc_mse(lm.biden.all.train, biden_split$test)
```
The mean squared error of a linear model using all of the predictors, and calculated on only the test set (30% of the data) is `r round(mse.test_only, 3)`. This value is `r round(mse.test_only - mse.train_only, 3)` (`r round(100 *(mse.test_only - mse.train_only) / mse.train_only, 2)`%) larger than the MSE calculated using only the training data. We expect the two MSE's to be different

***

### 3. Repeat the validation set approach 100 times, using 100 different splits of the observations into a training set and a validation set. Comment on the results obtained.

```{r vset_approach_100}
sim_biden_mse = function(n, df, method = 'vset', test = .3, train = .7, k = 10){
  set.seed(69)
  if(method == 'vset'){
    mses <- replicate(n, {
      df.split <-  resample_partition(df, c(train = train, test = test))
      df.lm <- lm(biden ~ . , data = df.split$train)
      mse <- calc_mse(df.lm, df.split$test)
      })
    } else if(method == 'kfold'){
      mses <- replicate(n, {
        df.kfolds <- crossv_kfold(df, k = k)
        models.kfolds <- map(df.kfolds$train, ~ lm(biden ~ ., data = .))
        mse.kfolds <- map2_dbl(models.kfolds, df.kfolds$test, calc_mse)
        mse <- mean(mse.kfolds)
      })
    } else if (method == 'loocv'){
      df.loocv <- crossv_kfold(df, k = nrow(na.omit(df)))
      models.loocv <- map(df.loocv$train, ~ lm(biden ~ ., data = .))
      mse.loocv <- map2_dbl(models.loocv, df.loocv$test, calc_mse)
      return(mean(mse.loocv))
    }
  return(tibble(mse = mses))
}

vset_mses <- sim_biden_mse(100, df.biden)

vset_mses %>%
  ggplot(aes(mse)) +
  geom_density() +
  labs(title = 'Test MSE density over 100 different 70/30 train/test splits of Biden Data') +
  geom_vline(aes(color = "Test MSE Mean", xintercept = mean(vset_mses$mse)), linetype = 'dashed') +
  geom_vline(aes(color = "All Data MSE", xintercept = mse.train_only)) +
  scale_color_manual('', breaks = c('Test MSE Mean', 'All Data MSE'), values = c('red', 'green')) + 
  theme(legend.position = 'bottom')
```

As we can see from the above density graph, thanks to the Central Limit Theorem (each Test MSE is independent!), the distribution of our Test MSE's is centered about the true mean of the distribution: the MSE for the whole dataset. Of course, since we have a distribution, there is some variability as to what the Test MSE actually is. If we were to select a model based on the test MSE of a certain validation set (1 train 1 test) split of the data, that model selected has a chance of differing from a model selected using the same procedure, but a different validation set split.

***

### 4. Estimate the test MSE of the model using the leave-one-out cross-validation (LOOCV) approach. Comment on the results obtained.

```{r loocv}
mse.loocv <-  sim_biden_mse(NA, df.biden, 'loocv')
```

The estimated test MSE of the model using the LOOCV approach is: `r mse.loocv`. This is only slightly above the training MSE of the model (`r mse.train_only`) we obtained when calculated against the entire dataset.

***

### 5. Estimate the test MSE of the model using the 10-fold cross-validation approach. Comment on the results obtained.

```{r 10fold}
mse.kfold <- sim_biden_mse(1, df.biden, 'kfold', k = 10)
```

The test MSE of the model using the 10-fold cross-validation approach is `r mse.kfold`. This is a bit higher than the full dataset mse of `r mse.train_only`.

***

### 6. Repeat the 10-fold cross-validation approach 100 times, using 100 different splits of the observations into 10-folds. Comment on the results obtained. 

```{r 10fold_100}
mse.kfold_100 <- sim_biden_mse(100, df.biden, 'kfold', k = 10)

mse.kfold_100 %>%
ggplot(aes(mse)) +
  geom_density() +
  labs(title = 'MSE density over 100 different splits of Biden Data into 10 folds') +
  geom_vline(aes(color = "Test MSE Mean", xintercept = mean(mse.kfold_100$mse)), linetype = 'dashed') +
  geom_vline(aes(color = "All Data MSE", xintercept = mse.train_only)) +
  scale_color_manual('', breaks = c('Test MSE Mean', 'All Data MSE'), values = c('red', 'green')) + 
  theme(legend.position = 'bottom')
```

As we can see from the density graph, the mean of 100 different test MSEs of the data split is much higher than the All Data MSE we calculated from step 1. This seems to better reflect the nature of how the MSE might be for *actual* out of sample test data.

***

### 7. Compare the estimated parameters and standard errors from the original model in step 1 (the model estimated using all of the available data) to parameters and standard errors estimated using the bootstrap (n = 1000).

```{r bootstrap_compare}
set.seed(69) # Need reproducible bootstrap
df.biden %>%
  modelr::bootstrap(1000) %>%
  mutate(model = map(strap, ~ lm(biden ~ ., data = .)),
         coef = map(model, tidy)) %>%
  dplyr::select(-strap) %>%
         {.} -> df.biden.boot

print('The bootstrap coefficients and standard errors:')
df.biden.boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(est.boot = mean(estimate),
            se.boot = sd(estimate, na.rm = TRUE))

coef(summary(lm.biden.all))
```

The bootstrap standard errors for `female`, `age`, `dem`, and `rep` are larger than the standard errors in the original model in step 1, while the bootstrap standard errors for `educ` and the intercept are smaller than the standard errors in the original model.  

In general, this confirms what we expect of the bootstrap standard errors, that they are larger (for the most part) than the non-bootstrap standard errors because they do not rely on distributional assumptions.

The parameters between the two models are essentially the same.

***
***

# 2. College (bivariate)

### (1) Simple Linear Model: `Outstate` on `Room.Board`

The first thing that we should check is whether or not a linear relationship is appropriate. Let's begin by simply plotting `Outstate` against `Room.Board`
```{r os_rb_scatter}
ggplot(df.college, aes(Room.Board, Outstate)) +
  geom_point() + 
  labs(title = 'Room.Board vs. Outstate')
```

Looking at this graph, a linear relationship, though not a perfect fit, seems to be appropriate to model the relationship between `Outstate` and `Room.Board`. Let's go ahead and look at the predicted values and residuals plot when a linear model is applied. If the residuals are not correlated with the fitted values, then a linear model is indeed appropriate.

```{r os_rb_predresid}
lm.college.rb = lm(Outstate ~ Room.Board, data = df.college)

df.college %>%
  add_predictions(lm.college.rb) %>%
  add_residuals(lm.college.rb) %>%
  ggplot(aes(pred, resid)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  labs(title = 'Linear model of Outstate regressed on Room.Board',
       x = 'Predicted Out of State Tuition', 
       y = 'Residuals')
```

Looking at the smooth fit line on this graph, it does indeed appear that the residuals are not correlated with the predicted values. Perhaps there seems to be a bit of a negative correlation towards the higher predicted out of state tuition, but the overall trend is clear.

***

However, it is still possible that higher degree polynomials might provide better results. Thus, we will use k-fold cross validation (`k = 10`) to determine the ideal polynomial.

```{r, os_rb_10fold}
set.seed(69) # Want reproducible results
df.college %>%
  crossv_kfold(k = 10) %>%
  {.} -> df.college.10fold

cv_error_fold10 <- vector("numeric", 5)
terms <- 1:5

for(i in terms){
  cv10_models <- map(df.college.10fold$train, ~ lm(Outstate ~ poly(Room.Board, i), data = .))
  cv10_mse <- map2_dbl(cv10_models, df.college.10fold$test, calc_mse)
  cv_error_fold10[[i]] <- mean(cv10_mse)
}

data_frame(terms = terms,
           fold10 = cv_error_fold10) %>%
  gather(method, MSE, fold10) %>%
  ggplot(aes(terms, MSE)) +
  geom_line() +
  labs(title = "MSE estimates for 10-fold cross validation",
       x = "Degree of Polynomial",
       y = "Mean Squared Error")
rm(df.college.10fold)
```

Looking at this graph of MSE, we see that a 2nd degree polynomialprovides the lowest MSE under 10-fold cross validation. Thus, we will create a 2nd degree polynomial linear model and report/graph the results.

```{r os_rb_results, results = 'asis'}
lm(Outstate ~ poly(Room.Board, 2), data = df.college) %>%
  {.} -> lm.college.rb.2p

lm.college.rb.2p %>%
  stargazer(type = 'html', title = 'Outstate on Room.Board | Linear 2nd degree polynomial')

df.college %>%
  ggplot(aes(Room.Board, Outstate)) +
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ poly(x, 2)) + 
  labs(title = 'Room.Board vs. Outstate with 2nd degree polynomial Regression')
```

Let's also do a final graph of the predicted out of state tuition versus the residuals.

```{r rs_ob_final}
df.college %>%
  add_predictions(lm.college.rb.2p) %>%
  add_residuals(lm.college.rb.2p) %>%
  ggplot(aes(pred, resid)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  labs(title = 'Linear model of Outstate regressed on Room.Board (2nd Degree Polynomial)',
       x = 'Predicted Out of State Tuition', 
       y = 'Residuals')
```

As desired, the residuals do not apear to be correlated with predicted out of state tuition.

***

### (2) Simple Linear Model: `Outstate` on `Grad.Rate`

The first thing that we should check is whether or not a linear relationship is appropriate. Let's begin by simply plotting `Outstate` against `Grad.Rate`
```{r os_gr_scatter}
ggplot(df.college, aes(Room.Board, Grad.Rate)) +
  geom_point() + 
  labs(title = 'Graduation Rate vs. Out of State Tuition')
```

Looking at this graph, a linear relationship, seems to be appropriate to model the relationship between `Outstate` and `Grad.Rate`. However, looking closely at the graph, we see a graduation rate of over 100%. This observation seems to be erroneous so we will have it removed in the proceeding work done to explore the bivariate relationship between `Outstate` and `Grad.Rate`.  

Let's go ahead and look at the predicted values and residuals plot when a linear model is applied. If the residuals are not correlated with the fitted values, then a linear model is indeed appropriate.

```{r os_gr_predresid}
lm.college.rb = lm(Outstate ~ Grad.Rate, data = df.college)

df.college %>%
  filter(Grad.Rate <= 100) %>%
  add_predictions(lm.college.rb) %>%
  add_residuals(lm.college.rb) %>%
  ggplot(aes(pred, resid)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  labs(title = 'Linear model of Outstate regressed on Grad.Rate',
       x = 'Predicted Out of State Tuition', 
       y = 'Residuals')
```

Looking at the smooth fit line on this graph, with the exception of the earlier predicted Out of State Tuition, it does indeed appear that the residuals are not correlated with the predicted values. However, in order to deal with the correlation between the residuals and predictd out of state tuition in for lower predicted of state tuition values, we should consider polynomial regression.

***

```{r, os_gr_10fold}
set.seed(69) # Want reproducible results
df.college %>%
  filter(Grad.Rate <= 100) %>%
  crossv_kfold(k = 10) %>%
  {.} -> df.college.10fold

cv_error_fold10 <- vector("numeric", 5)
terms <- 1:5

for(i in terms){
  cv10_models <- map(df.college.10fold$train, ~ lm(Outstate ~ poly(Grad.Rate, i), data = .))
  cv10_mse <- map2_dbl(cv10_models, df.college.10fold$test, calc_mse)
  cv_error_fold10[[i]] <- mean(cv10_mse)
}

data_frame(terms = terms,
           fold10 = cv_error_fold10) %>%
  gather(method, MSE, fold10) %>%
  ggplot(aes(terms, MSE)) +
  geom_line() +
  labs(title = "MSE estimates for 10-fold cross validation",
       x = "Degree of Polynomial",
       y = "Mean Squared Error")
```

Looking at this graph of MSE, we see that a 3rd degree polynomialprovides the lowest MSE under 10-fold cross validation. Thus, we will create a 3rd degree polynomial linear model and report/graph the results.

```{r os_gr_final, results = 'asis'}
df.college %>%
  filter(Grad.Rate <+ 100) %>%
  lm(Outstate ~ poly(Room.Board, 2), data = .) %>%
  {.} -> lm.college.gr.3p

lm.college.gr.3p %>%
  stargazer(type = 'html', title = 'Outstate on Room.Board | Linear 2nd degree polynomial')

df.college %>%
  filter(Grad.Rate <= 100) %>%
  ggplot(aes(Grad.Rate, Outstate)) +
    geom_point() +
    geom_smooth(method = 'lm', formula = y ~ poly(x, 3)) + 
    labs(title = 'Grad.Rate vs. Outstate with 3rd degree polynomial Regression')
```

Let's also do a final graph of the predicted out of state tuition versus the residuals.

```{r rs_gr_final}
df.college %>%
  filter(Grad.Rate <= 100) %>%
  add_predictions(lm.college.gr.3p) %>%
  add_residuals(lm.college.gr.3p) %>%
  ggplot(aes(pred, resid)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  labs(title = 'Linear model of Outstate regressed on Grad.Rate (3rd Degree Polynomial)',
       x = 'Predicted Out of State Tuition', 
       y = 'Residuals')
```

As desired, the residuals do not apear to be correlated with predicted out of state tuition.

***

### (3) Simple Linear Model: `Outstate` on `Expend`

The first thing that we should check is whether or not a linear relationship is appropriate. Let's begin by simply plotting `Outstate` against `Expend`
```{r os_ex_scatter}
ggplot(df.college, aes(Expend, Outstate)) +
  geom_point() + 
  labs(title = 'Expenditure vs. Out of State Tuition',
       y = 'Out of State Tuition',
       x = 'Instructional Expenditure per Student')
```

The relationship between `Outstate` and `Expend` does not appear to be linear, but rather logarithmic. Accordingly, we will regress `Outstate` on the logarithm of X.

```{r os_ex_log}
lm.college.ex.log <- lm(Outstate ~ log(Expend), data = df.college)

df.college %>%
  add_predictions(lm.college.ex.log) %>%
  add_residuals(lm.college.ex.log) %>%
  ggplot(aes(pred, resid)) +
    geom_point() +
    geom_smooth() +
    labs(title = 'Linear model of Outstate regressed on Expend',
         y = 'Residuals',
         x = 'Predicted Out of State Tuition')

ggplot(df.college, aes(Expend, Outstate)) + 
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ log(x)) +
  labs(title = 'Scatter Plot of Expend vs. Outstate',
       subtitle = 'With linear model of Outstate regressed on log(Expend)',
       y = 'Out of State tuition',
       x = 'Instructional Expenditure per Student')
```

How do we know that a linear model of `Outstate` regressed on `log(Expend)` is better than a linear model regressed on `Expend`? Let's use 10-fold cross validation to compare the log MSE vs the untransformed MSE with several polynomial degrees.

```{r os_ex_mse}
set.seed(69) # Want reproducible results
df.college %>%
  crossv_kfold(k = 10) %>%
  {.} -> df.college.10fold

cv_error_fold10 <- vector("numeric", 5)
terms <- 1:5

lms.college.ex.log.fold <- map(df.college.10fold$train, ~ lm(Outstate ~ log(Expend), 
                                                             data = .))
log_mse <- mean(map2_dbl(lms.college.ex.log.fold, df.college.10fold$test, calc_mse))

for(i in terms){
  cv10_models <- map(df.college.10fold$train, ~ lm(Outstate ~ poly(Expend, i), data = .))
  cv10_mse <- map2_dbl(cv10_models, df.college.10fold$test, calc_mse)
  cv_error_fold10[[i]] <- mean(cv10_mse)
}

data_frame(terms = terms,
           fold10 = cv_error_fold10) %>%
  gather(method, MSE, fold10) %>%
  ggplot(aes(terms, MSE)) +
    geom_line() +
    labs(title = "MSE estimates for 10-fold cross validation",
         x = "Degree of Polynomial",
         y = "Mean Squared Error") +
    geom_hline(aes(color = '10-fold CV Log(Expend) MSE', yintercept = log_mse)) + 
    scale_color_manual('', breaks = c('10-fold CV Log(Expend) MSE'), values = c('red')) +
    theme(legend.position = 'bottom')
```

As we can see form the above graph, the 10-fold MSE is actually lower for a third degree polynomial of `Expend` than it is for the `log(Expend)`. However, paying attention to the scale here, the difference is actually not all that large. Because interpretation is more tractable for a regression on `log(Expend)` we will elect to use that model. Let's report/graph the results then.

```{r os_ex_results, results = 'asis'}
stargazer(lm.college.ex.log, type = 'html', title = 'Outstate on Room.Board | Linear 2nd degree polynomial')

ggplot(df.college, aes(Expend, Outstate)) + 
  geom_point() +
  geom_smooth(method = 'lm', formula = y ~ log(x)) +
  labs(title = 'Scatter Plot of Expend vs. Outstate',
       subtitle = 'With linear model of Outstate regressed on log(Expend)',
       y = 'Out of State tuition',
       x = 'Instructional Expenditure per Student')
```

A note for the interpretation of the coefficient of `log(Expend)`. We can say that a 1% increase in Instructional Expenditure per student results in a `coefficient(Expend) / 100` increase in Out of State tuition. In this case, we can say that a 1% increase in `Expend` results in a `r coef(lm.college.ex.log)[2] / 100` increase in `Outstate` (Out of state tuition).

# Part 3. College (GAM)

### 1. Split the data into a training set and a test set.

```{r college_split, echo = TRUE}
set.seed(69)
college_split <- resample_partition(df.college, c(test = .3, train = .7))
```

### 2. Estimate an OLS model on the training data, using out-of-state tuition (Outstate) as the response variable and the other six variables as the predictors. Interpret the results and explain your findings, using appropriate techniques (tables, graphs, statistical tests, etc.).

```{r college_ols, results = 'asis'}
lm.college.train <- lm(Outstate ~ Private + Room.Board + PhD + perc.alumni + Expend + Grad.Rate, 
                       data = college_split$train)
stargazer(lm.college.train, title = 'OLS', type = 'html')
```

As we can see from the table, all 6 predictor variables are significant even at the $\alpha = .01$ level. Let's take a quick look at the residuals versus predicted values plot.

```{r college_ols_pred_resid_plot}
df.college[unlist(college_split$train["idx"], use.names = FALSE),] %>%
  add_predictions(lm.college.train) %>%
  add_residuals(lm.college.train) %>%
  ggplot(aes(pred, resid)) +
    geom_point() +
    geom_smooth()
```

For the most part, with the exception of the upper tail, the residuals seem uncorrelated with the predicted values, as desired.

***

### 3. Estimate a GAM on the training data, using out-of-state tuition (Outstate) as the response variable and the other six variables as the predictors. You can select any non-linear method (or linear) presented in the readings or in-class to fit each variable. Plot the results, and explain your findings. Interpret the results and explain your findings, using appropriate techniques (tables, graphs, statistical tests, etc.).

We will use a GAM model that regresses `Outstate` on the binary predictor Private (no alterations to this), a 2nd degree polynomial of `Room.Board` (opted for in part 2), local regressions for `PhD` and `perc.alumni`, the `log(Expend)` (from part 2 again), and a third degree polynomial for `Grad.Rate` (opted for in part 2).

```{r college_gam}
gam.college.train <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + lo(PhD) + lo(perc.alumni) + log(Expend) + poly(Grad.Rate, 3), 
                       data = college_split$train)
summary(gam.college.train)
```

All the predictors have statistically significant (at the `0` level) F-values for parametric effects. This is not too surprising, as our OLS model, in which the assumed a linear parametric effect showed these predictors to be statistically significant as well.

The predictor `PhD` has a statistically significant (at the $\alpha = .05$ level) nonparametric F-value. Meaning, it likely has a nonparametric effect on `Outstate`.

Now let's graph each effect each predictor has on the response `Outstate`.

#### Effect of `Private`

```{r pr_effect}
# Setup 
college.gam.terms <- preplot(gam.college.train, se = TRUE, rug = FALSE)

data_frame(x = college.gam.terms$Private$x,
           y = college.gam.terms$Private$y,
           se.fit = college.gam.terms$Private$se.y) %>%
  unique() %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit,
         x = factor(x, levels = c('No', 'Yes'), labels = c("Public", "Private"))) %>%
  ggplot(aes(x, y, ymin = y_low, ymax = y_high)) +
  geom_errorbar() +
  geom_point() +
  labs(title = "GAM of Out of State Tuition",
       x = '',
       y = expression(f[1](Private)))
```

We see the effect of `Private` the binary variable (0 = Public, 1 = Private) is quite distinct across its two values. The effect of a University being public clearly has a negative effect on the response `Outstate`, and the effect of a University being private clearly has a positive effect on `Outstate`, though to a lesser magnitude than a public university.

***

#### Effect of `Room.Board`

```{r rb_effect}
data_frame(x = college.gam.terms$`poly(Room.Board, 2)`$x,
           y = college.gam.terms$`poly(Room.Board, 2)`$y,
           se.fit = college.gam.terms$`poly(Room.Board, 2)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Out of State Tuition",
       subtitle = "2nd Degree Polynomial",
       x = "Room and Board Costs",
       y = expression(f[2](Room.Board)))
```

This graph shows that as `Room.Board` increases, `Outstate` consistently incresases.

#### Effect of  `PhD`

```{r phd_effect}
data_frame(x = college.gam.terms$`lo(PhD)`$x,
           y = college.gam.terms$`lo(PhD)`$y,
           se.fit = college.gam.terms$`lo(PhD)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Out of State Tuition",
       subtitle = "Local Regression",
       x = "% of Faculty with PhD",
       y = expression(f[3](PhD)))
```

The wide confidence intervals for values of `PhD` below 30% indicate that the effect that increases in `PhD` have on `Outstate` are hard to determine at those levels. However, the confidence interval decreases in size as `PhD` grows larger, eventually showing us that as `PhD` increases, `Outstate` does as well.

#### Effect of  `perc.alumni`

```{r pa_effect}
data_frame(x = college.gam.terms$`lo(perc.alumni)`$x,
           y = college.gam.terms$`lo(perc.alumni)`$y,
           se.fit = college.gam.terms$`lo(perc.alumni)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Out of State Tuition",
       subtitle = "Local Regression",
       x = "Percentage of Alumni who Donate",
       y = expression(f[4](perc.alumni)))
```

This graph clearly shows that as the percentage of alumni who donate (`perc.alumni`) increases, so does out of state tuition (`Outstate`).

#### Effect of `Expend`

```{r ex_effect}
data_frame(x = college.gam.terms$`log(Expend)`$x,
           y = college.gam.terms$`log(Expend)`$y,
           se.fit = college.gam.terms$`log(Expend)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Out of State Tuition",
       subtitle = "Log Transformed",
       x = "Instructional Expenditure per Student",
       y = expression(f[5](Expend)))
```

This graph shows that as `Expend` increases at first from its lower values, it increases `Outstate` by more than it does when it increases from its higher values.

#### Effect of `Grad.Rate`

```{r gr_effect}
data_frame(x = college.gam.terms$`poly(Grad.Rate, 3)`$x,
           y = college.gam.terms$`poly(Grad.Rate, 3)`$y,
           se.fit = college.gam.terms$`poly(Grad.Rate, 3)`$se.y) %>%
  mutate(y_low = y - 1.96 * se.fit,
         y_high = y + 1.96 * se.fit) %>%
  ggplot(aes(x, y)) +
  geom_line() +
  geom_line(aes(y = y_low), linetype = 2) +
  geom_line(aes(y = y_high), linetype = 2) +
  labs(title = "GAM of Out of State Tuition",
       subtitle = "3rd Degree Polynomial",
       x = "Graduation Rate (%)",
       y = expression(f[6](Grad.Rate)))
```

From the lower values of `Grad.Rate`, the effect is hard to determine. As `Grad.Rate` increases, past these values however, it starts to increase `Outstate` until the higher levels of `Grad.Rate` in which the confidence interval opens up again and increases `Grad.Rate` have weaker, harder to determine effects on `Outstate`.

***

### 4. Use the test set to evaluate the model fit of the estimated OLS and GAM models, and explain the results obtained.

```{r mse_compare}
college_test <- df.college[college_split$test$idx,]

ols_mse <- calc_mse(lm.college.train, college_test)

college_test %>%
  add_residuals(gam.college.train) %>%
  {.} -> df.gam.resids 

gam.resids <- df.gam.resids$resid
squared_resids <- gam.resids ^ 2

gam_mse <- mean(squared_resids)
```

Using the test set to calculate the MSE for both the OLS and the GAM models, we get MSEs of `r ols_mse` and `r gam_mse` respectively. So it seems that the the GAM model more accurately fits the test data.

### 5. For which variables, if any, is there evidence of a non-linear relationship with the response?

```{r nonlinear_evidence_prep}
gam.all <- gam.college.train # Base model

# Room and Board (1/5)
gam.no_rb <- gam(
  Outstate ~ Private + lo(PhD) + lo(perc.alumni) + log(Expend) + poly(Grad.Rate, 3), 
                       data = college_split$train)
gam.lin_rb <- gam(
  Outstate ~ Private + Room.Board + lo(PhD) + lo(perc.alumni) + log(Expend) + poly(Grad.Rate, 3), 
                       data = college_split$train)


# PhD (2/5)
gam.no_phd <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + lo(perc.alumni) + log(Expend) + poly(Grad.Rate, 3), 
                       data = college_split$train)
gam.lin_phd <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + PhD + lo(perc.alumni) + log(Expend) + poly(Grad.Rate, 3), 
                       data = college_split$train)

# Expenditure (3/5)
gam.no_ex<- gam(
  Outstate ~ Private + poly(Room.Board, 2) + lo(PhD) + lo(perc.alumni) + poly(Grad.Rate, 3), 
                       data = college_split$train)
gam.lin_ex <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + PhD + lo(perc.alumni) + Expend + poly(Grad.Rate, 3), 
                       data = college_split$train)

# Percent Alumni (4/5)
gam.no_pa <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + lo(PhD) + log(Expend) + poly(Grad.Rate, 3), 
                       data = college_split$train)
gam.lin_pa <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + lo(PhD) + perc.alumni + log(Expend) + poly(Grad.Rate, 3), 
                       data = college_split$train)

# Graduation Rate (5/5)
gam.no_gr <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + lo(PhD) + lo(perc.alumni) + log(Expend), 
                       data = college_split$train)
gam.lin_gr <- gam(
  Outstate ~ Private + poly(Room.Board, 2) + lo(PhD) + lo(perc.alumni) + log(Expend) + Grad.Rate, 
                       data = college_split$train)
```

Following 7.8.3 from ISLR we will be computing 5 different ANOVA tests, one for each variable except `Private` which is a qualitative binary variable. Each ANOVA will compare a GAM model utilizing the variable in question in a non-linear fashion, a GAM model utilizing the variable in a linear fashion, and a GAM model not utilizing the variable at all. If one of the models is significant under the ANOVA test, that is evidence that the variable is either not needed, needed with a linear funciton, or needed with a non-linear function.

#### Room and Board 
```{r rb_test}
anova(gam.no_rb, gam.lin_rb, gam.all)
```

Here we see that the 2nd model (linear `Room.Board`) is statistically significant at the '`0`' level and that the 3rd model (non-linear `Room.Board`) is not statistically significant. Thus we have evidence that `Room.Board` has a linear relationship with the reponse variable `Outstate`.

#### PhD
```{r phd_test}
anova(gam.no_phd, gam.lin_phd, gam.all)
```

Here we see that the 2nd model (linear `PhD`) is statistically significant at the '`0`' level and that the 3rd model (non-linear `PhD`) is statistically significant at the `.05` level. Thus we have evidence that `Room.Board` has a and non-linear linear relationship with the response variable: `Outstate`. The evidence for a linear relationship seems to be stronger, however.

#### Expenditure
```{r ex_test}
anova(gam.no_ex, gam.lin_ex, gam.all)
```

Here we see that the 2nd model (linear `Expend`) is not statistically significant that the 3rd model (non-linear `Expend`) is statistically significiant at the '`0`' level. Thus we have evidence that `Room.Board` has a non-linear relationship with the response variable: `Outstate`.

#### Percent Alumni
```{r pa_test}
anova(gam.no_pa, gam.lin_pa, gam.all)
```

Here we see that the 2nd model (linear `perc.alumni`) is statistically significant at the '`0`' level and that the 3rd model (non-linear `perc.alumni`) is not statistically significant. Thus we have evidence that `perc.alumni` has a linear relationship with the response variable: `Outstate`.

#### Graduation Rate
```{r gr_test}
anova(gam.no_gr, gam.lin_gr, gam.all)
```

Here we see that the 2nd model (linear `Grad.Rate`) is statistically significant at the '`0`' level and that the 3rd model (non-linear `Grad.Rate`) is not statistically significiant. Thus we have evidence that `Grad.Rate` has a linear relationship with the response variable: `Outstate`.