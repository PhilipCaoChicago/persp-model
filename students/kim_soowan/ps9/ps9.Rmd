---
title: 'Problem set #9: nonparametric methods and unsupervised learning'
author: "Soo Wan Kim"
date: "March 15, 2017"
output:
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)

library(tidyverse)
library(knitr)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(rcfss)
library(pROC)
library(grid)
library(gridExtra)
library(FNN)
library(kknn)
library(tree)
library(gbm)
library(randomForest)

set.seed(1234)
theme_set(theme_minimal())
options(na.action = na.omit)

#function to calculate MSE
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}
```

# Attitudes towards feminists [3 points]

  1. Split the data into a training and test set (70/30%).

```{r fem_split_set}
fem <- read.csv("data/feminist.csv")

fem_split <- resample_partition(fem, c(test = 0.3, train = 0.7)) #split data into 70/30 training/test set
fem_train <- fem_split$train %>% 
  tbl_df()
fem_test <- fem_split$test %>% 
  tbl_df()
```

  2. Calculate the test MSE for KNN models with $K = 5, 10, 15, \dots, 100$, using whatever combination of variables you see fit. Which model produces the lowest test MSE?

```{r fem_knn_mse}
# estimate test MSE for LM and KNN models

#LM model
mse_lm <- lm(feminist ~ ., data = fem_train) %>%
  mse(fem_test)

#KNN 1: feminist ~ .
mse_knn1 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ knn.reg(select(fem_train, -feminist), y = fem_train$feminist,
                         test = select(fem_test, -feminist), k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

#KNN 2: feminist ~ female + age + educ + income

mse_knn2 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ knn.reg(select(fem_train, -feminist, -dem, -rep), y = fem_train$feminist,
                         test = select(fem_test, -feminist, -dem, -rep), k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

#KNN 3: feminist ~ age + educ + income + dem

mse_knn3 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ knn.reg(select(fem_train, -feminist, -female, -rep), 
                                             y = fem_train$feminist,
                         test = select(fem_test, -feminist, -female, -rep), k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

#KNN 4: feminist ~ female + income + dem

mse_knn4 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ knn.reg(select(fem_train, female, income, dem), y = fem_train$feminist,
                         test = select(fem_test, female, income, dem), k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))

#plot test MSE
ggplot() +
  geom_line(data = mse_knn1, mapping = aes(k, mse), color="red") +
  geom_point(data = mse_knn1, mapping = aes(k, mse), color="red") +
  geom_line(data = mse_knn2, mapping = aes(k, mse), color="orange") +
  geom_point(data = mse_knn2, mapping = aes(k, mse), color="orange") +
  geom_line(data = mse_knn3, mapping = aes(k, mse), color="yellow") +
  geom_point(data = mse_knn3, mapping = aes(k, mse), color="yellow") +
  geom_line(data = mse_knn4, mapping = aes(k, mse), color="green") +
  geom_point(data = mse_knn4, mapping = aes(k, mse), color="green") +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  ylim(300,600) + 
  labs(title = "KNN for feminist",
       x = "K",
       y = "Test mean squared error",
       caption = "Red: feminist ~ .\n
       Orange: feminist ~ female + age + educ + income\n
       Yellow: feminist ~ feminist ~ age + educ + income + dem\n
       Green: feminist ~ female + income + dem") +
  expand_limits(y = 0) + 
  theme_gray()
```

The `feminist ~ female + income + dem` model produces the lowest test MSE. From these results it looks like gender, income, and party identification are the most important factors for predicting feminist warmth.

  3. Calculate the test MSE for weighted KNN models with $K = 5, 10, 15, \dots, 100$ using the same combination of variables as before. Which model produces the lowest test MSE?

```{r fem_wknn_mse}
# estimate test MSE for WKNN models

#WKNN 1: feminist ~ .
mse_wknn1 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ kknn(feminist ~ .,
                                          train = fem_train, test = fem_test, k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$fitted.values)^2)))

#WKNN 2: feminist ~ female + age + educ + income
mse_wknn2 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ kknn(feminist ~ female + age + educ + income,
                                          train = fem_train, test = fem_test, k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$fitted.values)^2)))

#WKNN 3: feminist ~ age + educ + income + dem

mse_wknn3 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ kknn(feminist ~ age + educ + income + dem,
                                          train = fem_train, test = fem_test, k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$fitted.values)^2)))

#WKNN 4: feminist ~ female + income + dem

mse_wknn4 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ kknn(feminist ~ female + income + dem,
                                          train = fem_train, test = fem_test, k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$fitted.values)^2)))
ggplot() +
  geom_line(data = mse_wknn1, mapping = aes(k, mse), color="red") +
  geom_point(data = mse_wknn1, mapping = aes(k, mse), color="red") +
  geom_line(data = mse_wknn2, mapping = aes(k, mse), color="orange") +
  geom_point(data = mse_wknn2, mapping = aes(k, mse), color="orange") +
  geom_line(data = mse_wknn3, mapping = aes(k, mse), color="yellow") +
  geom_point(data = mse_wknn3, mapping = aes(k, mse), color="yellow") +
  geom_line(data = mse_wknn4, mapping = aes(k, mse), color="green") +
  geom_point(data = mse_wknn4, mapping = aes(k, mse), color="green") +
  geom_hline(yintercept = mse_lm, linetype = 2) +
  ylim(400,700) + 
  labs(title = "Weighted KNN for feminist",
       x = "K",
       y = "Test mean squared error",
       caption = "Red: feminist ~ .\n
       Orange: feminist ~ female + age + educ + income\n
       Yellow: feminist ~ feminist ~ age + educ + income + dem\n
       Green: feminist ~ female + income + dem") +
  expand_limits(y = 0) + 
  theme_gray()
```

Using default weights in the `kknn` package, the `feminist ~ .` model performed the best, followed by the `feminist ~ female + income + dem` model.

  4. Compare the test MSE for the best KNN/wKNN model(s) to the test MSE for the equivalent linear regression, decision tree, boosting, and random forest methods using the same combination of variables as before. Which performs the best? Why do you think this method performed the best, given your knowledge of how it works?

First, I compare test MSEs across methods using all of the predictors (`feminist ~ .`).

```{r fem_mse_party_mod1}
#KNN
mse_knn1 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ knn.reg(select(fem_train, -feminist), y = fem_train$feminist,
                         test = select(fem_test, -feminist), k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))
#WKNN
mse_wknn1 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ kknn(feminist ~ .,
                                          train = fem_train, test = fem_test, k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$fitted.values)^2)))

#LM model
mse_lm <- lm(feminist ~ ., data = fem_train) %>%
  mse(fem_test)

#decision tree
mse_dt <- tree(feminist ~ ., data = fem_train) %>%
  mse(fem_test)

#boosting
#boosting with interaction depth = 4
fem_boost <- gbm(feminist ~ ., data = fem_train, 
                   distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
#get optimal number of trees with interaction depth 4
opt_iter <- gbm.perf(fem_boost, plot.it=FALSE)
#boosting with interaction depth = 4 and optimal number of trees for that depth
fem_boost_opt <- gbm(feminist ~ ., data = fem_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4)
#predicted boosting values
yhat_boost <- predict(fem_boost_opt, newdata = fem_test, n.trees = opt_iter[[1]])
mse_boost <- mean((yhat_boost - fem_test$feminist)^2)

#random forest
mse_rf <- randomForest(feminist ~ ., data = fem_train, ntree = 500) %>%
  mse(fem_test)

#plot test MSEs
ggplot() +
  geom_line(data = mse_knn1, mapping = aes(k, mse), color="red") +
  geom_point(data = mse_knn1, mapping = aes(k, mse), color="red") +
  geom_line(data = mse_wknn1, mapping = aes(k, mse), color="orange") +
  geom_point(data = mse_wknn1, mapping = aes(k, mse), color="orange") +
  geom_hline(yintercept = mse_lm, linetype = 1, color = "pink") +
  geom_hline(yintercept = mse_dt, linetype = 2, color = "green") +
  geom_hline(yintercept = mse_boost, linetype = 1, color = "gray") +
  geom_hline(yintercept = mse_rf, linetype = 1, color = "black") +
  ylim(420,460) + 
  labs(title = "Test MSEs using different methods",
       subtitle = "feminist ~ .",
       x = "K",
       y = "Test mean squared error",
       caption = "Red: KNN
       Orange: Weighted KNN\n
       Pink: Linear regression\n
       Green: Decision tree\n
       Gray: Boosting\n
       Black: Random Forest") +
  expand_limits(y = 0)

```

The boosting method performed the best, although random forest, decision tree, linear regression and boosting produced similar MSEs. Seeing as how the linear regression outperformed the decision tree and the random forest, the relationship between the predictors and `feminist` is probably very close to linear. Boosting just barely outperformed linear regression. Boosting yields small errors because it fits sequentially to the residuals of each new tree.

Now I compare test MSEs across methods using `feminist ~ female + income + dem`. 

```{r fem_mse_party_mod4}
#KNN
mse_knn4 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ knn.reg(select(fem_train, female, income, dem), y = fem_train$feminist,
                         test = select(fem_test, female, income, dem), k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$pred)^2)))
#WKNN
mse_wknn4 <- data_frame(k = c(seq(5, 100, by = 5)),
                      knn = map(k, ~ kknn(feminist ~ female + income + dem,
                                          train = fem_train, test = fem_test, k = .)),
                      mse = map_dbl(knn, ~ mean((fem_test$feminist - .$fitted.values)^2)))

#LM model
mse_lm <- lm(feminist ~ female + income + dem, data = fem_train) %>%
  mse(fem_test)

#decision tree
mse_dt <- tree(feminist ~ female + income + dem, data = fem_train) %>%
  mse(fem_test)

#boosting
#boosting with interaction depth = 4
fem_boost <- gbm(feminist ~ female + income + dem, data = fem_train, 
                   distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
#get optimal number of trees with interaction depth 4
opt_iter <- gbm.perf(fem_boost, plot.it=FALSE)
#boosting with interaction depth = 4 and optimal number of trees for that depth
fem_boost_opt <- gbm(feminist ~ female + income + dem, data = fem_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4)
#predicted boosting values
yhat_boost <- predict(fem_boost_opt, newdata = fem_test, n.trees = opt_iter[[1]])
mse_boost <- mean((yhat_boost - fem_test$feminist)^2)

#random forest
mse_rf <- randomForest(feminist ~ female + income + dem, data = fem_train, ntree = 500) %>%
  mse(fem_test)

#plot test MSEs
ggplot() +
  geom_line(data = mse_knn4, mapping = aes(k, mse), color="red") +
  geom_point(data = mse_knn4, mapping = aes(k, mse), color="red") +
  geom_line(data = mse_wknn4, mapping = aes(k, mse), color="orange") +
  geom_point(data = mse_wknn4, mapping = aes(k, mse), color="orange") +
  geom_hline(yintercept = mse_lm, linetype = 1, color = "pink") +
  geom_hline(yintercept = mse_dt, linetype = 2, color = "green") +
  geom_hline(yintercept = mse_boost, linetype = 1, color = "gray") +
  geom_hline(yintercept = mse_rf, linetype = 1, color = "black") +
  ylim(430,460) + 
  labs(title = "Test MSEs using different methods",
       subtitle = "feminist ~ female + income + dem",
       x = "K",
       y = "Test mean squared error",
       caption = "Red: KNN
       Orange: Weighted KNN\n
       Pink: Linear regression\n
       Green: Decision tree\n
       Gray: Boosting\n
       Black: Random Forest") +
  expand_limits(y = 0)

```

For this set of variables random forest performed the best, though linear regression, boosting, and random forest performed similarly. Again, the relationship is probably quite close to linear, but not perfectly so. Boosting and random forest generally produce similar MSEs because they reduce bias by producing multiple decision trees. Random forest reduces bias by sampling with replacement many times, reducing sampling bias.

Now we compare the best models: boosting with all predictors and random forest with `female`, `income`, and `dem`.

```{r fem_mse_party_best}
#boosting
#boosting with interaction depth = 4
fem_boost <- gbm(feminist ~ ., data = fem_train, 
                   distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
#get optimal number of trees with interaction depth 4
opt_iter <- gbm.perf(fem_boost, plot.it=FALSE)
#boosting with interaction depth = 4 and optimal number of trees for that depth
fem_boost_opt <- gbm(feminist ~ ., data = fem_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4)
#predicted boosting values
yhat_boost <- predict(fem_boost_opt, newdata = fem_test, n.trees = opt_iter[[1]])
mse_boost <- mean((yhat_boost - fem_test$feminist)^2)

#random forest
mse_rf <- randomForest(feminist ~ female + income + dem, data = fem_train, ntree = 500) %>%
  mse(fem_test)

#plot test MSEs
ggplot() +
  geom_hline(yintercept = mse_boost, linetype = 1, color = "red") +
  geom_hline(yintercept = mse_rf, linetype = 1, color = "blue") +
  ylim(430,440) + 
  labs(title = "Test MSEs using different methods",
       subtitle = "feminist ~ female + income + dem",
       x = "K",
       y = "Test mean squared error",
       caption = "Red: feminist ~ ., Boosting\n
       Blue: feminist ~ female + income + dem, Random Forest") +
  expand_limits(y = 0)
```

Interestingly, the more parsimonious model outperformed the model with all predictors. However, the test MSEs are very very similar. 

# Voter turnout and depression [2 points]

  1. Split the data into a training and test set (70/30).

```{r mh_split}
mh <- read.csv("data/mental_health.csv") %>%
  na.omit()
mh$vote96 <- as.factor(mh$vote96)

mh_split <- resample_partition(mh, c(test = 0.3, train = 0.7)) #split data into 70/30 training/test set
mh_train <- mh_split$train %>% 
  tbl_df()
mh_test <- mh_split$test %>% 
  tbl_df()
```

  2. Calculate the test error rate for KNN models with $K = 1,2,\dots,10$, using whatever combination of variables you see fit. Which model produces the lowest test MSE?

```{r mh_knn_terror}
er_knn <- data_frame(k = 1:10,
                      knn_train = map(k, ~ class::knn(select(mh_train, -vote96),
                                                test = select(mh_train, -vote96),
                                                cl = mh_train$vote96, k = .)),
                      knn_test = map(k, ~ class::knn(select(mh_train, -vote96),
                                                test = select(mh_test, -vote96),
                                                cl = mh_train$vote96, k = .)),
                      mse_train = map_dbl(knn_train, ~ mean(mh_test$vote96 != .)),
                      mse_test = map_dbl(knn_test, ~ mean(mh_test$vote96 != .)))

#plot test error rate
ggplot() +
  geom_line(data = er_knn, mapping = aes(k, mse_test)) +
  labs(x = "K",
       y = "Test error rate") +
  expand_limits(y = 0) +  
  scale_x_continuous(breaks = seq(1,10,1)) + 
  ylim(.25,0.4)

min_err_knn <- min(er_knn$mse_test)
```

Using all predictors, the test error rate is minimized at K = 9 and is `r round(min_err_knn, 2)`.

  3. Calculate the test error rate for weighted KNN models with $K = 1,2,\dots,10$ using the same combination of variables as before. Which model produces the lowest test error rate?

```{r mh_wknn_mse}
er_wknn <- data_frame(k = 1:10,
                     knn = map(k, ~ kknn(vote96 ~ .,
                                         train=mh_train,
                                         test=mh_test, k =.)),
                     err = map_dbl(knn, ~ mean(mh_test$vote96 != .$fitted.values)))

ggplot(er_wknn, aes(k, err)) +
  geom_line() +
  geom_point() +
  labs(x = "K",
       y = "Test error rate") + 
  scale_x_continuous(breaks = seq(1,10,1), labels = seq(1,10,1))

```

The test error rate is minimized at K = 9.

  4. Compare the test error rate for the best KNN/wKNN model(s) to the test error rate for the equivalent logistic regression, decision tree, boosting, random forest, and SVM methods using the same combination of variables as before. Which performs the best? Why do you think this method performed the best, given your knowledge of how it works?

```{r}
#logistic

#
logis_2 <- glm(vote96 ~ ., data=df_mentalF_tr, family=binomial)

logistic_2 <- df_mentalF_te %>%
  add_predictions(logis_2) %>%
  mutate(prob = exp(pred) / (1 + exp(pred))) %>%
  mutate(pred_bi = as.numeric(prob > .5))

err_logistic2 <- mean(df_mentalF_te$vote96 != logistic_2$pred_bi)

#--Decision tree
tree_2 <- tree(vote96 ~ ., data=df_mentalF_tr)

#test error rate
mh_model_accuracy <- mh_rf_test %>%
  as_tibble() %>%
  add_predictions(mh_rf1) %>% #model 1
  mutate(pred1 = as.numeric(pred > .5)) %>%
  rename(prob1 = pred) %>%
  add_predictions(mh_rf2) %>% #model 2
  mutate(pred2 = as.numeric(pred > .5)) %>%
  rename(prob2 = pred) %>%
  add_predictions(mh_rf3) %>% #model 3
  mutate(pred3 = as.numeric(pred > .5)) %>%
  rename(prob3 = pred) %>%
  add_predictions(mh_rf4) %>%
  mutate(pred4 = as.numeric(pred > .5)) %>%
  rename(prob4 = pred) %>%
  add_predictions(mh_rf5) %>%
  mutate(pred5 = as.numeric(pred > .5)) %>%
  rename(prob5 = pred)

mh_model_acc1 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred1, na.rm = TRUE)
mh_model_acc2 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred2, na.rm = TRUE)
mh_model_acc3 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred3, na.rm = TRUE)
mh_model_acc4 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred4, na.rm = TRUE)
mh_model_acc5 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred5, na.rm = TRUE)

acc_list <- as.data.frame(c(mh_model_acc1,
              mh_model_acc2,
              mh_model_acc3,
              mh_model_acc4,
              mh_model_acc5))
```


# Colleges [2 points]

Perform PCA analysis on the college dataset and plot the first two principal components. Describe the results. What variables appear strongly correlated on the first principal component? What about the second principal component?

```{r college_pca}
college <- read.csv("data/college.csv")
college$Private <- as.character(college$Private)
college$Private[college$Private == "Yes"] <- 1
college$Private[college$Private == "No"] <- 0
college$Private <- as.numeric(college$Private)

pca_col <- prcomp(college, scale = TRUE)
biplot(pca_col, scale = 0, cex = .6)
```

The plot is difficult to read due to the number of variables, but `Apps`, `F.Undergrad`, `P.Undergrad`, and `S.F.Ratio` appear to cluster on the first principal component. These are all correlated with the size of the student body. On the second prinicipal component, `Top10perc`, `Top25perc`, `Room.Board`, `Books`, `PhD`, `Terminal`, `Outstate`, and `perc.alumni` appear to cluster together. It seems like higher values on these factors would indicate more prestigious schools.

# Clustering states [3 points]1

  1. Perform PCA on the dataset and plot the observations on the first and second principal components.

Using the code provided in the notes leads to the error message: `Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric`. I create a new column identifying the state by number and drop the `State` variable in the PCA analysis.

```{r states_pca}
usa <- read.csv("data/USArrests.csv")

pca_usa <- prcomp(select(usa, -State), scale = TRUE)
biplot(pca_usa, scale = 0, cex = .6, xlabs=usa$State)
```

  2. Perform $K$-means clustering with $K=2$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.
  
```{r}

usa_kmean2<-usar %>%
  mutate(k2 = kmeans(usar[, 2:5], 2, nstart = 1)[[1]])

# But finally I found the autoplot is the much easier way to do this job. 
autoplot(kmeans(usar[, 2:5], 2), data = usar) +
  geom_text(vjust=-1, label=usar$State, size = 1.8) +
  labs(title = 'K-means Clustering over PCA',
       subtitle = 'k=2')

k2 <- kmeans(usa_filt, centers=2, nstart=1)

ggplot(data = pca_usa, mapping = aes()
  mapping=aes(x=pca_4$x[,1], y=pca_4$x[,2], label=df_arrest$State, color=factor(kmean_4_2$cluster))) +
  geom_point() +
  geom_text(hjust=0, vjust=0) +
  scale_color_discrete(guide=FALSE) +
  labs(title = "State clustering regarding crime statistics",
       x = "PC 1 (safety)",
       y = "PC 2 (rural-inclined)")

```


  3. Perform $K$-means clustering with $K=4$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.

```{r}

```


  4. Perform $K$-means clustering with $K=3$. Plot the observations on the first and second principal components and color-code each state based on their cluster membership. Describe your results.

  5. Perform $K$-means clustering with $K=3$ on the first two principal components score vectors, rather than the raw data. Describe your results and compare them to the clustering results with $K=3$ based on the raw data.

```{r}

```


  6. Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}

```


  7. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r}

```

  8. Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation $1$. What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

```{r}

```


```{r session_info, include=FALSE}
devtools::session_info()
```
