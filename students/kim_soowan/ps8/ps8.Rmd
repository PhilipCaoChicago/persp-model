---
title: "Problem set #8: tree-based methods and support vector machines"
author: "Soo Wan Kim"
date: "March 4, 2017"
output:
  github_document:
    toc: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)

library(tidyverse)
library(knitr)
library(modelr)
library(tree)
library(randomForest)
library(ggdendro)
library(forcats)
library(gbm)
library(pROC)

set.seed(1234)
theme_set(theme_minimal())
options(na.action = na.fail)

#function to calculate MSE
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}
```

# Part 1: Sexy Joe Biden (redux times two) [3 points]

###Subpart 1
**Split the data into a training set (70%) and a validation set (30%).**

```{r biden_split}
biden <- read.csv("data/biden.csv") #import data

biden_split <- resample_partition(biden, c(test = 0.3, train = 0.7)) #split data into 70/30 training/test set
biden_train <- biden_split$train %>% 
  tbl_df()
biden_test <- biden_split$test %>% 
  tbl_df()
```

###Subpart 2
**Fit a decision tree to the training data, with `biden` as the response variable and the other variables as predictors. Plot the tree and interpret the results. What is the test MSE? Leave the control options for `tree()` at their default values.**

```{r biden_tree_default}
biden_tree <- tree(biden ~ ., data = biden_train) #fit tree
mod <- biden_tree

# plot tree
tree_data <- dendro_data(mod)

ggplot(segment(tree_data)) +
  labs(title = "Decision tree of Biden warmth",
       subtitle = "Default control options") + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

mse_default <- mse(mod, biden_test)
```

The plot includes only two variables, `dem` and `rep`, suggesting that party identification is the most important factor in this model. The plot shows that Democrats are more likely to have higher Biden warmth than non-Democrats, and independents are more likely to have higher Biden warmth than Republicans. The test MSE is `r mse_default`. 

###Subpart 3
**Now fit another tree to the training data with the following `control` options:**
  
  `tree(control = tree.control(nobs = # number of rows in the training set,
                              mindev = 0))`
                          
**Use cross-validation to determine the optimal level of tree complexity, plot the optimal tree, and interpret the results. Does pruning the tree improve the test MSE?**

```{r biden_tree_custom_cv_error}
biden_tree2 <- tree(biden ~ ., data = biden_train, #fit another tree
                    control = tree.control(nobs = nrow(biden_train),
                            mindev = 0))

# generate 10-fold CV trees
biden_cv <- crossv_kfold(biden, k = 10) %>%
  mutate(tree = map(train, ~ tree(biden ~ ., data = .,
     control = tree.control(nobs = nrow(biden),
                            mindev = 0))))

# calculate each possible prune result for each fold
biden_cv <- expand.grid(biden_cv$.id, 2:10) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(biden_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         mse = map2_dbl(prune, test, mse))

#display test MSE for each number of terminal nodes
biden_cv %>%
  select(k, mse) %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(title = "MSE at each number of terminal nodes",
       subtitle = "Customized control options",
       x = "Number of terminal nodes",
       y = "MSE")
```

The optimal number of terminal nodes is 3.
    
```{r biden_tree_custom_opt}
#function to calculate MSE
mse <- function(model, data) {
  x <- modelr:::residuals(model, data)
  mean(x ^ 2, na.rm = TRUE)
}

biden_tree2 <- tree(biden ~ ., data = biden_train, 
                    control = tree.control(nobs = nrow(biden_train),
                            mindev = 0))

mod <- prune.tree(biden_tree2, best = 3)

# plot tree
tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  labs(title = "Decision tree of biden warmth",
       subtitle = "Updated control options, 3 terminal nodes") + 
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

mse_whole <- mse(biden_tree2, biden_test)
mse_pruned <- mse(mod, biden_test)
```

This tree is identical to the one produced in subpart 2. Without pruning, the test MSE for this model is `r mse_whole`. Pruning reduces it to `r mse_pruned`, showing that pruning improves model fit.

###Subpart 4
**Use the bagging approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results.**
  
```{r biden_bagging}
biden_rf_data <- biden %>% #prep data for random forest method
    mutate_each(funs(as.factor(.)), female, dem, rep) %>%
    na.omit

biden_rf_split <- resample_partition(biden_rf_data, c(test = 0.3, train = 0.7)) #split data into 70/30 training/test set
biden_rf_train <- biden_rf_split$train %>% 
  tbl_df()
biden_rf_test <- biden_rf_split$test %>% 
  tbl_df()

(biden_bag <- randomForest(biden ~ ., data = biden_rf_train, #bagging
                             mtry = 5, ntree = 500))

mse_bagged <- mse(biden_bag, biden_rf_test)

data_frame(var = rownames(importance(biden_bag)),
           MeanDecreaseGini = importance(biden_bag)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting Biden warmth",
       subtitle = "Bagging",
       x = NULL,
       y = "Average decrease in the Gini Index")
```

Using the bagging approach, age appears to be the most influential factor, followed by being a Democrat and then by education. The test MSE for the bagging approach is `r mse_bagged`.

###Subpart 5
**Use the random forest approach to analyze this data. What test MSE do you obtain? Obtain variable importance measures and interpret the results. Describe the effect of $m$, the number of variables considered at each split, on the error rate obtained.**

```{r biden_rf}
(biden_rf <- randomForest(biden ~ ., data = biden_rf_train, #fit Random Forest
                            ntree = 500))

mse_rf <- mse(biden_rf, biden_rf_test)

data_frame(var = rownames(importance(biden_rf)),
           MeanDecreaseGini = importance(biden_rf)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting Biden warmth",
       subtitle = "Random Forest",
       x = NULL,
       y = "Average decrease in the Gini Index")
```

Using the random forest approach, party identification is the most important factor, with other factors having much smaller effects. The test MSE is `r mse_rf`, showing that the random forest approach improves on the bagging approach. Decreasing $m$ reduces the error rate by reducing bias in model estimation. 
 
###Subpart 6
**Use the boosting approach to analyze the data. What test MSE do you obtain? How does the value of the shrinkage parameter $\lambda$ influence the test MSE?**

First, we calculate the optimal number of iterations when depth = 4:
```{r biden_boost_opt_iter}
biden_boost <- gbm(biden ~ ., data = biden_train, 
                   distribution = "gaussian", n.trees = 5000, interaction.depth = 4)

opt_iter <- gbm.perf(biden_boost, plot.it=FALSE)
opt_iter
```

Then we fit the boosting model with the optimal number of trees.
```{r biden_boost_opt_mod}
biden_boost_opt <- gbm(biden ~ ., data = biden_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4)

summary(biden_boost_opt)

yhat_boost <- predict(biden_boost_opt, newdata = biden_test, n.trees = opt_iter[[1]])
mse_boost <- mean((yhat_boost - biden_test$biden)^2)
```

Party identification is the most influential factor, followed by age. 

Partial dependence plots for `dem`, `rep`, `age`, and `educ`:
```{r biden_boost_partial_dependence}
par(mfrow =c(2,2))
plot(biden_boost,i="dem")
plot(biden_boost,i="rep")
plot(biden_boost,i="age")
plot(biden_boost,i="educ")
```

Expected Biden warmth increases with Democratic party identification and decreases with Republican party identification. There is an overall upward trend as age increases, and an overall downward trend as education increases.

The test MSE is `r mse_boost`.

Increasing the shrinkage parameter $\lambda$ increases the MSE, as shown below:
```{r biden_boost_lambda}
#fit different models with varying lambda values
biden_boost1 <- gbm(biden ~ ., data = biden_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4,
                   shrinkage = 0.001, verbose = F)

biden_boost2 <- gbm(biden ~ ., data = biden_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4,
                   shrinkage = 0.005, verbose = F)
                   
biden_boost3 <- gbm(biden ~ ., data = biden_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4,
                   shrinkage = 0.02, verbose = F)

biden_boost4 <- gbm(biden ~ ., data = biden_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4,
                   shrinkage = 0.03, verbose = F)

biden_boost5 <- gbm(biden ~ ., data = biden_train, 
                   distribution = "gaussian", n.trees = opt_iter[[1]], interaction.depth = 4,
                   shrinkage = 0.1, verbose = F)

boost_models <- c(biden_boost1, biden_boost2, biden_boost3, biden_boost4, biden_boost5)

#calculate MSEs for each
mse_booster <- function(model) {
  yhat_boost <- predict(model, newdata = biden_test, n.trees = opt_iter[[1]])
  mse <- mean((yhat_boost - biden_test$biden)^2)
}

boost_mse_vals <- as.data.frame(c(mse_booster(biden_boost1), mse_booster(biden_boost2), 
                    mse_booster(biden_boost3), mse_booster(biden_boost4), mse_booster(biden_boost5)))

#report in graph
boost_lambda_vals <- as.data.frame(c(0.001, 0.005, 0.02, 0.03, 0.1))
boost_df <- cbind(boost_lambda_vals, boost_mse_vals)
colnames(boost_df) <- c("Lambda", "MSE")

ggplot(data = boost_df, mapping = aes(x = Lambda, y = MSE)) + 
  geom_point() + 
  geom_line() + 
  labs(title = "Test MSEs for different shrinkage parameters, depth = 4",
       subtitle = "Using boosting to predict Biden warmth",
       x = "Lambda",
       y = "Test MSE")
```

###MSE by Method Comparison Chart

```{r biden_mse_comparison}
#make chart of mse values by method 
subpart_list <- as.data.frame(c(2, 3, 3, 4, 5, 6))
desc_list <- as.data.frame(c("Single tree, default control options", 
                             "Single tree, not pruned", 
                             "Single tree, pruned (3 leaves)", 
                             "Bagging", "Random forest", "Boosting"))
mse_list <- as.data.frame(c(mse_default, mse_whole, 
                            mse_pruned, mse_bagged, mse_rf, mse_boost))

mse_df <- cbind(subpart_list, desc_list)
mse_df <- cbind(mse_df, mse_list)
colnames(mse_df) <- c("Subpart", "Method", "MSE")

kable(mse_df, format = "html")
```

# Part 2: Modeling voter turnout [3 points]

###Subpart 1
**Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five tree-based models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)**

For this part of the problem I will estimate five random forest models:  

  1. vote96 ~ mhealth_sum
  2. vote96 ~ mhealth_sum + I(mhealth_sum^2)
  3. vote96 ~ mhealth_sum + age
  4. vote96 ~ mhealth_sum + inc10
  5. vote96 ~ mhealth_sum + educ + inc10

I use the validation set approach (70/30 training-test split) to train and test each model. 

```{r mh_setup}
mh <- read.csv("data/mental_health.csv")

#define regression models
model1 <- vote96 ~ mhealth_sum
model2 <- vote96 ~ mhealth_sum + I(mhealth_sum^2)
model3 <- vote96 ~ mhealth_sum + age
model4 <- vote96 ~ mhealth_sum + inc10
model5 <- vote96 ~ mhealth_sum + educ + inc10

mh_rf_data <- mh %>% #prep data for random forest method
  select(-black, -female, -married) %>%
  na.omit

mh_rf_split <- resample_partition(mh_rf_data, c(test = 0.3, train = 0.7)) #split data into 70/30 training/test set
mh_rf_train <- mh_rf_split$train %>% 
  tbl_df()
mh_rf_test <- mh_rf_split$test %>% 
  tbl_df()

#estimate random forest models
mh_rf1 <- randomForest(model1, data = mh_rf_train, ntree = 500)
mh_rf2 <- randomForest(model2, data = mh_rf_train, ntree = 500)
mh_rf3 <- randomForest(model3, data = mh_rf_train, ntree = 500)
mh_rf4 <- randomForest(model4, data = mh_rf_train, ntree = 500)
mh_rf5 <- randomForest(model5, data = mh_rf_train, ntree = 500)
```


```{r mh_rf_validation}
#Test each model and report the results

#Test MSEs
mse_list <- as.data.frame(c(mse(mh_rf1, mh_rf_test), 
                            mse(mh_rf2, mh_rf_test), 
                            mse(mh_rf3, mh_rf_test), 
                            mse(mh_rf4, mh_rf_test), 
                            mse(mh_rf5, mh_rf_test)))

#test error rate
mh_model_accuracy <- mh_rf_test %>%
  as_tibble() %>%
  add_predictions(mh_rf1) %>% #model 1
  mutate(pred1 = as.numeric(pred > .5)) %>%
  rename(prob1 = pred) %>%
  add_predictions(mh_rf2) %>% #model 2
  mutate(pred2 = as.numeric(pred > .5)) %>%
  rename(prob2 = pred) %>%
  add_predictions(mh_rf3) %>% #model 3
  mutate(pred3 = as.numeric(pred > .5)) %>%
  rename(prob3 = pred) %>%
  add_predictions(mh_rf4) %>%
  mutate(pred4 = as.numeric(pred > .5)) %>%
  rename(prob4 = pred) %>%
  add_predictions(mh_rf5) %>%
  mutate(pred5 = as.numeric(pred > .5)) %>%
  rename(prob5 = pred)

mh_model_acc1 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred1, na.rm = TRUE)
mh_model_acc2 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred2, na.rm = TRUE)
mh_model_acc3 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred3, na.rm = TRUE)
mh_model_acc4 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred4, na.rm = TRUE)
mh_model_acc5 <- mean(mh_model_accuracy$vote96 == mh_model_accuracy$pred5, na.rm = TRUE)

acc_list <- as.data.frame(c(mh_model_acc1,
              mh_model_acc2,
              mh_model_acc3,
              mh_model_acc4,
              mh_model_acc5))

#PREs
# get the actual values for y from the data
y <- mh_model_accuracy$vote96
# get the predicted values for y from the model
y.hat1 <- mh_model_accuracy$pred1
y.hat2 <- mh_model_accuracy$pred2
y.hat3 <- mh_model_accuracy$pred3
y.hat4 <- mh_model_accuracy$pred4
y.hat5 <- mh_model_accuracy$pred5

# calculate the errors for the null model and each rf model

getmode <- function(v) { #function to calculate mode
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

vote_mode <- getmode(mh_model_accuracy$vote96)

E1 <- sum(y != vote_mode)
E2_1 <- sum(y != y.hat1)
E2_2 <- sum(y != y.hat2)
E2_3 <- sum(y != y.hat3)
E2_4 <- sum(y != y.hat4)
E2_5 <- sum(y != y.hat5)

# calculate the proportional reduction in error
pre1 <- (E1 - E2_1) / E1
pre2 <- (E1 - E2_2) / E1
pre3 <- (E1 - E2_3) / E1
pre4 <- (E1 - E2_4) / E1
pre5 <- (E1 - E2_5) / E1

pre_list <- as.data.frame(c(pre1, pre2, pre3, pre4, pre5))

#put metrics in one table and report

model_list <- as.data.frame(c("vote96 ~ mhealth_sum",
                              "vote96 ~ mhealth_sum + I(mhealth_sum^2)",
                              "vote96 ~ mhealth_sum + age",
                              "vote96 ~ mhealth_sum + inc10",
                              "vote96 ~ mhealth_sum + educ + inc10"))

eval_df <- cbind(model_list, mse_list)
eval_df <- cbind(eval_df, acc_list)
eval_df <- cbind(eval_df, pre_list)

colnames(eval_df) <- c("Model", "Test MSE", "Test accuracy", "PRE")

kable(eval_df, caption = "Test MSEs, accuracy rates, and PREs for random forest models", format = "html")
```

I plot the ROC curves below:

```{r mh_rf_roc}
roc1 <- roc(mh_model_accuracy$vote96, mh_model_accuracy$prob1)
roc2 <- roc(mh_model_accuracy$vote96, mh_model_accuracy$prob2)
roc3 <- roc(mh_model_accuracy$vote96, mh_model_accuracy$prob3)
roc4 <- roc(mh_model_accuracy$vote96, mh_model_accuracy$prob4)
roc5 <- roc(mh_model_accuracy$vote96, mh_model_accuracy$prob5)

plot.new()
plot(roc1, print.auc = TRUE, col = "red", print.auc.x = .2)
plot(roc2, print.auc = TRUE, col = "blue", print.auc.x = .2, print.auc.y = .1, add = TRUE)
plot(roc3, print.auc = TRUE, col = "green", print.auc.x = .2, print.auc.y = .2, add = TRUE)
plot(roc4, print.auc = TRUE, col = "orange", print.auc.x = .2, print.auc.y = .3, add = TRUE)
plot(roc5, print.auc = TRUE, col = "purple", print.auc.x = .2, print.auc.y = .4, add = TRUE)
```

The purple line represents the `vote96 ~ mhealth_sum + educ + inc10` model. Overall, the `vote96 ~ mhealth_sum + educ + inc10` model performs best.

```{r mh_rf_interpret}
mh_rf5

data_frame(var = rownames(importance(mh_rf5)),
           MeanDecreaseGini = importance(mh_rf5)[,1]) %>%
  mutate(var = fct_reorder(var, MeanDecreaseGini, fun = median)) %>%
  ggplot(aes(var, MeanDecreaseGini)) +
  geom_point() +
  coord_flip() +
  labs(title = "Predicting voting turnout",
       subtitle = "Random Forest",
       x = NULL,
       y = "Average decrease in the Gini Index")

mse_rf5 <- mse(mh_rf5, mh_rf_test)
```

Based on this model, income is the most influential factor affecting voter turnout, followed by education level. Mental health does not appear to have a large effect. The test MSE for this model is `r mse_rf5`. The test accuracy rate is `r mh_model_acc5`, meaning it predicted voting accurately `r round(mh_model_acc5*100,2)`% of the time. The PRE is `r pre5`, indicating that it outperforms the useless classifier. The AUC is `r auc(roc5)[[1]]`, which is respectably close to 1.

###Subpart 2
**Use cross-validation techniques and standard measures of model fit (e.g. test error rate, PRE, ROC curves/AUC) to compare and evaluate at least five SVM models of voter turnout. Select the best model and interpret the results using whatever methods you see fit (graphs, tables, model fit statistics, predictions for hypothetical observations, etc.)**



```{r session_info, include=FALSE}
devtools::session_info()
```