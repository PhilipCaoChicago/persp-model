---
title: "Problem Set 6"
author: "Reid McIlroy-Young"
date: "February 20, 2017"
output: html_document
---

``` {r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE, warning = FALSE)

library(tidyverse)
library(modelr)
library(pROC)
```

# Description 

``` {r, Q 1.1.1}
targetFile <- "data/mental_health.csv"
data <- read.csv(targetFile)

qplot(vote96, data=data, geom="histogram", main = "Voter turnout", binwidth = 1, xlab = 'Number of times voted', ylab ='Count') +
  scale_x_discrete(limits=c(0,1))

probVote <- sum(data$vote96, na.rm=TRUE) / length(data$vote96)


```

The probability of turning out out vote (assuming `NA`s did not vote) is `r sprintf("%.2f", probVote)`.

``` {r, Q 1.1.2}
ggplot(data, aes(mhealth_sum, vote96)) +
  geom_point() +
  geom_smooth(method = lm) +
  labs(title="Mental health score vs number of times voted",  x ="Mental health score", y = "Number of times voted") +
  scale_y_continuous(limits = c(0, 1))
```

The plot tells us that having a low mental heath score (where low means healthier) is associated more highly with voting. Unfortunately, since for each score there are only 3 possible values (of which the model uses 2). This is much to small of a range for a linear fit to work well, linear models assume the range and domain are both intervals over $\mathbb{R}$ and often extrapolate to all of $\mathbb{R}$ which in this case would lead to silly results, such as people with inhumanly high mental health voting multiple times.

# Basic Model

``` {r, Q 1.2.1}
svLogit = glm(vote96 ~ mhealth_sum, data = data, family = binomial)
summary(svLogit)
```

The p-value for the mental health score's effect on voting is quite low at $3 * 10^{-13}$ which means it is statistically significant. The value is has is $-.14$ which when exponentiated gives `r exp(-.14)` $~ .15$ which is the ratio of change in voting likelihood caused by an increase in 1 on the mental health scale. This means increasing by 1 on the health scale, decreases the likelihood of voting by almost $15%$ Which indicates the relationship is substantive.

``` {r, Q 1.2.2}
#From notes
logit2prob <- function(x){
  exp(x) / (1 + exp(x))
}

prob2odds <- function(x){
  x / (1 - x)
}

prob2logodds <- function(x){
  log(prob2odds(x))
}

vote96_pred <- add_predictions(data, svLogit) 
vote96_pred <- mutate(vote96_pred, prob = logit2prob(pred))
vote96_pred <- mutate(vote96_pred, odds = prob2odds(prob))
vote96_pred <- na.omit(vote96_pred)
  
ggplot(vote96_pred, aes(mhealth_sum, pred)) +
  geom_line(color = "orange", size = 1) +
  labs(
    title = "Log odds of voting vs Mental Health rating",
    x = "Mental Health rating",
    y = "Log-odds of voting")
```

Nice linear relationship, the slope is `r svLogit$coefficients[2]` which is also the $\beta_1$ discussed above.

``` {r, Q 1.2.3}
ggplot(vote96_pred, aes(mhealth_sum, odds)) +
  geom_line(color = "orange", size = 1) +
  labs(
    title = "Odds of voting vs Mental Health rating",
    x = "Mental Health rating",
    y = "Odds of voting")
```

This plot shows the odds increasing by a factor of $e^{\beta_1}$ for each increase in mental health score, leading to the inverse curve.

``` {r, Q 1.2.4}
ggplot(vote96_pred, aes(mhealth_sum, prob)) +
  geom_line(color = "orange", size = 1) +
  labs(
    title = "Prob of voting vs Mental Health rating",
    x = "Mental Health rating",
    y = "Prob of voting")

grid <- data_grid(data, mhealth_sum)
grid <- add_predictions(grid, svLogit)
grid <- mutate(grid, prob = logit2prob(pred))

diff12 <- grid[3,] - grid[2,]
diff12 <- diff12$prob
diff56 <- grid[7,] - grid[6,]
diff56 <- diff56$prob
```

The plot shows that the probability decrease from increasing score is nearly linear.

There is a change of `r diff12` probability of voting from going from 1 to 2.

There is a change of `r diff56` probability of voting from going from 5 to 5.

``` {r, Q 1.2.5}
#From notes
PRE <- function(model){
  # get the actual values for y from the data
  y <- model$y
  
  # get the predicted values for y from the model
  y.hat <- round(model$fitted.values)
  
  # calculate the errors for the null model and your model
  E1 <- sum(y != median(y))
  E2 <- sum(y != y.hat)
  
  # calculate the proportional reduction in error
  PRE <- (E1 - E2) / E1
  return(PRE)
}


vote96_accuracy <- add_predictions(data, svLogit)
vote96_accuracy <- mutate(vote96_accuracy, pred = as.numeric(logit2prob(pred) > .5))

ac <- mean(vote96_accuracy$vote96 == vote96_accuracy$pred, na.rm = TRUE)

pre <- PRE(svLogit)

auc_val <- auc(vote96_accuracy$vote96, vote96_accuracy$pred)
```

The accuracy rate is `r ac`, the PRE is `r pre` and the AUC is `r auc_val` for this model. These indicate that the model is better than random guessing, but not by a large amount (`r auc_val - .5` to be exact). We could make some trade offs between sensitivity and specificity, but even at best the model is note good.

# Multivariate Model

First we will look to find which variables are significant

``` {r, Q 1.3.2a}
mvLogit <- glm(vote96 ~ . , data = data, family = binomial)

summary(mvLogit)
```

`mhealth_sum`, `age`, `educ` all have p-values below $.1$, so we will use them for the model below

+ The probability distribution is a single Bernoulli trial $Prob(vote96 == y_i | p_i) = p_i^{y_i}(1-p_i)^{(1-y_i)}$
+ The linear predictor is $g(p_i) = \eta_i = \beta_0 + \beta_1 mhealth\_sum + \beta_2 age + \beta_3 educ$
+ The link function is $p_i = \frac{e^{\eta_i}}{1 + e^{\eta_i}}$

``` {r, Q 1.3.2b}
mvLogit <- glm(vote96 ~ mhealth_sum + age + educ , data = data, family = binomial)

summary(mvLogit)
```

Nicely enough all three variables are still highly statistically significant after the others were removed, which indicates they are all important. If they are all substantively significant is less clear. The change in probability induced by an increase in `educ` is large at $1.30$  and people can differ by multiple years. But the other two are less clear `mhealth_sum` has a much smaller effect than before, an increase by one only changes the voting probability by $.9$ instead of $.86$ we saw before and since the effects are multiplicative and not linear this decrease is larger than just $.04$. `age` has an even smaller effect but the possible range is much larger, looking at the change an increase by $10$ will induce shows a probability change of $1.57$ which is substantial. I believe that all three of these variables are both statistically and substantively significant. 


# TV consumption

We will first look at a model with all the variables

``` {r Q 2.1.1}
targetFile <- "data/gss2006.csv"
data <- na.omit(read.csv(targetFile))

tvLogit <- glm(tvhours ~ . , data = data, family = poisson)

summary(tvLogit)
```

The three with low p-values are `hrsrelax`, `black` and with a much higher value `educ`. So lets use these three for our model.


+ The probability distribution of the number of hours watched (`tvhours`) is Poisson, i.e. $p(Y_i == k | \mu) = \frac{\mu^k e^{-\mu}}{k !}$
+ The linear predictor is $\eta = \beta_0 + \beta_1 hrsrelax + \beta_2 black + \beta_3 educ$
+ The link function is $log(\mu) = \eta_i$


``` {r Q 2.1.2}
tvLogit <- glm(tvhours ~  hrsrelax + black + educ , data = data, family = poisson)

summary(tvLogit)
```

We now have all three variables being highly significant, p-values all less than $.01$ with `black` having the largest impact for a change of 1 (prob increases by `r exp(0.440052)`). Neither of these results are surprising reducing the number of variables means we could be experience overdispersion and the predicted p-values are too low, while `black` being significant is in part due to it being a binary variable and thus the variability given across the ranges of `educ` and `hrsrelax` must be confined to a unit change.
